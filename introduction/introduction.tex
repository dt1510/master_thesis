
\chapter{Introduction}

\section{Motivation and Objectives}
\subsection{Problem of Induction}
From the observations of the environment on Earth the reader knows that the sun has always been rising in the morning and setting in the evening. This does not imply that the sun would rise tomorrow in the morning, however from our statistical observations and assumptions on the environment it is extremely likely. Therefore the reader may induce a hypothesis: the sun rises in the morning, the sun sets in the evening. As an agent the reader can profit from making the decisions based on the induced hypothesis provided that it remains true during the reader's lifespan. For example if the reader would like to maximize the time spent on the daylight, then the reader goes to bed in the evening and wakes up in the morning.

\subsubsection{Hypothesis form}

However, an induction of hypotheses of certain forms tends to be statistically more successful than an induction of hypotheses of other forms. The problem is addressed as well in a wikipedia article on Problem of Induction \cite{wikipediaProblemOfInduction}.

\begin{quote}
Nelson Goodman presented a different description of the problem of induction  in the third chapter of "Fact, Fiction, and Forecast" entitled "The New Riddle of Induction" (1954). Goodman proposed a new predicate, "grue". Something is grue if and only if it has been observed to be green before a certain time or blue after that time. The "new" problem of induction is, since all emeralds we have ever seen are both green and grue, why do we suppose that after time T we will find green but not grue emeralds? The problem here raised is that two different inductions will be true and false under the same conditions. In other words:
Given the observations of a lot of green emeralds, someone using a common language will inductively infer that all emeralds are green (therefore, he will believe that any emerald he will find will be green, even after T).
Given the same set of observations of green emeralds, someone using the predicate "grue" will inductively infer that all emeralds, which will be observed after T, will be blue, despite the fact that he observed only green emeralds so far.
Goodman, however, points out that the predicate "grue" only appears more complex than the predicate "green" because we have defined grue in terms of blue and green. If we had always been brought up to think in terms of "grue" and "bleen" (where bleen is blue before time T, or green thereafter), we would intuitively consider "green" to be a crazy and complicated predicate. Goodman believed that which scientific hypotheses we favour depend on which predicates are "entrenched" in our language.
\end{quote}

\subsubsection{Hypothesis sufficiency}
When Newton introduced his laws of motion, these could explain numerous observations and phenomena in nature, but not all. Later, Einstein came with his theory of relativity that could explain more phenomena and paradoxes of Newtonian mechanics. The theory of relativity was a better approximation of the physical world.

\subsection{Inductive Logic Programming}
Inductive Logic Programming (ILP) systems consist of software and algorithms that take a set of positive and negative examples represented as sentences in a logic programming language, then they output a set of sentences called a hypothesis which is a finite axiomatization of the theory of the approximated model of the environment that produced the observed examples.

\subsubsection{Problems of Inductive Logic Programming}
In recent years there has been an expansion in ILP field. However, to the author's knowledge there have been numberous problems in ILP and in its research that have not been given much attention:

\begin{itemize}
\item No clear objective of research. The results in the ILP field of research consist of new ILP system frameworks, their implementation and philosophical argumentation of their contribution which is disputable unlike correct proofs of mathematical theorems.
\item Philosophical foundations on the induction are not complete. Stove \cite{stove1986} resolves some questions on the problem of induction, however the solution is not universaly accepted. We still do not know if an induction is a correct method of learning and how to use it.
\item A limited development of the formalism and mathematical foundations for the theory of inductive logic programming. Consequently, restricted means of comparisons of the new ILP systems.
\end{itemize}

The author provides specific manifestations of the problems:

\subsubsection{Reliance on well-behaved examples}
ILP researchers often consider a narrow set of well-know examples, e.g. learning family relationships, but it is questionable whether the problems we are interested in solving have the same form. From the author's experience, a slight modification to the problem statement can cause some unnamed ILP systems to be very inefficient. However, these problems are not addressed by researchers as they do not become manifested in the set of chosen examples.

\subsubsection{Hypotheses space and bias}
ILP researchers restrict a hypotheses space arguing that searching for fewer hypotheses is more efficient. However, it is not clear, whether by restricting the hypotheses space we do not become oblivious to the correct hypothesis. Any statement can be a potential hypothesis since it implies consequences that could form a set of its observations.

However, a positive direction has been recorded in ILP systems capable of learning the grammar problems. In such cases we know that if such an ILP system is going to learn a regular or context-free grammar, then our bias on the hypotheses space will include all possible hypotheses.

How the hypotheses space should be restricted depends on the type of the problem we would like to solve.

\subsubsection{Model approximation}
Systems like Toplog allow hypotheses inconsistent with the observations in favour of generalization. Other systems like Imparo produce only hypotheses consistent with the observations. However, we do not understand for what types of problems what our preference should be.

%\subsubsection{Biased comparisons}
%Researchers have need to compare these systems based on certain criteria, however to the author's knowledge only a limited formalism has been developed providing the basis of comparison.

\section{Contributions}
The author is aware of the hardness of the inductive logic programming problems presented and would like to participate in their solving by developing a limited formalism for the comparison of the top-down (Toplog), bottom-up (Imparo) and metainterpretive learning (Metagol) ILP systems by considering their abstracted properties in sequence.


\section{Statement of Originality}

I declare that the work presented in the thesis is my own unless indicated with proper references.

\chapter{Project Plan}
The objective of the project is to provide the means of classification of the systems of inductive logic programming.

\section{Current state}
I have researched ILP systems Toplog, Imparo in detail and Metagol with less detail, in addition other systems in breadth Prolog, Tal, etc. I have learnt the knowledge from the areas of Model Theory, Logic Programming, Measure Theory, Kolmogorov Complexity.

\subsection{Addressed problems}
The initial problem in the field of ILP has been the lack of the direction in research and mathematical foundations with which a basis of comparison could be made. I researched the ILP systems and devised unifying definitions that identified which concepts may be of importance if we are interested in comparing ILP systems - a hypotheses space and bias, a score function, model approximation. The definitions have been formed throughout the long process of understanding the differences and similarities of the systems Imparo and Toplog from the observations of the formation of their hypotheses from various inputs as well as from the theoretical frameworks they follow.

\section{Plan}

\subsection{Problems to address}
Within the general definitions devised it is important to identify and define important subproperties of the main properties of the ILP systems - a hypotheses space/bias, a score function, model approximation. The properties should be weak enough but still interesting so that useful results could be established.

The established results should directly refer and build upon the examples of series of inputs and outputs from the ILP systems Toplog, Imparo and Metagol.

The questions and problems of potential interest include:
1. find the ways to represent a regular language in a logic programming language in order to reason about the learning capabilities of ILP systems.
2. are ILP systems capable of learning regular languages (Herbrand models whose language representation is regular)?
3. How can we reason about learning of regular languages in computability setting of an inductive inference problem?
4. Are top-down and bottom-up computability abstractions of inductive inference systems equivalent - i.e. capable of learning the same Hebrand models?
5. Find any BIIS or TIIS that can learn a regular language.
6. What languages from the Chomsky Hierarchies or definable by formulas from a given arithemetical hierarchy can a TIIS learn?

\subsection{Spring term until the  week 10}
By week 4 meeting: identify a few subproperties of the hypotheses space,
choose one most trivial property, make abstractions of Imparo and Toplog and prove whether a given property in these abstractions holds or not.

5. Choose 2nd hypotheses space property.

6. Choose 3rd hypotheses space property.

7. 4th hypotheses space property.

8. Reevaluate the progress on the classification of the ILP systems by their properties on the hypotheses space, correct inconsistences and list other properties of interest.

9. Space for other problems in the classification of ILP systems on hypotheses space.

10. Summarize the work so far, make a plan for the next period.

\subsection{End of Spring term until 20th May}
Classify the ILP systems by their score function:
How does the score function affect the hypotheses space bias?
What properties do models have that are learnable with a system using a given score function?

Classify the ILP systems by their model approximation:
How much information is preserved in the model that is an approximation of the reality, how much information can be recovered?
What is the error bound threshold that makes learning still useful?

\subsection{20th May until 17th June}
Identify the most important problem to focus on from the established results.
10th June - finalize the report.

\subsection{Fallback-back positions}
In case the project does not follow the outlined plan. In the end of the Spring term, an alternative plan would be chosen:
Study the ILP systems from the various examples (potentially biased) and provide a statistical evaluation of the results - hypotheses of what forms (how many clauses, with how many body atoms, predicates, etc.) are learnable by ILP systems and therefore establish minor incompletenss results.

\subsection{Posible extensions}
A possible extension includes inspecting the coherence of the inductive inference in an abstraction of an ILP system in a probabilistic logic. Note, that some systems like Toplog can learn a hypothesis that is consistent with the examples.

\chapter{Evaluation Report}