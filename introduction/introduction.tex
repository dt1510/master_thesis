
\chapter{Introduction}

%\section{Machine learning}

\section{Inductive inference}
Aristotle recognized 2 forms of logical reasoning: a deduction and an induction\cite{sep-aristotle-logic} (3rd form being an abduction recognized later by the philosopher C. S. Peirce(1839-1914)\cite{kimber2012learning}\cite{peirce1935collected}.

A deduction is a sound process of deriving truth from the known facts. Let p be the proposition "All men are mortal." and q be the proposition "Aristotle is a man.". Then we can \emph{deduce} the proposition r "Aristotle is mortal." from p and q.

Given the observations q, r one may \emph{induce} the proposition q as a possible explanation of r from q. However, this process of induction is not sound as in general q, r do not imply p. Indeed, one may induce an explanation p2 "Every being whose name starts with A is mortal." for the observations q, r. But we know that Aphrodite, being a Greek mythology goddess, is not mortal. While  deduced propositions restate the known, an induced proposition provides an explanation for the known and unknown.

The problem of finding a sound explanation for the observations known as a problem of induction has been investigated by Hume\cite{hume1902enquiries}\cite{selby1888treatise}, Popper\cite{keuth2013karl}, Stove\cite{stove1986rationality} and many others.

\section{Inductive logic programming}\cite{muggleton1995inverse}\cite{nienhuys1997foundations}
Inductive logic programming is a subdiscipline of machine learning and logic programming. The goal of inductive logic programming is to study how the theory of inductive inference and logic programming can be used to design theoretical frameworks and programs solving the machine learning problems. We outline the notions of an ILP system, bias and hypothesis search by inverse subsumption.

\paragraph{ILP system}
An ILP system is a program that takes as an input a set logical statements $B$ called background knowledge, a set of logical statements $E$ called examples or observations. The output of an ILP system is a set of logical statements $H$ called a hypothesis explaining the observations from the background knowledge. Meaning $E$ can be deduced from $B$ and $H$, i.e. $B \cup H \models E$.

ILP systems have been successfully applied in solving scientific problems with their applications in the areas of mesh design, mutagenicity, river water quality 
\cite{bratko1995applications}.

\paragraph{Bias}
Searching for all hypotheses $H$ that explain the observations $E$ from the background knowledge $B$ may be computationally inefficient or even infeasible. A bias is usually a much smaller subset $\mathcal{H}$ of possible hypotheses. A bias is specified at the input to an ILP system, consequently an ILP system can search more efficiently. We concern with two types of a bias: a language bias and a search bias. An example of a \emph{language bias} is ``All hypotheses whose logical formula consists of at most 10 characters.'' An example of a \emph{search bias} is ``All the hypotheses that can be found by the algorithm within 100 steps.''

\paragraph{Hypothesis search}
A hypothesis search concerns with designing an algorithm that can derive a hypothesis $H$ from the observations $E$ and background knowledge $B$, that is we require $B \cup H \models E$. ILP systems usually construct a theory $F$ called a \emph{bridge formula} from $B$ and $E$ for which the following holds $H \models \neg F$. This reduces the problem to finding a theory $H$ that entails $\neg F$, this process is called \emph{anti-entailment}. Anti-entailment in general is an expensive operation. But provided that $F, H$ meet certain conditions ($H$ subsumes $\neg F$, denoted $H \subsumes \neg F$), a more efficient operator of \emph{anti-subsumption} can be used to derive $H$ from $\neg F$. Therefore searching algorithms that can construct a bridge theory $F$ such that $H \subsumes \neg F$ can consequently find a hypothesis $H$ by using a more efficient operation of anti-subsumption instead of anti-entailment.

\section{Motivation and Objectives}
We explain the value of a classification of ILP systems, summarise the classification conducted in the field of ILP so far and conclude with the objectives of our classication of ILP systems.

\subsection{Value of classification of ILP systems}
The main benefits of the classification of ILP systems are:
\begin{itemize}
\item Identification of the successful methods,
\item Understanding of the differences between various approaches,
\item Formalisation and standarization of common concepts.
\end{itemize}

\subsection{Identification of successful methods}
\paragraph{Identification of the successful methods}
When comparing ILP systems for practical applications, we have some measure of success in mind that creates a basis for comparison. In a broader context comparison we are concerned with the structural properties and methods of ILP systems as well.
A comprehensive classification of ILP systems creates the correlation between the measures of success and the defining properties of an ILP system from which we can identify both \emph{established and emerging} techniques that prove to be successful relative to our interests.
\paragraph{Identification of the points for improvement}
Classification of ILP systems not only reveals the strengths of particular techniques, but identifies their weaknesses (and therefore the points of the improvement) evident from the comparison of the systems designed with different objectives.
\subsection{Understanding of differences between various approaches}
As the strengths and weaknesses of ILP systems are correlated with the defining properties induced by their approaches, one gains understanding of the benefits of a particular approach. This often results in removing black-white understanding of the value of ILP systems and aids in making an informed decision in selection of an ILP system for a specific application.

\subsection{Formalisation and standarization of common concepts}
ILP systems are often incompatible as they are developed by numerous researchers with different objectives, but many of their successful techniques are built on the same intuition which makes them in some sense similar. The classification of ILP systems requires that the properties of similar techniques are formalised so that a comparison between these systems could be made. This process of formalisation of the common concepts is natural and arises from what has been proved in practice to be successful as opposed to designing a theoretical framework and then to require ILP systems to be complaint. Because of its naturality, the formalism extending the theory of ILP is appropriate. The newly formalised concepts can be studied theoretically further from which knowledge for designing new ILP systems may arise.

\subsection{Current classification of ILP systems}
There have been numerous comparisons of ILP systems. But mutual comparisons of several systems are very brief as for example a comparison of ILP systems Mis, Cigol, Foil, Golem, Linus, Clint, Mobal, Claudient, Spectre, Progol based on their application \cite{nienhuys1997foundations}.
On the other hand deeper comparisons tend to concern fewer systems: 
a comparison of two ILP systems Progol and Foil on induction of a hypothesis from  large background knowledge \cite{srinivasan1995comparing},
a comparison of Progol, Tilde and C4.5 on road traffic data \cite{roberts1998comparison}. The third kinds of comparisons are between newly created systems with the existing systems \cite{muggleton2012mc} \cite{corapi2011nonmonotonic}.
ILP researchers typically evaluate their new designed system empirically based on the success of finding a hypothesis for a carefully chosen datasets that may not be representative of dataset in the real world.

But to the author's knowledge there has not been an extensive experimental or theoretical comparison of multiple ILP systems (cf. \fullref{evaluation_experimental_classification}). 

\subsection{Objectives}
The objectives of the classification of ILP systems in this thesis are:
\begin{itemize}
\item experimental classification of ILP systems,
\item formalisation and classification by a bias,
\item theoretical classification of ILP systems.
\end{itemize}

\paragraph{Experimental classification of ILP systems}
During the development phase, ILP systems are usually tested on a set of well-known examples (cf. \ref{evaluation_experimental_classification}) and therefore produce the results compatible with their theoretical frameworks.
We would like to find datasets of examples for which ILP systems produce
unexpected results, for which specifics of ILP systems not extractable from their theoretical frameworks can be observed. Consequently, we would like to classify ILP systems based on the unexpected results and their peculiarities produced for these examples.
\paragraph{Formalisation and classification by bias}
We would like to formalise the notions of a language bias used in implementations of ILP systems and establish a correspondence with a theoretical specification of a language bias. Consequently we would like to classify ILP systems by the properties of their bias.
\paragraph{Theoretical classification of ILP systems}
We would like to extend the newest theory of inverse subsumption method for finding a hypothesis efficiently that could serve in the future for the theoretical classification of a search bias of ILP systems. We would like to realize theoretical extensions in an implementation to test the soundness of our ideas.

\section{Summary of thesis achievements and contributions}
The main contributions of the thesis are
\begin{itemize}
\item formalisation of a language bias and theoretical correspondence between mode declarations, determinations, metaconstraints and production field ( \fullref{sec:background_language_bias}),
\item experimental classification of ILP systems (\fullref{chap:classification_of_ilp_systems}),
\item proof of completeness of Imparo by inverse subsumption (\fullref{completeness_ctis}),
\item extension of the results Inverse subsumption for complete explanatory induction (\fullref{inverse_subsumption_extensions}) and their implementation Rationale ILP system (\fullref{chap:rationale_ilp_system}).
\item an appendix with around 100 new experimental examples of learning problems demonstrating the pecularities of classified ILP systems with explanations of the results for each ILP system.
\end{itemize}

\section{Overview}
The rest of the thesis is organized as follows:
\begin{itemize}
\item \subsubsection{\fullref{ch:background}} We provide a common background for languages, logics, model theory and logic programming.
\item \subsubsection{\fullref{ch:inductive_logic_programming}}
We provide background for inductive logic programming and list the ILP systems to be classified.
\item \subsubsection{\fullref{ch:bias}}
We formalise the theory of a language bias and classify ILP systems by their language and search bias.
\item \subsubsection{\fullref{chap:classification_of_ilp_systems}}
We perform experimental classification of ILP systems based on the classes of problems they can solve and on the robustness of their implementations.
We provide a summary of the classification in the section \ref{classification_summary}.
\item \subsubsection{\fullref{inverse_subsumption_for_complete_explanatory_induction}}
We present a complete inverse subsumption with minimal complements algorithm by Yamamoto et al. \cite{yamamoto2012inverse} for finding a hypothesis with anti-subsumption. Further, we extend this algorithm to first-order theories, negative examples and relax it while preserving the completeness. We introduce two inverse subsumption operators and prove their completeness. The chapter is concluded with the proof of Imparo's completeness by inverse subsumption.
\item \subsubsection{\fullref{chap:rationale_ilp_system}}
We summarise our prototype ILP system implementation Rationale based on the theoretical extensions from the preceeding chapter.
\item \subsubsection{\fullref{ch:evaluation}}
We evaluate the achievements of the thesis and their relative contribution wrt what has been done in the field of ILP.
\item \subsubsection{\fullref{ch:conclusions}}
We conclude the chapter summarising the new knowledge that the thesis created and point out the future directions for the work.
\item \subsubsection{Appendix}
Appendix contains around 100 experimental examples of learning problems with explanations of the results for each ILP system we classified:\\
Progol \ref{appendix_progol},
Aleph \ref{appendix_aleph},
Toplog \ref{appendix_toplog},  
Xhail \ref{appendix_xhail},
Imparo \ref{appendix_imparo},
Tal \ref{appendix_tal}.
\end{itemize}