
\chapter{Background Theory}

%\label{â€¢}el{ch:background}

\section{Introduction}
Inductive Logic Programming (ILP) systems consist of software and algorithms that take a set of positive and negative examples represented as sentences in a logic programming language, then they output a set of sentences called a theory which is a finite axiomatization of the theory of the model of the environment that produced the examples. In recent years there has been an expansion in ILP field, researchers have need to compare these systems based on certain criteria, however only a limited formalism has been developed providing the basis of comparison.
We start with the definitions formalizing the problem of Inductive Logic Programming and defining an ILP system keeping in the definition the properties on which the comparison could be based.

\section{Prerequisities}
We assume the reader is familiar with basic concepts in several areas and consider the following references to be potentially useful:
\begin{enumerate}
\item Foundations of Inductive Logic Programming by Cheng and Wolf \cite{cheng1997}
\item Model theory: An introduction by David Marker \cite{marker2002}
\item An introduction to Kolmogorov complexity and its applications by Li and Vitanyi \cite{li1997}
\end{enumerate}
However, the usage of the concepts does not exceed its rudimentary application and the reader is encouraged to proceed further even if one may not be familiar with all the areas outlined, we suggest to use the literature when the need arises.
\section{Definitions}

\subsection{A logic language}
\begin{defn}
An \emph{alphabet} is a set $\mathcal{A}$ of elements called \emph{letters} of an alphabet.
\end{defn}

\begin{defn}
A word over an alphabet $\mathcal{A}$ is a finite sequence (a string) of letters of the alphabet $\mathcal{A}$.
\end{defn}

\begin{defn}
A formal language $L$ is a set of words over the specified alphabet $\mathcal{A}$, i.e. $L \subseteq 2^\mathcal{A}$.
\end{defn}

\begin{exmp}
Let $R$ be a regular expression $0001*$.
\begin{itemize}
\item The alphabet is the set $\mathcal{A}={\epsilon, 0,1,*}$. $0$, $1$, $*$ are called letters of $\mathcal{A}$.
\item The words over $A$ are $000$.
\end{itemize}
\end{exmp}

Formal languages we are going to use are first-order languages:
\begin{defn}
A \emph{language} $\mathcal{L}$ is a formal language given by the signature and grammar of the first-order logic and the additional data:
\begin{enumerate}
\item a set $\mathcal{C}$ of letters called constant symbols,
\item a set $\mathcal{R}$ of letters called relation symbols and a positive integer (an arity) $n_R$ for each $R \in \mathcal{R}$,
\item a set $\mathcal{F}$ of letters called function symbols and a positive integer (an arity) $n_f$ for each $f \in \mathcal{F}$.
\end{enumerate}
\end{defn}

\begin{exmp}
The language $\mathcal{L}_{EST}$ of elementary set theory is given by one constant symbol called an empty set, $\mathcal{C}=\{\emptyset\}$, one relation symbol called a set membership $\mathcal{R}=\{\in\}$ with a positive integer (arity) $n_{\in}=2$. The set of function symbols $\mathcal{F}$ are the set operations of a union, an intersection, each with an arity $2$.
\end{exmp}

\subsection{Languages in the theory of inductive inference}
\begin{defn}
Let a language $\mathcal{L}$ be given by sets $\mathcal{C}$, $\mathcal{R}$, $\mathcal{F}$ and a language $\mathcal{L}'$ given by sets $\mathcal{C}'$, $\mathcal{R}'$, $\mathcal{F}'$.
If $\mathcal{C} \subseteq \mathcal{C}'$, $\mathcal{R} \subseteq \mathcal{R}'$, $\mathcal{F} \subseteq \mathcal{F}'$ we say that $\mathcal{L}$ is a sublanguage of $\mathcal{L}'$, $\mathcal{L}'$ is a superlanguage of $\mathcal{L}$ and denote the relation between the languages by $\mathcal{L} \subseteq \mathcal{L}'$.
\end{defn}

\begin{defn}
Given two languages $\mathcal{L}_o$ called an observational language, $\mathcal{L}_h$ called a hypothesis language, a \emph{language of enquiry} $\mathcal{L}$ is a language that is a superlanguage of a hypothesis language and a superlanguage of an observational language.
\end{defn}

\begin{remark}
The language of enquiry is a model-theoretic language by our definition. We will apply model theory with the first-order logic since many ILP systems are based on Prolog - the logic programming language with its roots in first-order logic. Therefore the boolean connectives $\neg$, $\land$, $\lor$, $\implies$ are implicitly added to the language of enquiry $\mathcal{L}$ and the formation rules of first-order logic are assumed. In generality one could devise a theory working in any formal language.
\end{remark}

\begin{exmp}
\begin{itemize}
\item The language of enquiry $\mathcal{L}$ is given by $\mathcal{C}_o=\{milk, curry, rice\}$,$\mathcal{R}_o=\{TastesHot, IsWhite, ContainsSpice, ContainsSugar\}$, $\mathcal{F}_o=\{\}$.
\item Let the observational language $\mathcal{L}_o$ be $\mathcal{C}_o=\{milk, curry, rice\}$,\\ $\mathcal{R}_o=\{TastesHot, IsWhite\}$, $\mathcal{F}_o=\{\}$
\item Let the hypothesis language $\mathcal{L}_h$ be $\mathcal{C}_h=\{milk, curry, rice\}$,\\ $\mathcal{R}_h=\{TastesHot, IsWhite, ContainsSpice\}$, $\mathcal{F}_h=\{\}$.
\item $\mathcal{L}_h$-sentences are $\forall x. TastesHot(x) \implies ContainsSpice(x)$,\\ $\forall x. IsWhite(x) \lor TastesHot(x)$.
\end{itemize}
\end{exmp}

\begin{defn}
Given a language $\mathcal{L}$, the \emph{language bias} is a property (a boolean valued function) defined on all elements of the sets $\mathcal{C}$, $\mathcal{R}$, $\mathcal{F}$. For example, given a language of enquiry $\mathcal{L}$, its hypothesis sublanguage $\mathcal{L}_h$, if the property $H$ satisfies $\mathcal{C}_h=\{c \in \mathcal{C} | H(c)\}$, $\mathcal{R}_h=\{R \in \mathcal{R} | H(R)\}$, $\mathcal{F}_h=\{f \in \mathcal{F} | H(f)\}$ then we say $H$ is a language bias of the hypothetical language $\mathcal{L}_h$ in its superlanguage of enquiry $\mathcal{L}$.
\end{defn}
TODO: Correct: the language bias is a restriction to a hypotheses space not to the language. A better term would be search space bias.

\begin{exmp}
Let $P(ContainsSugar)=false$ and $true$ otherwise. $P$ is the language bias of $\mathcal{L}_h$ in $\mathcal{L}$.
\end{exmp}

\begin{defn}
The class of all $\mathcal{L}$-structures of a language of enquiry $\mathcal{L}$ are called the \emph{hypothetical models}, an $\mathcal{L}$-structure is a model of $\mathcal{L}$. For every language of enquiry $\mathcal{L}$ there is exactly one $\mathcal{L}$-structure (model) $\mathcal{M}$ that is called the \emph{reality} of $\mathcal{L}$.
\end{defn}

\begin{exmp}
\begin{itemize}
\item In England, the reality $\mathcal{M}_E$ is a model of the sentences\\ $\Sigma_E=\{TastesHot(curry), \neg TastesHot(milk)\}$.
\item In India, the reality $\mathcal{M}_I$ is a model of the sentences\\ $\Sigma_I=\{TastesHot(curry), TastesHot(milk)\}$.
\item Both $\mathcal{M}_E$, $\mathcal{M}_I$ are hypothetical models of $\mathcal{L}$.
\end{itemize}
\end{exmp}

\begin{remark}
In generality one may develop a theory that assumes existence of multiple or none model of enquiry, this may be useful when the language of enquiry has a limited expressivity as demonstrated later.
\end{remark}

\begin{notation}
Given a set of $\mathcal{L}$-formulas $\Sigma$, we denote $M(\Sigma)$ to be the class of hypothetical models of $\Sigma$.
\end{notation}

\begin{defn}
The set of \emph{observations} $O$ is any set of $\mathcal{L}_o$-formulas.
\end{defn}

\begin{exmp}
\begin{itemize}
\item The set $O_E=\{TastesHot(curry), \neg TastesHot(milk), IsWhite(milk)\}$ is the set of observations.
\item The set $O_I=\{TastesHot(milk)\}$ is the set of observations.
\item The set $O_{world}=\{TastesHot(curry), \neg TastesHot(milk), TastesHot(milk), \\
IsWhite(milk)\}$ is the set of observations.
\end{itemize}
\end{exmp}

\begin{remark}
\begin{itemize}
\item The language of enquiry $\mathcal{L}$ is not powerful enough to express $O_{world}=\{\neg TastesHot(milk) \land InEngland(milk), TastesHot(milk) \land InIndia(milk)\}$
 as it does not have predicate symbols $InEngland$ and $InIndia$.
\item The inconsistency in the observations implies that there is no $\mathcal{L}$-structure $\mathcal{M}$ of $O_{world}$.
\item One of the central problems in ILP is the predicate invention - adding symbols $InEngland$ and $InIndia$ to the language of enquiry $\mathcal{L}$.
\end{itemize}
\end{remark}

\begin{defn}
The \emph{background knowledge} (or current induced theory) is a set $B$ of formulas in a hypothesis language $\mathcal{L}_h$.
\end{defn}

\begin{note}
Given a set of observations $O$, an $\mathcal{L}$-formula $Q$ \emph{divides} the models of $O$ into models of $O \wedge \{Q\}$ denoted $M(O \union \{Q\})$ and the models of $\{\neg Q\} \wedge O$ denoted $M(O \union \{Q\})$. That is $\forall \mathcal{M} \in M(O \union \{Q\}). \mathcal{M} \models O \union \{Q\}$.
\end{note}

\begin{defn}
A hypothesis space $\mathcal{H}$ is the space of all possible hypotheses in the search space. A set of hypotheses is denoted $H$, one hypothesis denoted $h$. Note $h \in H \subseteq \mathcal{H}$.
\end{defn}

\subsection{Ultimate problem of scientific discovery}
Statement: Given a language of enquiry $\mathcal{L}$, find the reality $\mathcal{M}$.

We do not consider the situation in which we may desire to learn approximate models of the reality. In general, depending on our language of enquiry, the reality may not exist, however our definition considers only the languages of enquiry with exactly one reality. Some realities, e.g. (regular expression) are learnable, others may be not. The ultimate problem consists of numerous incremental problems.

\subsection{Incremental problem of scientific discovery}
We would like to find the reality most efficiently, where the efficiency may be measured by some criteria - e.g. time and resources. The following simplified definition does not take into the account that some questions may be found more efficiently than others.

Set a probability measure $\mu:M(\emptyset) \to [0,1] \subset \mathcal{R}$ over hypothetical models of a language of enquiry $\mathcal{L}$. Given a background knowledge $B$, find a question $Q$ that minimizes the following equation: $|M(B \union {Q})-M(B \union {\neg Q})|$, i.e. a question that divides the models of $B$ most evenly.

\subsection{Definition of a problem of an induction}
A problem of induction is a functional problem whose input is a pentuple $P=\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle$ where $\mathcal{L}$ is a language of enquiry with its sublanguages $\mathcal{L}_o$, $\mathcal{L}_h$, observations $O$ and the background knowledge $B$.

Define an equivalence relation over the set (called the search space) $H$ of the sets of the $\mathcal{L}$-formulas: $\Sigma \sim \Gamma \iff K(\Sigma)=K(\Gamma)$ where $K$ is the Kolmogorov complexity with respect to the description language $\mathcal{L}$. Find a representative of the axiomatization with the least Kolmogorov complexity of the theory $\Sigma$ whose elementary class of models are precisely the models of $B \union O$. The solution  to a problem of induction (the output) is the representative $\Sigma$ called an induced theory from the background knowledge $B$ and the observations $O$.

\subsection{Definition of a scientific method}
A scientific method is an algorithm used to solve the ultimate problem of the scientific discovery:

0. Start with the emty theory $B_i=\emptyset$, set $i=0$.

1. If $B_i$ has the only model, then terminate, $B_i$ is a complete axiomatization of the reality of the language of enquiry.

2. Solve an incremental problem of scientific discovery by finding a question $Q$ given the current induced theory $B_i$.

3. Make an observation - ask an environment oracle a question $Q$.

4. Induce a more precise theory $B_{i+1}:=B \union \{Q\}$ if the oracle says that $Q$ is true, $B_{i+1}:=B \union \{\neg Q\}$ if the oracle says that $\neg Q$ is true.

5. Increment $i$ and start from step 1 again.

\subsection{Definition of an ILP problem}
Inductive logic programming problem is a problem of an induction where the language of enquiry $\mathcal{L}$ is the logic programming language (e.g. Prolog), the background knowledge and induced theory are finite.

In the context of ILP one divides the observations into positive examples $E^+$ where the reality is a model of instances of $E^+$, and negative examples $E^-$ - where the reality is a model of negated instances of $E^-$. The division is conceptually important as the clause that is not provable from the axioms is false as opposed to the monotonic logics whose theories can be incomplete.

Note: A constructed $\mathcal{L}$-formula in Prolog is a definite clause. 

\subsection{Definition of an ILP system}
An ILP system is any function $f:\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle \mapsto \Sigma \in \mathcal{H}$ where the language of enquiry $\mathcal{L}$ is the logic programming language, the background theory $B$ and induced theories of $H$ are finite.

\subsection{Definition of a correct ILP system}
An ILP system is correct if it solves an ILP problem, i.e. its output is a solution to an ILP problem.

\subsection{Examples of ILP systems}
1. $f:\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle \to \mathcal{H}$, $f:\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle \to \emptyset$ is an ILP system, however it is not correct since $M(O \union B) \not= M(\emptyset)$ for all sets of observations and the background knowledge.

2. Progol is an ILP system however it is not correct since it does not always return the induced theory of the least Kolmogorov complexity.

\subsection{Non-determinism, uncertainity, algorithm}

Since the Kolmogorov complexity is uncomputable no computable ILP system is correct. To develop a theory useful in real applications:

1. we need to consider approximations to the Kolmogorov complexity (which are computable) or allow that the induced theory $\Sigma$ is at most $p\%$ more complex from the simpliest induced theory.

2. allow that the induced theory $\Sigma$ found may not have exactly the models that are the models of $B \union O$. We need to think of some acceptable probability error $\epsilon>0$ where $\mu(M(\Sigma))-\mu(M(B \union O)) = \mu(M(\Sigma)  \triangle  M(B \union O)) < \epsilon$.

3. rather than giving a simple function as a black box which we could classify only based on its output from the input, we need a way to inspect the inside of such functions by dividing them into smaller algorithmic parts.
TODO: clarify.

\section{Inductive Logic Programming systems}
We give an overview of ILP systems taking different approaches to an ILP problem. The definition of an ILP system we gave is not sufficient for making meaningful comparisons based on the usefulness of these systems in real applications nor from a theoretic viewpoint. The aim of familiarizing with these systems is to find the intuition on the key properties of ILP systems that should be formalized in order to benefit from the mathematical rigour required for reasoning about ILP systems.

\subsection{Progol}
\begin{itemize}
\item Inverse Entailment,
\item a bias provided by mode declaration,

\end{itemize}

\subsection{Toplog}
An ILP system implementation Toplog is based on the theoretical framework Top Directed Hypothesis Derivation (TDHD) with the hypothesis space bias called top theory $\top$:
\begin{itemize}
\item top theory $\top$ is specified in the input to an ILP system,
\item top theory $\top$ can be constructed from the mode declarations,
\item not every top theory bias can be expressed with the mode declarations,
\item a top theory consists of literals: terminals (in hypothesis language) and non-terminals (not allowed in hypothesis language and background knowledge)
\item a hypothesis clause in a hypothesis space consists of terminals and is derivable from the top theory by SLD-resolution and substitution, $\top \models h$
\item restriction on the predicates in the head/body of a hypothesis clause,
\item every constructed hypothesis must be subsumed by a top theory $\top$. If $H$ is a set of candidate hypotheses, then: $\forall h \in H. \exists e \in E. \top \models h \& B, h \models e$.
\end{itemize}
The algorithm used to construct the theory follows the steps:
\begin{itemize}
\item hypothesis derivation: derive refutations of $\neg e$ from $B$ and $\top$, derive a clause $h$ from the refutations, add $h$ to $H$.
\item coverage computation: which examples $E^+$ and $E^-$ are entailed by $h \in H$.
\item final theory construction: select $H' \subseteq H$ maximizing the score function - e.g. compression, coverage, accuracy,
\end{itemize}

\item uses Mode Directed Inverse Entailment,

\subsection{MC-Toplog}
\begin{itemize}
\item Derives hypotheses like Toplog, allows multiple clauses in a hypothesis.
\item Restricting hypotheses space to spaces entailing generalization of multiple examples (co-generalization).
\item Correctness: $\top$DTD algorithm returns all candidates hypotheses $H$ satisfying $B \wedge H \models e$ where $e \in E^+$ and the hypothesis space is restricted by the $\top$.
\end{itemize}

\subsection{ProGolem}
\begin{itemize}
\item Inverse Entailment,
\item co-generalization, 
\end{itemize}

\subsection{Imparo}
\begin{itemize}
\item solo-generalization
\end{itemize}

\chapter{Problems in ILP}

\chapter{Learning the reality}
It is important to understand how powerful our ILP systems are - what models they are capable of learning and what models they are never going to learn from the given data.
\section{Philosophical discussion}
In order to learn the reality, do we want to induce questions or theories of the models of the observations and the background knowledge? We could use all the observations to reason about the world, but the observations may be too numerous, hence a more compact theory axiomatization would be useful.
Why do we ask a question $Q$? That is so that we would find out what observation we should make. It is in the case that an environment oracle is available.
\section{Problems of research in ILP}
1. No clear objective of research. Note: Maybe there is no objective for constructing an ILP system as there is no objective for constructing a group. But is of our interest is the classification of the groups and learning general patterns about withing Group theory.
2. Philosophical foundations of the field, but no mathematical foundations, no axioms for the theory of inductive logic programming. No model of an ILP system to reason about.
3. No means of mutual comparison of ILP systems.
\section{Mathematical Foundations of ILP}
We would like to describe an ILP system with the axioms in order to study ILP systems with the mathematical rigour, to make logic statements about such systems.
I would like to have mappings (e.g. homomorphisms) between these ILP systems so that I may reason about the sub-ILP-systems. It seems Category theory to help me with the foundations. The class of the ILP systems is a category.
\subsection{An ILP system is a category}
What are objects and arrows?
\subsection{Category of ILP systems}

\subsection{Properties of ILP systems}

\subsection{}
\subsection{}
\section{Completeness of ILP systems}
Often ILP systems are complete as a result of using a strong bias rather than a sophisticated induction system.
\section{Generalization}
\subsection{Incoherence of the generalization}
Consider the observations $P(0), \neg P(1), P(2)$ which are generalized by the rule $R1=\forall x. P(x) \iff x$ is even. By inducing the $\Pi_1$ statement into the theory, it may be impossible for some true $\Pi_2$ statements to be part of the theory. Therefore limit ourselves to studying only the models where $\Pi_1$ statement true, $\Pi_2$ statement false.
\subsection{Consequences of the generalization}
1. A theory $T_g$ with a general statement $\forall x.P(x)$ entails more statements $P(a_i)$ then a theory $T$ with $\Pi_0$ statements $P(a_1), ..., P(a_n)$. Therefore $\#M(T_g) \leq \#M(T)$.
2. 
\subsection{The reasons to generalize}
1. A general statement may be a more compact representation than specific multiple statements.
2. A general theory $T_g$ entailing some of the statements in a theory $T$ and contradicting other statements in a theory $T$ (usually fewer) is an approximation of a theory $T$. A precision is the percentage of the models of $T_g$ that are models of $T$. A recall is the percentage of the models of $T$ that are models of $T_g$. An approximated theory may be simpler to reason about and to represent.
3. 
\subsection{How do the models of a theory and their number change during generalization and specialization?}
\section{The role of a language in learning the models}
Consider an empty language. It has one possible interpretation and one model up to an isomorphism.
By extending the language we can distinguish the models further. The problem of the predicate invention is to invent a predicate so that the right distinction between the models is made based on the language.
Questions:
1. How can we measure an expessivity and a complexity of a language?
2. How much information can be captured by a $\mathcal{L}$-theory given a language $\mathcal{L}$?
3. What is the maximal complexity of the language $\mathcal{L}$ for which the $\mathcal{L}$-theory is decidable?
4. Complexity of learning the equivalence class of the reality given a language.
5. Is there a language for which the task of learning the equivalence class of the reality is undecidable?
6. What percentage of the reality can we recover from observing and reasoning only with the statements in the restricted language?
7. Can a language be partitioned (or expressed as posets of sublanguages) into sublanguages and the theories of the sublanguages be learnt, then joint into a theory of the original language?
8. 
\section{Models with an empty language}

\section{Counting the number of the models of monadic theories}
Define a probability measure of the models $\mu:M(\emptyset) \to [0,1] \subset \mathcal{R}$. Let $M_0$ be the set of all models, $\mu(M_0)=0$. Let $\mathcal{L}=\{c_1, ..., c_n, P\}$ be the language with the $n$ constant symbols $c_i$ and with a unary predicate symbol $P(.)$. If $T=\{P(c_1\}$, then what is greater $\mu(M(T))$ or $\mu(M(\{\neg P(c_1)\}))$? We can define the default probability for the first atomic statement: $p(\mathcal{M} \models P(c_1))=1/2$, consequently $\mu(M(T))=1/2=\mu(M(\{\neg P(c_1)\}))$. The next statement $P(c_i)$ is independent of the first, so define the probability $P(c_i)=1/2$ for $i \in \{1, ..., n\}$.
\begin{defn}
A formula $\phi$ is $k$-adic iff its all predicate symbols (including an equals sign) and function symbols translated to predicate symbols are of the arity at most $k$. A formula $\phi$ is monadic iff $\phi$ is $1$-adic. A set or a theory is $k$-adic iff all its formulas are $k$-adic.
\end{defn}
\begin{defn}
A theory (or a set) $T$ is maximally $k$-adically consistent iff $T$ is consistent and for every formula $\phi$ if $\phi$ is $k$-adic, then $\phi \in T$ or $\neg \phi \in T$.
\end{defn}
\begin{remark}
Let $\mathcal{L}$ be the language with $n$ constant symbols and $m$ unary predicate symbols. Then there are exactly $2^{mn}$ maximally monadically consistent monadic theories $T$ (equivalence classes of models of $T$).
\end{remark}
\begin{remark}
Let $\mathcal{L}$ be the language with countably many constant symbols and $m$ unary predicate symbols. Then there are uncountably many maximally monadically consistent monadic theories.
\end{remark}
\subsection{Asking an environment oracle a question}
\subsection{}
\section{Learning the theories from the probabilistic oracle}
The correctness of our observations may be distributed over the probability distribution of the environment.
\section{Approximations to learning}
\section{Which models are more probable?}
\section{Predicate Invention}
Consider the language $\mathcal{L}=\{father\}$ with a binary predicate symbol. A predicate invention is a problem of adding to the language a predicate $grandfather$ and an axiom to all the theories: $\forall x. \forall y. \forall z. father(x,y) \wedge father(y,z) \implies grandfather(x,z)$.
TODO: Is that right? In that case $grandfather$ is definable in terms of $father$ and therefore redundant.