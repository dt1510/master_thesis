
\chapter{Background Theory}

%\label{â€¢}el{ch:background}

\section{Introduction}
Inductive Logic Programming (ILP) systems consist of software and algorithms that take a set of positive and negative examples represented as sentences in a logic programming language, then they output a set of sentences called a theory which is a finite axiomatization of the theory of the model of the environment that produced the examples. In recent years there has been an expansion in ILP field, researchers have need to compare these systems based on certain criteria, however only a limited formalism has been developed providing the basis of comparison.
We start with the definitions formalizing the problem of Inductive Logic Programming and defining an ILP system keeping in the definition the properties on which the comparison could be based.

\section{Prerequizities}
We assume the reader is familiar with basic concepts in several areas and consider the following references to be potentially useful:
\begin{enumerate}
\item Foundations of Inductive Logic Programming by Cheng and Wolf \cite{cheng1997}
\item Model theory: An introduction by David Marker \cite{marker2002}
\item An introduction to Kolmogorov complexity and its applications by Li and Vitanyi \cite{li1997}
\end{enumerate}
However, the usage of the concepts does not exceed its rudimentary application and the reader is encouraged to proceed further even if one may not be familiar with all the areas outlined, we suggest to use the literature when the need arises.
\section{Definitions}

\begin{defn}
A \emph{language} $\mathcal{L}$ is a model-theoretic language given by:
\begin{enumerate}
\item a set $\mathcal{C}$ of constant symbols,
\item a set $\mathcal{R}$ of relation symbols and a positive integer $n_R$ for each $R \in \mathcal{R}$,
\item a set $\mathcal{F}$ of function symbols and a positive integer $n_f$ for each $f \in \mathcal{F}$.
\end{enumerate}
\end{defn}

\begin{exmp}
The language $\mathcal{L}_{EST}$ of elementary set theory is given by one constant symbol called an empty set, $\mathcal{C}=\{\emptyset\}$, one relation symbol called a set membership $\mathcal{R}=\{\in\}$ with a positive integer (arity) $n_{\in}=2$. The set of function symbols $\mathcal{F}$ are the set operations of a union, an intersection, each with an arity $2$.
\end{exmp}

\begin{defn}
Let a language $\mathcal{L}$ be given by sets $\mathcal{C}$, $\mathcal{R}$, $\mathcal{F}$ and a language $\mathcal{L}'$ given by sets $\mathcal{C}'$, $\mathcal{R}'$, $\mathcal{F}'$.
If $\mathcal{C} \subseteq \mathcal{C}'$, $\mathcal{R} \subseteq \mathcal{R}'$, $\mathcal{F} \subseteq \mathcal{F}'$ we say that $\mathcal{L}$ is a sublanguage of $\mathcal{L}'$, $\mathcal{L}'$ is a superlanguage of $\mathcal{L}$ and denote the relation between the languages by $\mathcal{L} \subseteq \mathcal{L}'$.
\end{defn}

\begin{defn}
Given two languages $\mathcal{L}_o$ called an observational language, $\mathcal{L}_h$ called a hypothesis language, a \emph{language of enquiry} $\mathcal{L}$ is a language that is a superlanguage of a hypothesis language and a superlanguage of an observational language.
\end{defn}

\begin{remark}
The language of enquiry is a model-theoretic language by our definition. We will apply model theory with the first-order logic since many ILP systems are based on Prolog - the logic programming language with its roots in first-order logic. Therefore the boolean connectives $\neg$, $\land$, $\lor$, $\implies$ are implicitely added to the language of enquiry $\mathcal{L}$ and the formation rules of first-order logic are assumed. In generality one could devise a theory working in any formal language.
\end{remark}

\begin{exmp}
\begin{itemize}
\item The language of enquiry $\mathcal{L}$ is given by $\mathcal{C}_o=\{milk, curry, rice\}$,$\mathcal{R}_o=\{TastesHot, IsWhite, ContainsSpice, ContainsSugar\}$, $\mathcal{F}_o=\{\}$.
\item Let the observational language $\mathcal{L}_o$ be $\mathcal{C}_o=\{milk, curry, rice\}$,$\mathcal{R}_o=\{TastesHot, IsWhite\}$, $\mathcal{F}_o=\{\}$
\item Let the hypothesis language $\mathcal{L}_h$ be $\mathcal{C}_h=\{milk, curry, rice\}$, $\mathcal{R}_h=\{TastesHot, IsWhite, ContainsSpice\}$,$\mathcal{F}_h=\{\}$.
\item $\mathcal{L}_h$-sentences are $\forall x. TastesHot(x) \implies ContainsSpice(x)$, $\forall x. IsWhite(x) \lor TastesHot(x)$.
\end{itemize}
\end{exmp}

\begin{defn}
Given a language $\mathcal{L}$, the \emph{language bias} is a property (a boolean valued function) defined on all elements of the sets $\mathcal{C}$, $\mathcal{R}$, $\mathcal{F}$. For example, given a language of enquiry $\mathcal{L}$, its hypothesis sublanguage $\mathcal{L}_h$, if the property $H$ satisfies $\mathcal{C}_h=\{c \in \mathcal{C} | H(c)\}$, $\mathcal{R}_h=\{R \in \mathcal{R} | H(R)\}$, $\mathcal{F}_h=\{f \in \mathcal{F} | H(f)\}$ then we say $H$ is a language bias of the hypothetical language $\mathcal{L}_h$ in its superlanguage of enquiry $\mathcal{L}$.
\end{defn}
TODO: Correct: the language bias is a restriction to a hypotheses space not to the language.

\begin{exmp}
Let $P(ContainsSugar)=false$ and $true$ otherwise. $P$ is the language bias of $\mathcal{L}_h$ in $\mathcal{L}$.
\end{exmp}

\begin{defn}
The class of all $\mathcal{L}$-structures of a language of enquiry $\mathcal{L}$ are called the \emph{hypothetical models}, an $\mathcal{L}$-structure is a model of $\mathcal{L}$. For every language of enquiry $\mathcal{L}$ there is exactly one $\mathcal{L}$-structure (model) $\mathcal{M}$ that is called the \emph{reality} of $\mathcal{L}$.
\end{defn}

\begin{exmp}
\begin{itemize}
\item In England, the reality $\mathcal{M}_E$ is a model of the sentences $\Sigma_E=\{TastesHot(curry), \neg TastesHot(milk)\}$.
\item In India, the reality $\mathcal{M}_I$ is a model of the sentences $\Sigma_I=\{TastesHot(curry), TastesHot(milk)\}$.
\item Both $\mathcal{M}_E$, $\mathcal{M}_I$ are hypothetical models of $\mathcal{L}$.
\end{itemize}
\end{exmp}

\begin{remark}
In generality one may develop a theory that assumes existence of multiple or none model of enquiry, this may be useful when the language of enquiry has a limited expressivity as demonstrated later.
\end{remark}

\begin{notation}
Given a set of $\mathcal{L}$-formulas $\Sigma$, we denote $M(\Sigma)$ to be the class of hypothetical models of $\Sigma$.
\end{notation}

\begin{defn}
The set of \emph{observations} $O$ is any set of $\mathcal{L}_o$-formulas.
\end{defn}

\begin{exmp}
\begin{itemize}
\item The set $O_E=\{TastesHot(curry), \neg TastesHot(milk), IsWhite(milk)\}$ is the set of observations.
\item The set $O_I=\{TastesHot(milk)\}$ is the set of observations.
\item The set $O_{world}=\{TastesHot(curry), \neg TastesHot(milk), TastesHot(milk), IsWhite(milk)\}$ is the set of observations.
\end{itemize}
\end{exmp}

\begin{remark}
\begin{itemize}
\item The language of enquiry $\mathcal{L}$ is not powerful enough to express $O_{world}=\{\neg TastesHot(milk) \land InEngland(milk), TastesHot(milk) \land InIndia(milk)\}$
 as it does not have predicate symbols $InEngland$ and $InIndia$.
\item The inconsistency in the observations implies that there is no $\mathcal{L}$-structure $\mathcal{M}$ of $O_{world}$.
\item One of the central problems in ILP is the predicate invention - adding symbols $InEngland$ and $InIndia$ to the language of enquiry $\mathcal{L}$.
\end{itemize}
\end{remark}

\begin{defn}
The \emph{background knowledge} (or current induced theory) is a set $B$ of formulas in a hypothesis language $\mathcal{L}_h$.
\end{defn}

\begin{defn}
Given a set of observations $O$, we say that any $\mathcal{L}$-formula $Q$ \emph{divides} the models of $O$ into models of $Q \wedge O$ denoted $M(O \union {Q})$ and the models of $\neg Q \wedge O$ denoted $M(O \union {Q})$.
\end{defn}

\subsection{Ultimate problem of scientific discovery}
Statement: Given a language of enquiry $\mathcal{L}$, find the reality $\mathcal{M}$.

We do not consider the situation in which we may desire to learn approximate models of the reality. In general, depending on our language of enquiry, the reality may not exist, however our definition considers only the languages of enquiry with exactly one reality. Some realities, e.g. (regular expression) are learnable, others may be not. The ultimate problem consists of numerous incremental problems.

\subsection{Incremental problem of scientific discovery}
We would like to find the reality most efficiently, where the efficiency may be measured by some criteria - e.g. time and resources. The following simplified definition does not take into the account that some questions may be found more efficiently than others.

Set a probability measure $\mu:M(\emptyset) \to [0,1] \subset \mathcal{R}$ over hypothetical models of a language of enquiry $\mathcal{L}$. Given a background knowledge $B$, find a question $Q$ that minimizes the following equation: $|M(B \union {Q})-M(B \union {\neg Q})|$, i.e. a question that divides the models of $B$ most evenly.

\subsection{Definition of a problem of an induction}
A problem of induction is a functional problem whose input is a pentuple $P=\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle$ where $\mathcal{L}$ is a language of enquiry with its sublanguages $\mathcal{L}_o$, $\mathcal{L}_h$, observations $O$ and the background knowledge $B$.

Define an equivalence relation over the set (called the search space) $H$ of the sets of the $\mathcal{L}$-formulas: $\Sigma \sim \Gamma \iff K(\Sigma)=K(\Gamma)$ where $K$ is the Kolmogorov complexity with respect to the description language $\mathcal{L}$. Find a representative of the axiomatization with the least Kolmogorov complexity of the theory $\Sigma$ whose elementary class of models are precisely the models of $B \union O$. The solution  to a problem of induction (the output) is the representative $\Sigma$ called an induced theory from the background knowledge $B$ and the observations $O$.

\subsection{Definition of a scientific method}
A scientific method is an algorithm used to solve the ultimate problem of the scientific discovery:

0. Start with the emty theory $B_i=\emptyset$, set $i=0$.

1. If $B_i$ has the only model, then terminate, $B_i$ is a complete axiomatization of the reality of the language of enquiry.

2. Solve an incremental problem of scientific discovery by finding a question $Q$ given the current induced theory $B_i$.

3. Make an observation - ask an environment oracle a question $Q$.

4. Induce a more precise theory $B_{i+1}:=B \union \{Q\}$ if the oracle says that $Q$ is true, $B_{i+1}:=B \union \{\neg Q\}$ if the oracle says that $\neg Q$ is true.

5. Increment $i$ and start from step 1 again.

\subsection{Definition of an ILP problem}
Inductive logic programming problem is a problem of an induction where the language of enquiry $\mathcal{L}$ is the logic programming language (e.g. Prolog), the background knowledge and induced theory are finite.

In the context of ILP one divides the observations into positive examples $E^+$ where the reality is a model of instances of $E^+$, and negative examples $E^-$ - where the reality is a model of negated instances of $E^-$. The division is conceptually important as the clause that is not provable from the axioms is false as opposed to the monotonic logics whose theories can be incomplete.

Note: A constructed $\mathcal{L}$-formula in Prolog is a definite clause. 

\subsection{Definition of an ILP system}
An ILP system is any function $f:\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle \mapsto \Sigma \in H$ where the language of enquiry $\mathcal{L}$ is the logic programming language, the background theory $B$ and induced theories of $H$ are finite.

\subsection{Definition of a correct ILP system}
An ILP system is correct if it solves an ILP problem, i.e. its output is a solution to an ILP problem.

\subsection{Examples of ILP systems}
1. $f:\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle \to H$, $f:\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle \to \emptyset$ is an ILP system, however it is not correct since $M(O \union B) \not= M(\emptyset)$ for all sets of observations and the background knowledge.

2. Progol is an ILP system however it is not correct since it does not always return the induced theory of the least Kolmogorov complexity.

\subsection{Non-determinism, uncertainity, algorithm}

Since the Kolmogorov complexity is uncomputable no computable ILP system is correct. To develop a theory useful in real applications:

1. we need to consider approximations to the Kolmogorov complexity (which are computable) or allow that the induced theory $\Sigma$ is at most $p\%$ more complex from the simpliest induced theory.

2. allow that the induced theory $\Sigma$ found may not have exactly the models that are the models of $B \union O$. We need to think of some acceptable probability error $\epsilon>0$ where $\mu(M(\Sigma))-\mu(M(B \union O)) = \mu(M(\Sigma)  \triangle  M(B \union O)) < \epsilon$.

3. rather than giving a simple function as a black box which we could classify only based on its output from the input, we need a way to inspect the inside of such functions by dividing them into smaller algorithmic parts.
TODO: clarify.

\subsection{Hypothesis search in Progol}
