
\chapter{Background Theory}

\section{Prerequisites}
We assume the reader is familiar with basic concepts in several areas and consider the following references to be potentially useful:
\begin{enumerate}
\item Foundations of Inductive Logic Programming by Cheng and Wolf \cite{nienhuys1997foundations}
\item Model theory: An introduction by David Marker \cite{marker2002model}
\item An introduction to Kolmogorov complexity and its applications by Li and Vitanyi \cite{li2009introduction}
\item Real Analysis by H.L.Royden  \cite{royden1988real}, a reference on measure theory
\end{enumerate}
However, the usage of the concepts does not exceed its rudimentary application and the reader is encouraged to proceed further even if one may not be familiar with all the areas outlined, we suggest to use the literature when the need arises.
\section{A logic language}
\begin{defn}
An \emph{alphabet} is a set $\mathcal{A}$ of elements called \emph{letters} of an alphabet.
\end{defn}

\begin{defn}
A word over an alphabet $\mathcal{A}$ is a finite sequence (a string) of letters of the alphabet $\mathcal{A}$.
\end{defn}

\begin{defn}
A formal language $L$ is a set of words over the specified alphabet $\mathcal{A}$, i.e. $L \subseteq \powerset{\mathcal{A}}$.
\end{defn}

\begin{exmp}
Let $R$ be a regular expression $0001*$.
\begin{itemize}
\item The alphabet is the set $\mathcal{A}={\epsilon, 0,1,*}$. $0$, $1$, $*$ are called letters of $\mathcal{A}$.
\item The words over $\mathcal{A}$ are $000$.
\end{itemize}
\end{exmp}

Formal languages we are going to use are first-order languages:
\begin{defn}
A \emph{language} $L$ is a formal language given by the grammar of first-order logic and the additional data $\mathcal{L}$ called the signature of $L$:
\begin{enumerate}
\item a set $\mathcal{C}$ of letters called constant symbols,
\item a set $\mathcal{R}$ of letters called relation symbols and a positive integer (an arity) $n_R$ for each $R \in \mathcal{R}$,
\item a set $\mathcal{F}$ of letters called function symbols and a positive integer (an arity) $n_f$ for each $f \in \mathcal{F}$.
\end{enumerate}
\end{defn}

\begin{remark}
We will often talk about the language $L$ by referring to its signature $\mathcal{L}$. This should not cause a confusion as the signature $\mathcal{L}$ and the grammar of the first order logic uniquely determine the language $L$.
\end{remark}

\begin{exmp}
The language $\mathcal{L}_{EST}$ of elementary set theory is given by one constant symbol called an empty set, $\mathcal{C}=\{\emptyset\}$, one relation symbol called a set membership $\mathcal{R}=\{\in\}$ with a positive integer (arity) $n_{\in}=2$. The set of function symbols $\mathcal{F}$ are the set operations of a union, an intersection, each with an arity $2$.
\end{exmp}

\begin{defn}
Given a language $L$, $\phi$ is an $L$-formula iff $\phi \in L$.
\end{defn}

\begin{defn}
Let $\Sigma \subseteq L$, $\phi \in L$, then we say that $\Sigma$ \emph{equals} $\phi$ iff $\phi$ is a conjunction of the formulas $\psi \in \Sigma$, i.e. $\land \Sigma = \phi$. We abuse the symbol $=$ to denote the defined relation, writing $\Sigma=\phi$.
\end{defn}

\begin{remark}
If $\Sigma \subseteq L$, $\phi \in L$, $\Sigma=\phi$, then under the defined equality relation it is allowed to write both, $\Sigma \subseteq L$ and $\Sigma \in L$, for the later meaning $\Sigma = \phi \in L$.
\end{remark}

\section{Model theory\cite{marker2002model}}
Since the author to best of his knowledge considers the first-order model theory more developed as well as having better learning resources than model theory  of other logics, he will reason in first-order model-theory and will use the first-order definitions from Model Theory by Marker\cite{marker2002model}.

\begin{defn}
An \emph{$\mathcal{L}$-structure} (\emph{a model}) $\mathcal{M}$ is given by the following data:
\begin{enumerate}
\item a nonempty set $M$ called the universe, domain, or underlying set of $\mathcal{M}$;
\item a function $f^{\mathcal{M}} : M^{n_f} \to M$ for each $f \in F$;
\item a set $\mathcal{R}^{\mathcal{M}} \subseteq M^{n_R}$ for each $R \in \mathcal{R}$;
\item an element $c^\mathcal{M} \in M$ for each $c \in C$.
\end{enumerate}
We refer to $f^\mathcal{M}, R^\mathcal{M}, c^\mathcal{M}$ as the interpretations of the symbols $f ,R, c$.
\end{defn}

\begin{exmp}
$\mathcal{M}_{India}$ is a model given by
$M=\{milk^\mathcal{M}, curry^\mathcal{M}\}$,
$TastesHot=\{milk^\mathcal{M}, curry^\mathcal{M}\}$,
$IsWhite=\{milk^\mathcal{M}\}$ with a canonical mapping of constants from
 $C=\{milk, curry\}$ to $M$. Therefore $\mathcal{M}$ is a model of a formula
$\phi=TastesHot(milk) \wedge \neg IsWhite(curry)$ denoting by
$\mathcal{M} \models \phi$ reading "a model $\mathcal{M}$ entails a formula $\phi$".
\end{exmp}

\begin{remark}
An $L$-structure is an $\mathcal{L}$-structure iff $\mathcal{L}$ is a signature of a language $L$.
\end{remark}

\begin{defn}
Given a set $T$ of logic sentences and a model $\mathcal{M}$. If $\forall \phi \in T. \mathcal{M} \models \phi$, then $\mathcal{M}$ is \emph{a model of $T$} and $T$ is \emph{a theory of $\mathcal{M}$}.
\end{defn}

\begin{defn}
$A$ is an axiomatization of the theory $T$ iff $M(A)=M(T)$.
\end{defn}

\begin{remark}
Typically we put further restrictions on the properties of the axiomatization $A$ that do not hold for $T$. E.g. $A$ must have finitely many axioms for $T$ with infinitely many axioms.
\end{remark}

\begin{defn}
$A$ is an axiomatization of the theory $O$ from the theory $B$ iff $M(A \union B)=M(O)$.
\end{defn}

\section{Logics\cite{sep-logic-nonmonotonic}}
\begin{defn}
A (monotone) \emph{consequence operator}\cite{sergot2005knowledgeRepresentationLectureNotes} on a set $S$ is a function $cl:\powerset{S} \to \powerset{S}$ satisfying the following conditions for all sets $X, Y \subseteq S$:
\begin{enumerate}
\item $X \subseteq cl(X)$ inclusion,
\item $cl(cl(X))=cl(X)$ idempotence,
\item $X \subseteq{Y} \implies cl(X) \subseteq{Y}$ monotony.
\end{enumerate}
\end{defn}

\begin{defn}
A \emph{non-monotone consequence operator} on a set $S$ is a function
$cl:\powerset{S} \to \powerset{S}$ satisfying the condition of extensivity, idempotence and not satisfying the condition of monotony.
\end{defn}

We will reason about the models in the first-order logic since this is the language of the model theory. However, there are other logics, notably non-monotonic logics that are used in ILP.

\begin{defn}
A logic satisfies a \emph{monotony} property iff its consequence operator $\models$ is monotone, i.e.
if $\Gamma \models \phi$ and $\Gamma \subseteq \Delta$ then $\Delta \models \phi$. A logic that satisfies a monotony property is called monotonic, otherwise non-monotonic.
\end{defn}

\begin{exmp}
First-order logic is monotonic.
\end{exmp}

\section{Measure theory}
The following definition of a measure is adapted from Mathworld, Wolfram Web Resource\cite{wolframMathworldMeasure}.

\begin{defn}
A \emph{measure} is a real-valued function $\mu$ on a powerset $\powerset{S}$ of a set $S$ satisfying the following properties:
\begin{enumerate}
\item $\mu(\emptyset)=0$ and $\mu(S)=1$,
\item if $X \subseteq Y$ then $\mu(X) \le \mu(Y)$,
\item if $X_n, n=0, 1, 2, ...$ are pairwise disjoint, then
$\mu(\cup^\infty_{n=0} X_n)=\Sigma^\infty_{n=0}\mu(X_n)$
\end{enumerate}
\end{defn}

\begin{exmp}
Let $S$ be the set of possible throws of a die, $S=\{1, 2, 3, 4, 5, 6\}$. Define a measure $\mu:\powerset{S} \to \mathbb{R}$ over $\powerset{S}$ by $\mu(\{i\})=1/6, i \in S$. Then from the axioms of a measure it follows that $\mu(\emptyset)=0$, $\mu(S)=1$ and $\mu(\{1,2,5\})=1/2 \le \mu(\{1,2,3,4,5\})=5/6$. Such $\mu$ is called a probability measure and a triple $\langle \powerset{S}, S, \mu \rangle$ is called a probability space.
\end{exmp}

\begin{exmp}
Let $S$ be a real line $S=\mathbb{R}$, define function $\mu|_J$ on the set of intervals $J \subseteq \powerset{S}$ on the real line by $\mu|_J: (a, b) \mapsto b-a$, then $\mu|_J$ has a unique extension $\mu:S \to \mathbb{R}$ which is a measure function.
\end{exmp}

\section{Logic Programming}

\begin{defn}
A logic programming language has a \emph{negation on failure}\cite{clark1978negation} NoF property iff $\forall \phi, \forall \psi. \phi \not\vdash \psi \implies \phi \vdash \neg\psi$ where the $\vdash$ is a consequence operator.
\end{defn}

\begin{exmp}
In a logic programming language Prolog in a program $P$ with a single sentence:

$spicy(X) :- curry(X).$

the atom $spicy(X)$ is not provable, therefore $P \vdash \neg spicy(X)$ since Prolog has a NoF property.
\end{exmp}

Notice that NoF property on the consequence operator directly implies its monotony. Unless indicated the logic programming language we will reason will will have a NoF property.

The following definitions and concepts are adaptations from Dianhuan Lin's Master thesis\cite{lin2009efficient}.

\subsection{Basic concepts and notation of Logic Programming\cite{lin2009efficient}}

\begin{defn}
A term is a constant, variable, or the application of a function symbol to the appropriate number of terms. A ground term is a term not containing variables.
An atom is the application of a predicate symbol to the appropriate number of terms. A literal is an atom or the negation of an atom.
\end{defn}

\begin{defn}
A definite goal is a clause of the form
$\leftObjectImplies B_1 , ..., B_n$.
where $n > 0$ and each $B_i$ is an atom.
Each $B_i$ is called a subgoal of the goal.
\end{defn}

\begin{defn}
A definite clause is a clause of the form
$A \leftObjectImplies B_1 , ..., B_n$
which contains precisely one positive literal $A$.
$A$ is called the head and $B_1$ , ..., $B_n$ is called the body of the clause.
A Horn clause is either a definite clause or a definite goal.
A unit clause consists of a single literal.
\end{defn}

\begin{defn}
A logic program is a finite set of clauses representing their conjunction.
\end{defn}

\subsubsection{Resolution\cite{kimber2012learning}}
The resolution inference rule for a literal $\phi$ and clauses $\psi, \chi$ is
$(\phi \vee \psi) \wedge (\neg \phi \wedge \chi) \models \phi \vee \chi$.

\subsection{SLD-resolution\cite{lin2009efficient}}

\begin{defn}
A \emph{substitution} $\theta$ is a finite set of the form
$\{v_1\/t_1 , .., v_n \/t_n \}$,
where each $v_i$ is a variable, each $t_i$ is a
term distinct from $v_i$ and the variables $v_1 , .., v_n$ are distinct. Each element $v_i \/t_i$ is called a binding
for $v_i$. $\theta$ is called a ground substitution if the $t_i$ are all ground terms.
\end{defn}

\begin{defn}
An expression is either a term, a literal, or a conjunction or disjunction of literals.
A simple expression is a term or a literal.
\end{defn}

\begin{defn}
\emph{Unification.}
Let $\Sigma$ be a finite set of expressions. A substitution $\theta$ is called a unifier for $\Sigma$ if $\Sigma \theta$ is a singleton (a
set containing exactly one element). A unifier $\theta$ for $\Sigma$ is called a most general unifier (mgu) for $\Sigma$
if, for each unifier $\theta$ of $\Sigma$ , there exists a substitution $\Gamma$ such that $\theta = \Sigma \Gamma$
\end{defn}
Details about unification algorithm can be found in \cite{lloyd1987foundations}.

\begin{defn}
Let $G$ be $\leftObjectImplies A_1 , ..., A_m , ..., A_k$ and
$C$ be $A \leftObjectImplies B_1 , ..., B_q$.
Then $G'$ is derived from $G$ and
$C$ using mgu $\theta$ if the following conditions hold:
i) $A_m$ is an atom, called the selected atom, in $G$.
ii) $\theta$ is an mgu of $A_m$ and $A$.
iii) $G'$ is the goal $\leftObjectImplies (A_1 , ..., A_{m−1} , B_1, ..., B_q , A_{m+1} , ..., A_k )\theta$, $G'$ is
called a resolvent of $G$ and $C$.
\end{defn}
SLD-resolution stands for SL-resolution for Definite clauses. SL stands for Linear resolution with Selected function.

\begin{defn}
\emph{SLD-derivation}.
Let $\Sigma$ be a set of clauses and $C$ a clause. A derivation of $C$ from $\Sigma$ is a finite sequence of clauses
$R_1 , .., R_k = C$, such that each $R_i$ is either in $\Sigma$, or a resolvent of two clauses in $\{R_1 , ..., R_{i−1} \}$. If
such a derivation exists, $\Sigma \vdash_r C$. Thus $C$ can be derived from $\Sigma$.
\end{defn}

SLD-refutation is a special case of SLD-derivation which derives empty clause $\emptyclause$.

\section{Machine learning}
Machine learning a subdiscipline of AI that studies the construction of algorithms learning from the data.

\begin{defn}\cite{mitchell1997machine}
A computer program (a function, an algorithm, an ILP system) is said to \emph{learn (a function $f$) from experience} $E$ (input/output pairs of a function $f$)
with respect to some class of tasks $T$ (computing the output $f(I)$ from the input $I$) and performance measure $P$,
if its performance at tasks $T$ as measured by $P$ improves with the experience $E$.
\end{defn}

\begin{remark}
A function $f$ is a learning problem to be learnt by a computer program.
\end{remark}

\subsection{Types of problem}
Let $\mathcal{I}$, $\mathcal{O}$ be any sets of inputs and outputs.

\begin{defn}
A \emph{function problem} is a function $\mathcal{I} \to \mathcal{O}$.
\end{defn}

\begin{defn}
A \emph{decision problem} is a function problem $\mathcal{I} \to \{yes, no\}$.
\end{defn}

\begin{defn}
A mathematical object $f':\mathcal{I}' \to \mathcal{O}'$ \emph{solves a problem} $f:\mathcal{I} \to \mathcal{O}$ iff $\mathcal{I} \subseteq \mathcal{I}'$ and
$\forall I \in \mathcal{I}. f'(I)=f(I)$.
$f$ is called a \emph{subproblem} of $f'$.
\end{defn}

In machine learning a function problem $f:\mathcal{I} \to \mathcal{O}$ is called a \emph{learning problem} and a computer program $f'$ \emph{learns} a problem $f$ if $f'$ solves $f$. In ILP a learning problem is called an \emph{ILP task}, an input $I \in \mathcal{I}$ may consist of logical theories $B, E$ and an output of a logical theory $H$ explaining $E$ in terms of $B$, i.e. $E \subseteq Cn(B \cup H), false \not\in Cn(B \cup H)$ for some consequence operator $Cn$.

\section{Inductive inference}
The main strategy used by the field of inductive logic programming to solve a learning problem is to use the knowledge and techniques of inductive inference.
The author provides his own exposition of inductive inference not influenced by any specific sources apart from the general knowledge in the field inductive logic programming and other scientific disciplines.
\subsection{Languages in the theory of inductive inference}
\begin{defn}
Fix languages $L$, $L'$ such that $L \subseteq L'$. We say that $L$ is \emph{a sublanguage} of $L'$, $L'$ is \emph{a superlanguage} of $L$.
\end{defn}

\begin{exmp}
Let a language $L$ be given by its signature $\mathcal{L}$=$\mathcal{C} \union \mathcal{R} \union \mathcal{F}$ and a language $L'$ by its signature $\mathcal{L}'=\mathcal{C}' \union \mathcal{R}' \union \mathcal{F}'$.
If $\mathcal{C} \subseteq \mathcal{C}'$, $\mathcal{R} \subseteq \mathcal{R}'$, $\mathcal{F} \subseteq \mathcal{F}'$, then $L \subseteq L'$. $L$ is a sublanguage of $L'$, $L'$ is a superlanguage of $L$.
\end{exmp}

\begin{defn}
Given two languages $L_o$ called \emph{an observational language}, $L_h$ called a \emph{hypothesis language}, \emph{a language of enquiry} $L$ is a language that is a superlanguage of a hypothesis language and a superlanguage of an observational language, i.e. $L_h \union L_o \subseteq L$.
\end{defn}

\begin{remark}
The language of enquiry is a model-theoretic language by our definition. We will apply model theory with the first-order logic since many ILP systems are based on Prolog - the logic programming language with its roots in first-order logic. Therefore the boolean connectives $\neg$, $\land$, $\lor$, $\implies$ are implicitly added to the language of enquiry $L$ and the formation rules of first-order logic are assumed. In generality one could devise a theory working in any formal language.
\end{remark}

\begin{exmp}
\begin{itemize}
\item The language of enquiry $L$ is given by $\mathcal{C}_o=\{milk, curry, rice\}$,$\mathcal{R}_o=\{TastesHot, IsWhite, ContainsSpice, ContainsSugar\}$, $\mathcal{F}_o=\{\}$.
\item Let the observational language $L_o$ be $\mathcal{C}_o=\{milk, curry, rice\}$,\\ $\mathcal{R}_o=\{TastesHot, IsWhite\}$, $\mathcal{F}_o=\{\}$
\item Let the hypothesis language $L_h$ be $\mathcal{C}_h=\{milk, curry, rice\}$,\\ $\mathcal{R}_h=\{TastesHot, IsWhite, ContainsSpice\}$, $\mathcal{F}_h=\{\}$.
\item $L_h$-sentences are $\forall x. TastesHot(x) \implies ContainsSpice(x)$,\\ $\forall x. IsWhite(x) \lor TastesHot(x)$.
\end{itemize}
\end{exmp}

\subsection{Inductive inference concepts}

\begin{defn}
\emph{An environment} $\mathcal{E}$ is a quadruple $\langle L, L_o, \mathtt{O}, \mathcal{M} \rangle$ where $L$ is a language of enquiry, $L_o$ its observational language, $\mathtt{O}:L_o \to \{true, false\}$ \emph{an environment oracle}, $\mathcal{M}$ an $\mathcal{L}$-structure called \emph{the reality of $L$} satisfying:
$\forall \phi \in L_o. \mathcal{M} \models \phi \iff \mathtt{O}(\phi)=true$.
\end{defn}

\begin{exmp}
\begin{itemize}
\item In an environment England, the reality $\mathcal{M}_E$ is a model of the sentences\\ $\Sigma_E=\{TastesHot(curry), \neg TastesHot(milk)\}$.
\item In India, the reality $\mathcal{M}_I$ is a model of the sentences\\ $\Sigma_I=\{TastesHot(curry), TastesHot(milk)\}$.
\item Both $\mathcal{M}_E$, $\mathcal{•}l{M}_I$ are models of $L$.
\item $\mathcal{E}_E=\langle L, L_o, \mathtt{O}_E, \mathcal{M}_E \rangle$ and
$\mathcal{E}_I=\langle L_, L_o, \mathtt{O}_I, \mathcal{E}_I \rangle$ are two environments distinguished by their reality, consequently by their environment oracle function as well.
\end{itemize}
\end{exmp}

\begin{remark}
In generality one may develop a theory in which an environment may have multiple or no realities and an environment oracle returns a probability of $\phi$ being true in a possible reality or randomly chooses a reality $\mathcal{M}$ between the realities, then acts as an environment oracle with a single reality $\mathcal{M}$. This may be useful when the language of enquiry has a limited expressivity as demonstrated later.
\end{remark}

\begin{defn}
The set of \emph{observations} $O$ with respect to an observational language $L_o$ is any set of $L_o$-formulas.
\end{defn}

\begin{exmp}
\begin{itemize}
\item The set $O_E=\{TastesHot(curry), \neg TastesHot(milk), IsWhite(milk)\}$ is the set of observations.
\item The set $O_I=\{TastesHot(milk)\}$ is the set of observations.
\item The set $O_{world}=\{TastesHot(curry), \neg TastesHot(milk), TastesHot(milk), \\
IsWhite(milk)\}$ is the set of observations.
\end{itemize}
\end{exmp}

\begin{remark}
\begin{itemize}
\item The language of enquiry $L$ is not powerful enough to express $O_{world}=\{\neg TastesHot(milk) \land InEngland(milk), TastesHot(milk) \land InIndia(milk)\}$
 as it does not have predicate symbols $InEngland$ and $InIndia$ in its signature $\mathcal{L}$.
\item The inconsistency in the observations implies that there is no $\mathcal{L}$-structure $\mathcal{M}$ of $O_{world}$.
\item One of the central problems in ILP is the predicate invention - adding symbols $InEngland$ and $InIndia$ to the signature $\mathcal{L}$ of the language of enquiry.
\end{itemize}
\end{remark}

\begin{defn}
The \emph{background knowledge} or a current induced theory is a set $B$ of formulas in the language of enquiry $L$.
\end{defn}

\begin{remark}
In the definition \ref{scientific_method} of a scientific method, a special case of the background knowledge is considered where $B$ can consist of only the previously made hypotheses, thus $B \subseteq L_h$.
\end{remark}

\begin{note}
Given a set of observations $O$, an $L$-formula $Q$ \emph{divides} the models of $O$ into models of $O \wedge \{Q\}$ denoted $M(O \union \{Q\})$ and the models of $\{\neg Q\} \wedge O$ denoted $M(O \union \{Q\})$. That is $\forall \mathcal{M} \in M(O \union \{Q\}). \mathcal{M} \models O \union \{Q\}$,
$\forall \mathcal{M} \in M(O \union \{\neg Q\}). \mathcal{M} \models O \union \{\neg Q\}$.
\end{note}

\begin{defn}
\emph{A hypotheses space} or a search space is the set $\mathcal{H}$ of all consistent subsets of a hypothesis language $L_h$. A formula $H \in \mathcal{H}$ is called \emph{a hypothesis}.
\end{defn}

\subsection{Ultimate problem of scientific discovery}
Statement: Given an environment oracle $\mathtt{O}$ of an environment $\mathcal{E}=\langle L, L_o, \mathtt{O}, \mathcal{M} \rangle$ find the reality $\mathcal{M}$.

We do not yet consider the situation in which we may desire to learn approximate models of the reality. In general, depending on an environment, the reality may not exist, however our definition considers only the environments with exactly one reality. Some realities, e.g. (regular expression) are learnable, others may be not. The ultimate problem consists of numerous incremental problems.

\subsection{Incremental problem of scientific discovery}
We would like to find the reality most efficiently, where the efficiency may be measured by some criteria - e.g. time and resources. The following simplified definition does not take into account that some questions may be found more efficiently than others.

Set a probability measure $\mu:M(\emptyset) \to [0,1] \subset \mathbb{R}$ over models of a language of enquiry $L$. Given a background knowledge $B$, find a question $Q$ that minimizes the following equation: $|M(B \union {Q})-M(B \union {\neg Q})|$, i.e. a question that divides the models of $B$ most evenly.

\subsubsection{Postulates of inductive inference}
\begin{defn}
The first postulate of inductive inference (Church-Turing thesis). The reality
$\mathcal{M}$ of the environment
$\mathcal{E}=\langle L, L_o, \mathtt{O}, \mathcal{M} \rangle$
is Turing-machine computable.
\end{defn}

\begin{defn}
The second postulate of inductive inference (Determinism). The oracle $\mathtt{O}$ of the environment $\mathcal{E}=\langle L, L_o, \mathtt{O}, \mathcal{M} \rangle$
is deterministic, i.e. $\forall x \in L_o. \#\mathtt{O}(x)=1$.
\end{defn}

\begin{defn}
The third postulate of inductive inference (Occam's Razor). The reality with its full axiomatization of less Kolmogorov complexity is more probable than a reality with its full axiomatization of greater Kolmogorov complexity.
\end{defn}

\subsection{Problem of induction - model theoretic setting}
A problem of induction is a functional problem whose input is a pentuple $P=\langle L, L_o, L_h, O, B\rangle$ where $L$ is a language of enquiry with its sublanguages $L_o$, $L_h$, observations $O$ and the background knowledge $B$. A solution to a problem of induction (the output) is a theory $A$ meeting the entailment criterion
$O \subseteq cl(A \union B )$ where the consequence operator $cl$ is specified  in a more specific version of a problem.
$A$ is called \emph{an induced theory} from the background knowledge $B$ and the observations $O$, or an axiomatization of a theory $O$ given $B$.

\subsection{Scientific method}
\label{scientific_method}
A scientific method is an algorithm used to solve the ultimate problem of the scientific discovery:

0. Start with the empty theory $B_i=\emptyset$, set $i=0$.

1. If $B_i$ has the only model, then terminate, $B_i$ is a complete axiomatization of the reality of an environment.

2. Solve an incremental problem of scientific discovery by finding a question $Q$ given the current induced theory $B_i$.

3. Make an observation - ask an environment oracle $\mathtt{O}$ a question $Q$.

4. Induce a more precise theory $B_{i+1}:=B \union \{Q\}$ if the oracle says that $Q$ is true, $B_{i+1}:=B \union \{\neg Q\}$ if the oracle says that $\neg Q$ is true.

5. Increment $i$ and start from step 1 again.

\section{Inductive logic programming}

\subsection{ILP problem}
The following definitions are inspired (but not adapted) from Inductive Logic Programming as Abductive Search paper by Corapi et al.\cite{corapi2010inductive}.

\begin{defn}\cite{corapi2010inductive}
An \emph{inductive logic programming problem} is a pentuple $\langle O, B, \mathcal{O}, \mathcal{B}, \mathcal{H} \rangle$ where $O$ is a set of sentences called observations, $\mathcal{O} \subseteq L_o$ a subset of an implicit language $L_o$ called an observational language, $B$ is a set of sentences called the background knowledge, $\mathcal{B} \subseteq L$ is a subset of an implicit language $L$ called the language of enquiry, and $\mathcal{H}$ called a bias is a subset of an implicit language $L_h$ called the hypotheses language such that $L_h \subseteq L$, $L_o \subseteq L$, $O \in \mathcal{O}$, $B \in \mathcal{B}$.

A \emph{solution to an ILP problem} is a set of sentences $H \in \mathcal{H}$ called a hypothesis satisflying the conditions
i) consistency $false \not \in cl(O \union B \union H)$,
ii) sufficiency $O \subseteq cl(B \union H)$.
\end{defn}

In ILP often the language of enquiry $L$ is the logic programming language (e.g. Prolog). A constructed $L$-formula in Prolog is a definite clause. The background knowledge and induced theory are finite.

In the context of ILP one divides the observations into positive examples $E^+$ where the reality is a model of instances of $E^+$, and negative examples $E^-$ - where the reality is a model of negated instances of $E^-$. If the logic programming language is non-monotone, then the consequence operator $cl$ does not satisfly the monotone condition and the division of the examples is conceptually important. The clause that is not provable from the axioms is false as opposed to the monotonic logics whose theories can be incomplete.

\subsection{ILP concepts}

\subsubsection{Duce's rules for inductive inference\cite{muggleton1995inverse}}
Duce had six inductive inference rules. Four of these were concerned with definite clause propositional logic. In the following description of the inference rules
lower-case letters represent propositional variables and upper-case letters represent conjunctions of propositional variables.

Absorption: $A \wedge B \objectImplies p, A \objectImplies q \models B \wedge q \objectImplies p, A \objectImplies q$,

Identification: $A \wedge B \objectImplies p, q \wedge A \objectImplies p
\models B \objectImplies q, A \wedge q \objectImplies p$,

Intra-construction: $A \wedge B \objectImplies p, A \wedge C \objectImplies p
\models B \objectImplies q, A \wedge q \objectImplies p, C \objectImplies q$

Inter-construction: $A \wedge B \objectImplies p, A \wedge C \objectImplies p
\models r \wedge B \objectImplies p, A \objectImplies r, r \wedge C \objectImplies q$

\subsubsection{Inverse Entailment}
Inverse Entailment is a correspondence between an induction and a deduction:
\begin{thm}\cite{kimber2012learning}
Let $B$ be a Horn program, and let $h$ and $e$ be
Horn clauses. Then $B \wedge h \models e \iff B \wedge \neg e \models \neg h$.
\end{thm}

\subsection{Language bias}
The search space of all possible hypotheses $H \in \mathcal{H}$ is restricted by a language bias $\mathcal{H}$. The main types of the biases in ILP are:
\begin{itemize}
\item mode declarations,
\item determinations,
\item metaconstrains,
\item induction and production field.
\end{itemize}

\subsubsection{Mode declarations}\label{background_mode_declarations}
\paragraph{Mode declaration in Progol}
Muggleton defines the hypothesis bias $\mathcal{H}$ with mode language and mode declarations.
\begin{defn}\cite{muggleton1995inverse}
A \emph{mode declaration} has either the form
\tc{modeh(n,atom)} or \tc{modeb(n,atom)} where $n$, the \emph{recall}, is either an integer, $n \ge 1$,
or \tc{*} and atom is a ground atom. Terms in the atom are either normal or placemarker. A normal term is either a constant or a function symbol followed by a
bracketed tuple of terms. A placemarker is either \tc{+type}, \tc{-type} or \tc{\#type}, where
type is a constant. If $m$ is a mode declaration then $a(m)$ denotes the atom of m
with place-markers replaced by distinct variables. The sign of $m$ is positive if $m$
is a \tc{modeh} and negative if $m$ is a \tc{modeb}.
\end{defn}

\begin{exmp}\cite{muggleton1995inverse}
\begin{lstlisting}
modeh(1,plus(+int,+int,-int))
modeb(*,append(-list,+list,+list)
modeb(1,append(+list,[+any],-list))
modeb(4,(+int ? #int))
\end{lstlisting}
\end{exmp}

\begin{remark}\cite{muggleton1995inverse}
The recall is used to bound the number of alternative solutions for instantiating
the atom. For simplicity, we assume in the following that all the modes have the
recall \tc{*}, meaning all solutions. The following defines when a clause is within Progol's definite mode language $L$.
\end{remark}

\begin{defn}\label{definition_definite_mode_language}\cite{muggleton1995inverse}
Let $C$ be a definite clause with a
defined total ordering over the literals and $M$ be a set of mode declarations.
$C = h \leftObjectImplies b1, ..., bn$ is in the \emph{definite mode language} $L(M)$ iff 1) $h$ is the atom
of a \tc{modeh} declaration in $M$ with every place-marker \tc{+type} and \tc{-type} replaced by
variables and every place-marker \tc{\#type} replaced by a ground term and 2) every
atom $b_i$ in the body of $C$ is the atom of a \tc{modeb} declaration in $M$ with every
place-marker \tc{+type} and \tc{-type} replaced by variables and every place-marker \tc{\#type}
replaced by a ground term and 3) every variable of \tc{+type} in any atom $b_i$ is either
of \tc{+type} in $h$ or of \tc{-type} in some atom $b_j$, $1 \le j < i$.
\end{defn}

\subsubsection{Determinations}
Some ILP systems like Aleph in addition to mode declarations use determination statements\cite{aleph2007} to impose a finer control on a search space.

\begin{defn}\cite{aleph2007}
A \emph{determination} is a logic statement of the form\\
\tc{determination(TargetName/TargetArity, BackgroundName/BackgroundArity).}
where \tc{TargetName}, \tc{BackgroundName} are predicate symbols and
\tc{TargetArity}, \tc{BackgroundArity} are their arities (non-negative integers) respectively.
\end{defn}

\begin{defn}\label{definition_determination_language}
Let $D=\langle d_1(tn_1/ta_1, bn_1/ba_1), ..., d_n(tn_n/ta_n, bn_n/ba_n)\ \rangle$ be an ordered list of determinations. A formula $H=h \leftObjectImplies b_1, ..., b_m$ is in \emph{determination language} $L(D)$ iff
1) $D \not=\emptyset$, 2) $tn_1=h$, 3) $\forall b_i (1 \ge i \ge m) \exists j (1 \ge j \ge n) bn_j=b_i$.
\end{defn}

\begin{exmp}
Let $D$ be the list of the determinations
\begin{lstlisting}
:− determination(man/ 1, male/1).
:− determination(man/ 1, bridegroom/1).
:− determination(woman/ 1, female/1).
\end{lstlisting}
Let the hypotheses be

\begin{minipage}[t]{.50\textwidth}
\begin{lstlisting}
H1=man(X) :- male(X).
H2=man(X) :- bridegroom(X).
H3=man(X) :- male(X), bridegroom(X).
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{.50\textwidth}
\begin{lstlisting}
H4=woman(X) :- female(X).
H5=man(X) :- male(X), female(X).
\end{lstlisting}
\end{minipage}

Then for the hypothesis bias (determination language) $\mathcal{H}=L(D)$,
$\{H_1, H_2, H_3, H_4, H_5\} \cap \mathcal{H}=\{H_1, H_2, H_3\}$.
\end{exmp}

\subsubsection{Metaconstraints}
Metaconstraints specify the additional constraints on the hypothesis space $\mathcal{H}$ not definable with the mode declarations and determinations.
An example being a metaconstraint $Cond$ specifying the maximum number of the literals allowed in a formula $H \in L$.

\begin{defn}
A \emph{metaconstraint} on a language $L$ is any formula $Cond$ defining some subset $L(Cond)$ (\emph{metaconstraint language}) of the language $L$, i.e. $L(Cond)=\{x \in L | Cond(x)\}\subseteq L$.
\end{defn}

Each ILP system has its own set of allowed metaconstrains. An interested reader is encouraged to consult their respective manuals.
\begin{exmp}
In Toplog \tc{set(maximum\_literals\_in\_hypothesis, 5)} defines clausal theories whose clauses consist of at most 5 literals.
In Imparo \tc{set\_max\_clauses(2)} defines clausal theories with at most 2 clauses.
In Tal \tc{option(max\_body\_literals, 5)} defines definite theories whose clauses have at most 5 positive literals.
\end{exmp}

\begin{proposition}
Let $Cond_1, Cond_2$ be metaconstrains on a language $L$.
Then the following are metaconstrains on the language $L$:
$\neg Cond_1$, $Cond_1 \land Cond_2$, $Cond_1 \lor Cond_2$.
\end{proposition}

\begin{proof}
Follows trivially from the definition of a metaconstraint.
\end{proof}

\begin{remark}
Therefore we will often think of a set of the metaconstraints to be a metaconstraint created from their conjunction.
\end{remark}

\begin{defn}
Let $C=\{Cond_1, ..., Cond_n\}$ be a set of metaconstraints defining subsets of the language $L$.
A \emph{metaconstraint language} $L(C)$ is the intersection of the subsets defined by $Cond_1, ..., Cond_n$,
i.e. $L(C)=\{x \in L | Cond_1(x) \land ... \land Cond_n(x) \}$.
\end{defn}

\subsubsection{Induction and production field}
Inoue defines a production field as another form of a language bias.

\begin{defn}\cite{inoue2004induction}
A \emph{production field} $\mathcal{P}$ is a represented by a pair,
$\langle Lit, Cond\rangle$, where $Lit$
(\emph{the characteristic literals} of $\mathcal{P}$) is a subset of literals in a language $L$ and is closed under instantiation (that is, if a literal containing variables is in $Lit$, then all its instances are also in $Lit$), and $Cond$ is a certain condition to be satisfied. 
A clause $C$ is said to belong to a production field $\mathcal{P} = \langle Lit, Cond \rangle$ if every literal in $C$ belongs to $Lit$ and $C$ satisfies $Cond$.
\end{defn}

A special case of the production field when the set of the conditions $Cond$ is empty is called an induction field.

\begin{defn}\cite{yamamoto2012inverse}\label{induction_field_definition}
An \emph{induction field}, denoted by $\mathcal{I}_H = \langle Lit \rangle$,
where $Lit$ is a finite
set of literals to appear in ground hypotheses.
A ground hypothesis $H_g$ \emph{belongs to} $\mathcal{I}_H$ if
every literal in $H_g$ is included in $L$.
Given an induction field $\mathcal{I}_H = \langle Lit \rangle$, $Taut(\mathcal{I}_H)$ is defined
as the set of tautologies $\{\neg A \land A | A \in Lit, \neg A \in Lit\}$.
\end{defn}

\subsubsection{Correspondence between mode declarations, determinations, meta constraints and production field}
A production field is clearly a generalization of mode declarations, determinations and metaconstraints.

\begin{proposition}\label{md_d_pf_correspondence_proposition}
Let $M$ be a set of mode declarations, $D$ an ordered list of determinations, $C$ a set of the metaconstraints. Then there exist production fields
$\mathcal{P}_M$,
$\mathcal{P}_D$,
$\mathcal{P}_{M,D}$
such that for any clause $H \in L$:
1) $H \in L(M) \iff H \in \mathcal{P}_M$,
2) $H \in L(D) \iff H \in \mathcal{P}_D$,
3) $H \in L(M) \cap L(D) \iff H \in \mathcal{P}_{M,D}$.
\end{proposition}
The following proof is non-constructive.
\begin{proof}
Let $L$ be a logic language. By the definitios of definite mode language\ref{definition_definite_mode_language} and determination language\ref{definition_determination_language}, $L(M)$ and $L(D)$ are definable.
Let $Cond_M$, $Cond_D$ be defining formulas for $L(M)$ and $L(D)$, i.e. $L(M)=\{x \in L | Cond_M(x)\}$, $L(D)=\{x \in L | Cond_D(x)\}$.
Let $Cond_{M,D}=Cond_M \land Cond_D$. Let $Lit$ be a set of all literals of $L$ and let
$Lit_M=L(M) \cap Lit$,
$Lit_D=L(D) \cap Lit$,
$Lit_{M,D}=Lit_M \cap Lit_D$.
Then for every clause $H \in L$ the production fields
$\mathcal{P}_M=\langle Lit_M, Cond_M \rangle$,
$\mathcal{P}_D=\langle Lit_D, Cond_D \rangle$,
$\mathcal{P}_{M,D}=\langle Lit_{M,D}, Cond_{M,D} \rangle$
satisfy the conditions 1, 2, 3 respectively trivially.
\end{proof}

\begin{proposition}
Let $\mathcal{P}$ be any production field, $C$ a set of metaconstraints.
Then there exists a production field $\mathcal{P}_C$ such that
$\mathcal{P}_C=\mathcal{P} \cap L(C)$, i.e.
$\forall H \in L. H \in \mathcal{P}_C \iff (H \in \mathcal{P}) \land (H \in L(C))$.
\end{proposition}
\begin{proof}
Let $\mathcal{P}=\langle Lit, Cond \rangle$,
$C=\{Cond_1, ..., Cond_n\}$.
Define $\mathcal{P}_C=\langle Lit, Cond \land Cond_1 \land ... \land Cond_n \rangle$. Take an arbitrary $H \in L$. Denote $Lit(H)$ to be the set of the literals of the clausal form of $H$.
Then $H \in \mathcal{P}_C$ iff $Lit(H) \subseteq Lit$ and
$(Cond \land Cond_1 \land ... \land Cond_n)(H)$ iff
$Lit(H) \subseteq Lit$ and $Cond(H)$ and $Cond_1(H) \land ... \land Cond_n(H)$
iff $H \in \langle Lit, Cond \rangle = \mathcal{P}$ and $H \in L(Cond_1 \land ... \land Cond_n) = L(C)$. Thus $\mathcal{P}_C=\mathcal{P} \cap L(C)$.
\end{proof}

The converse depends on the possible meta constraints. If one allowed a meta constraint for every possible definable condition $Cond$ then one could express any production field $\mathcal{P}=\langle Lit, Cond \rangle$ with mode declarations, determinations and meta constraints.

\section{ILP systems}

\subsection{ILP system definition}
An ILP system is a function
$f:\langle O, B, \mathcal{O}, \mathcal{B}, \mathcal{H}\rangle \mapsto H$ where the pentuple $\langle O, B, \mathcal{O}, \mathcal{B}, \mathcal{H}\rangle$
is an inductive logic programming problem and a hypothesis $H \in \mathcal{H}$ is a solution to an ILP problem.

We give an overview of ILP systems taking different approaches to an ILP problem. The definition of an ILP system we gave is not sufficient for making meaningful comparisons based on the usefulness of these systems in real applications nor from a theoretical viewpoint. The aim of familiarizing with these systems is to find the intuition on the key properties of ILP systems that should be formalized in order to benefit from the mathematical rigour required for reasoning about ILP systems.

\subsection{TDHD framework\cite{muggleton2008toplog}}
A set of clauses $\top$ called a top theory is required on the input in addition to observations and the background knowledge by the systems solving the problem of induction with a TDHD framework. Consequently the search space is restricted by requiring that each hypothetised clause of a hypothesis $H$ must be entailed by the top theory $\top$.

\subsubsection{Toplog\cite{muggleton2008toplog}}
Toplog is an ILP system implementing TDHD framework. The algorithm used to construct the hypothesis uses Mode Directed Inverse Entailment and follows the steps:
\begin{itemize}
\item construct the top theory $\top$,
\item hypothesis derivation: derive refutations of $\neg e$ from $B$ and $\top$, derive a clause $h$ from the refutations, add $h$ to $H$.
\item coverage computation: which examples $E^+$ and $E^-$ are entailed by $h \in H$.
\item hypothesis construction: select $H' \subseteq H$ maximizing the score function - e.g. compression, coverage, accuracy,
\end{itemize}

\subsubsection{MC-Toplog\cite{muggleton2012mc}}
MC-Toplog is a sequel of Toplog, derives hypotheses like Toplog, in addition allows multiple clauses in a hypothesis. Its extended framework TDTcD restricts a hypotheses space to clauses entailing generalization of multiple examples (co-generalization) as opposesed to Toplog that could generalizing only a single example.

\subsection{Induction on Failure framework\cite{kimber2012learning}}
Induction on Failure framework (IoF) is a method for deriving a hypothesis $H$ where a single clause $h \in H$ does not necessarily need to explain an example $e \in E$, but an example can be explained by multiple clauses. Such a search space is called a connected theory.
\begin{defn}
A connected theory $T$ for a ground Horn clause $e$ and a Horn theory $B$ is a set of clauses that can be partitioned into sets $T_1, ..., T_n$ so that
(i) $B \union T_1^+ \models e_{head}$,
(ii) $\forall i \in \{1, ..., n-1\}. B \union e_{body} \union T_{i+1}^+ \models T_i^-$,
(iii) $B \union e_{body} \models T_n^-$,
(iv) $B \union T \not\models \square$.
\end{defn}

\subsubsection{Imparo\cite{kimber2012learning}}
Imparo is an ILP system based on a general IoF theoretical framework with the following algorithm:
\begin{itemize}
\item 1: select an example $E$ from the set of positive examples $E_{pos}$,
\item compute the most specific connected theory for an example $E$ and the background knowledge $B$,
\item search the lattice of sets of clauses subsuming the connected theory and choose the hypothesis $H$ with the highest score according to the score function such that $H \models E$,
\item add $H$ to $B$,
\item remove all $E' \in E_{pos}$ implied by new $B$, $B \models E'$.
\item if $E_{pos} = \emptyset$ finish, otherwise go to 1.
\end{itemize}

\subsection{Meta-Interpretive Learning framework\cite{muggleton2014meta}}
Meta-Interpretive Learning (MIL) framework solves a problem of induction in a variant of the normal setting for ILP.

\begin{defn}
\emph{(Meta-Interpretive Learning setting)} A Meta-Interpretive Learning (MIL)
problem consists of $Input = \langle B, E \rangle$ and $Output =H$ where the background knowledge
$B = B_M \union B_A$ . $B_M$ is a definite logic program representing a meta-interpreter and $B_A$ and
$H$ are ground definite Higher-Order Datalog programs consisting of positive unit clauses.
The predicate symbol constants in $B_A$ and $H$ are represented by Skolem constants. The examples are $E = E^+ , E^−$ where $E^+$ is a ground logic program consisting of positive unit
clauses and $E^−$ is a ground logic program consisting of negative unit clauses. The $Input$ and
$Output$ are such that $B, H \models E^+$ and for all $e^-$ in $E^-$, $B, H \not\models e^-$.
\end{defn}

\begin{defn}
\emph{(Meta-interpretive learner)} Let $\mathcal{H}_{B,E}$ represent the complete set of abductive
hypotheses $H$ for the MIL setting of the previous definition. Algorithm $A$ is said to be a Meta-interpretive learner iff for all $B$, $E$ such that $H$ is the output of Algorithm $A$ given $B$ and $E$
as inputs, it is the case that $H \in \mathcal{H}_{B,E}$.
\end{defn}

Capability of predicate invention.
Based on T-directed framework. Unique T element. Unique bottom element.

\subsubsection{Metagol}
Metagol is an implementation of an MIL framework that finds the hypothesis of a minimal length within its search space.

\subsection{Aleph}
In the Aleph manual\cite{aleph2007} a reader would find the description of the basic algorithm:
\begin{enumerate}
\item \emph{Select example.} Select an example to be generalised. If none exist, stop, otherwise proceed to the next step.
\item \emph{Build most-specific-clause.} Construct the most specific clause that entails the example selected, and is within language restrictions provided. This is usually a definite clause with many literals, and is called the "bottom clause." This step is sometimes called the "saturation" step. Details of constructing the bottom clause can be found in Stephen Muggleton's 1995 paper: Inverse Entailment and Progol\cite{muggleton1995inverse}.
\item \emph{Search.} Find a clause more general than the bottom clause. This is done by searching for some subset of the literals in the bottom clause that has the "best" score. Two points should be noted. First, confining the search to subsets of the bottom clause does not produce all the clauses more general than it, but is good enough for this thumbnail sketch. Second, the exact nature of the score of a clause is not really important here. This step is sometimes called the "reduction" step.
\item \emph{Remove redundant.} The clause with the best score is added to the current theory, and all examples made redundant are removed. This step is sometimes called the "cover removal" step. Note here that the best clause may make clauses other than the examples redundant. Again, this is ignored here. Return to Step 1.
\end{enumerate}

\subsection{Other systems}
Some other ILP systems include Aleph, Golem, Progol,
Spectre, EBG, Alecto, FOIL, Linus, Marvin, Mis, Confucius, Quinlan, ASPAL, Hyper, Tal, Tilde, Hail, CF-induction method.

\section{Inverse subsumption for complete explanatory induction\cite{yamamoto2012inverse}}
This section paraphrases work by Yamamoto et al. \cite{yamamoto2012inverse} unless stated otherwise. For more examples, explanations and results, due to space constrains the reader is encouraged to consult the original source shall the need arise. The main result is a complete algorithm \emph{inverse subsumption with minimal complements} for deriving a hypothesis wrt $B, E$ by the method of the inverse subsumption in lieu of the antientailment.

\subsection{Preliminaries}\label{subsec:preliminaries}
\begin{defn}
Let $C$, $D$ be clauses, then $C$ \emph{(theta-)subsumes} $D$ iff there is a substitution $\theta$ such that $C \theta \subseteq D$. We denote the relation by $C \subsumes D$.
\end{defn}

\begin{defn}\label{definition_theory_subsumption}
Let $S$ and $T$ be two clausal theories. Then, $S$ \emph{(theory-)subsumes} $T$, denoted by $S \subsumes T$, if for any clause $D \in T$, there is a clause $C \in S$ such
that $C \subsumes D$. We denote by the inverse relation $\subsumed$ of the (theory-) subsumption, called \emph{anti-subsumption}.
\end{defn}

\begin{defn}
Let $S$ be a ground clausal theory $\{C_1, C_2,... ,C_n\}$ where each clause $C_i$ $(1 \le i \le n)= l_{i,1} \lor l_{i,2} \lor... \lor l_{i,m_i}$.
The \emph{complement} of $S$ is defined as
\\$\bar{S} = \{\neg l_{1,k_1} \lor \neg l_{2,k_2} \lor... \lor \neg l_{n,k_n} |
1 \le k_1 \le m_1 , 1 \le k_2 \le m_2,..., 1 \le k_n \le m_n\}$.
In case that $S$ is empty, $\bar{S}$ is defined as the set $\{\bot\}$ where $\bot$ is the empty clause.
\end{defn}

\begin{remark}
$S$ is a CNF formula such that $\bar{S} \equiv \neg S$.
\end{remark}

\begin{defn}
\emph{$\tau(S)$} denotes the clausal theory obtained by removing all the tautologies from a logic theory $S$.
\end{defn}

\begin{defn}
\emph{$\mu(S)$} denotes the clausal theory obtained by removing from $S$ all clauses that are properly subsumed by clauses in a logic theory $S$.
The \emph{minimal complement} of $S$ is $M(S)=\mu(\bar{S})$.
\end{defn}

\begin{exmp}
Let $S=\{a \lor b, b \lor c, \neg c\}$. Then
$\bar{S}=\{\neg a \lor \neg b \lor c, \neg a \lor \neg c \lor c, \neg b \lor c, \neg b \lor \neg c \lor c\}$.
$M(S)=\{\neg a \lor \neg c \lor c, \neg b \lor c\}$,
$\tau(\bar{S})=\{\neg a \lor \neg b \lor c, \neg b \lor c\}$.
\end{exmp}

The inverse subsumption with minimal complements computes a hypothesis $H$ wrt to the bridge theory $F$ and the induction field $\mathcal{I}_H$.

\begin{defn}\label{definition_bridge_theory}
Let $B$ and $E$ be a background theory and examples, respectively.
Let $F$ be a ground clausal theory. Then $F$ is a \emph{bridge theory} wrt $B$ and $E$ if
$B \land \bar{E} \models F$ holds. If no confusion arises, a bridge theory wrt $B$ and $E$ will simply be called a bridge theory.
\end{defn}

The author finds the notion of the maximal bridge theory useful in later chapters.
\begin{defn}\label{maximal_bridge_theory_definition}
A \emph{maximal bridge theory} wrt $B$ and $E$ is a theory $F = B \cup \bar{E}$.
\end{defn}
\begin{remark}
Every bridge theory is entailed by a maximal bridge theory.
\end{remark}

We next define the target hypotheses using the notion of an induction field $\mathcal{I}_H$\ref{induction_field_definition}, together with a bridge theory $F$ as follows:

\begin{defn}\label{definition_hypothesis_wrt_induction_field_bridge_theory}
Let $H$ be a hypothesis. $H$ is a \emph{hypothesis wrt $\mathcal{I}_H$ and $F$} if there is a ground hypothesis $H_g$ such that $H_g$ consists of instances from $H$,
$F \models \neg H_g$ and $H_g$ belongs to $\mathcal{I}_H$.
\end{defn}

\subsection{Inverse subsumption with minimal complements}
The generalization procedure based on inverse subsumption with minimal complements is as follows:

\begin{defn}\label{inverse_subsumption_with_minimal_complements_algorithm}
Let $B, E$ and $\mathcal{I}_H = \langle L \rangle$ be a background theory, examples and an induction
field, respectively. Let $F$ be a bridge theory wrt $B$ and $E$. A clausal theory $H$ is derived
by \emph{inverse subsumption with minimal complements} from $F$ wrt $\mathcal{I}_H$ if $H$ is constructed as follows.
\begin{itemize}
\item Step 1. Compute $Taut(\mathcal{I}_H)$;
\item Step 2. Compute $\tau(M(F \cup Taut(\mathcal{I}_H)))$;
\item Step 3. Construct a clausal theory $H$ satisfying the condition:
$H \subsumes \tau(M(F \cup Taut(\mathcal{I}_H)))$.
\end{itemize}
\end{defn}

Inverse subsumption with minimal complements ensures the completeness for finding
hypotheses wrt $\mathcal{I}_H$ and $F$.

\begin{lemma}\label{yamamoto2012inverseLemma2}\cite{yamamoto2012inverse}
Let $B$, $E$ and $I_H$ be a background theory, examples and an induction field,
respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$, $H$ satisfies the following condition:

$H \subsumes \tau(M(F \cup Taut(I_H))$.
\end{lemma}

\begin{thm}\emph{Completeness of inverse subsumption with minimal complements} Let $B$, $E$ and $\mathcal{I}_H$ be a background theory, examples and an induction field,
respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$,
$H$ is derived by inverse subsumption with minimal complements from $F$ wrt $\mathcal{I}_H$.
\end{thm}
\begin{proof}
Follows from \fullref{yamamoto2012inverseLemma2}.
\end{proof}