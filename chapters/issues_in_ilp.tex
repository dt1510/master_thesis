\chapter{Issues in ILP}\label{chap:issues_in_ilp}
The objective of this chapter is to analyse the central issues in ILP: ILP task definition, completeness by problem classes, hypothesis selection and hypothesis search in order to be able to compare theoretical frameworks and ILP systems in the chapter \nameref{chap:classification_of_ilp_systems} based on how they approach solving a particular issue.

For an analysis of an issue to establish the means for a useful comparison of ILP systems we require:
\begin{itemize}
\item an explanation of the importance of solving an issue,
\item an extraction of the defining properties of an issue in terms of which a corresponding answer is formed.
\end{itemize}

\section{ILP task definition}\label{sec:ilp_task_definition}
ILP task definition issue arises from ILP systems using the same \emph{incomplete ILP task definition} although each solves a different machine learning problem.

The objectives of this section are to present the main machine learning problems sought to be solved by ILP systems: generalization and explanatory induction; and then extract their properties upon which a comparison of ILP systems can be based.

We consider defining properties of a machine learning problem: positive examples, negative examples, semantics; that is whose presence or absence in a learning problem is spefified by a corresponding ILP task definition.

\subsection{Learning problem vs. ILP task}
A \emph{learning problem} is a general machine learning problem. An \emph{ILP task} is a learning problem specified formally by an input and a required output within the context of ILP.

\subsection{Generalization}

\begin{defn}
A theory $T_2$ is \emph{more general} that a theory $T$ wrt the consequence operator $Cn$ iff $Cn(T) \subseteq Cn(T_2)$.
\end{defn}
When the consequence operator $Cn$ is clear or general, we say more succintly that $T_2$ is more general than $T$. Every theory $T$ is more general than itself since $Cn(T) \subseteq Cn(T)$.

\begin{defn}
A theory $T_2$ is \emph{strictly more general} that a theory $T$ wrt the consequence operator $Cn$ iff $Cn(T) \subset Cn(T_2)$.
\end{defn}

\begin{defn}
A learning problem of \emph{generalization} is given a logical theory $T$, find a consistent logical theory $T_2$ more general than the theory $T$ wrt to some consequence operator $Cn$, i.e.
$Cn(T) \subseteq Cn(T_2)$, $false \not\in Cn(T_2)$.
\end{defn}

The objective of generalization is to produce from a theory $T$ a new theory $T_2$ with a greater predictive power to enable logical reasoning about the possible realities with the additional knowledge.

\begin{defn}
Let $T$ be a logical theory. $\phi$ is \emph{predictable} from $T$ iff there is a consistent theory $T_2$ more general than $T$ with $\phi \in Cn(T_2)$. We say that $T_2$ \emph{predicts} $\phi$ from $T$.
\end{defn}

\begin{defn}
A theory $T_2$ is of a \emph{greater predictive power} than $T_1$ iff
for some $T$ it holds that $\forall \phi \in L.$ if $T_2$ predicts $\phi$ from $T$, then $T_1$ predicts $\phi$ from $T_2$ for the language $L$ of the theories $T, T_1, T_2$.
\end{defn}

\begin{exmp}
Let $T$ be a theory $\{mortal(aristotle) \leftObjectImplies man(aristotle), man(adam)\}$. Then the theory $T_2=T \cup \{mortal{adam} \leftObjectImplies man(adam)\}$ is consistent and more general than $T$ assuming a classical consequence operator. $mortal(adam)$ cannot be deduced from the theory $T$ although it can be deduced from its generalized theory $T_2$. Hence $mortal(adam)$ is predictable from $T$.
\end{exmp}

\subsection{Explanatory induction\cite{yamamoto2012inverse}}
\begin{defn}\cite{flach1996rationality} A learning problem of \emph{explanatory induction} is given logical theories background knowledge $B$, examples $E$ to find a hypothesis $H$ satisfying $E \subseteq Cn(B \cup H)$ and $false \not\in Cn(B \cup H)$ for some consequence operator $Cn$.
\end{defn}

This learning problem is alternatively called \emph{learning from entailment}\cite{muggleton1995inverse}\cite{de1997logical}. The examples $E$ can be conceptually divided into positive examples and negative examples $E=E+ \cup \neg E-$ where $E, E+, \neg E-$ are conjunctions of logical statements, hence $E-$ being a disjunction of logical statements.

\begin{exmp}
Let $E=\{man(adam), \neg man(alice), \neg man(susam)\}$. Then $E+=\{man(adam)\}$,
$\neg E- = \{\neg man(alice), \neg man(susam)\}=\neg man(alice) \land \neg man(susan)$, hence
$E- = man(alice) \lor man(susan)$.
\end{exmp}

Therefore one can define explanatory induction with the negative examples.
\begin{defn}A learning problem of \emph{explanatory induction} is given logical theories background knowledge $B$, positive examples $E+$, negative examples $E-$ to find a hypothesis $H$ satisfying
$E+ \subseteq Cn(B \cup H)$ and $E- \not\in Cn(B \cup H)$.
\end{defn}
The consistency condition has become redundant since $false \in Cn(B \cup H$ implies $E- \in Cn(B \cup H)$.

Explanatory induction is a case of generalization where the theory $T_2=B \cup H$ generalizes the theory $T = B \cup E$.

\subsection{Expressivity}
One may wish to limit the logical theories $T, B, E, H$ to be of a certain form to specify or relax a learning problem by affecting the expressivity of the respective theories.

\subsubsection{Expressivity by arithmetical hierarchies}
\paragraph{Ground theories} do not contain sentences with bound variables. A ground theory has a correspondent equivalent propositional theory. Treating unbounded variables as Skolem constants theories whose variables are free can be treated as "template" ground theories.
\paragraph{First-order theories} allow sentences to have variables bounded by an \emph{existential} and \emph{universal} quantifier. One may limit bounding to be induced by existential quantifiers only or universal quantifiers only; or put additional restrictions on the occurance of quantifiers in the logical sentences.

\subsubsection{Logical representation}
\emph{Clausal theories} allow sentences to be clauses, but one may restrict the theories to be \emph{extended logic programs}, \emph{normal logic programs}, 
\emph{Horn theories}, \emph{definite logic programs}, \emph{Datalog programs}, \emph{literals}, \emph{atoms}. For example, Imparo\cite{kimber2012learning} requires background knowledge and a hypothesis to be a definite logic programs and examples $E+$ and $E-$ to be ground atomic whereas Yamamoto et al. reason about the explanatory induction in \cite{yamamoto2012inverse} with any clausal theories $B, E, H$. This has an impact on the complexity of the learning problem as every definite theory can be expressed as a clausal theory, but not vice versa.

\subsubsection{Other properties}
The expressivity of the learning problem is further affected by
\begin{itemize}
\item \emph{concept limitations} on how many predicate and function symbols are allowed in the theories,
\item \emph{theory size limitations} whether the allowed theories have to be finite or of what maximal size. 
\end{itemize}

\subsection{Examples}
Positive examples $E+$ and negative examples $E-$ in explanatory induction put conditions on a possible explanation $H$: $E+ \in Cn(B \cup E+)$, $E- \not \in Cn(B \cup H)$. As the objective of explanatory induction is to find a good hypothesis explaining the observations from the data, it is important to analyse how the selection of the positive and negative examples affects the learning of a hypothesis.

\subsection{Semantics}
If two ILP tasks use a different consequence operator, then each specifies a different learning problem and a solution to one of them does not necessarily constitute a solution to the other problem.

\begin{exmp}
Let $Cl$ be a classical consequence operator and $Cp$ be a consequence operator induced by Prolog interpreter. Let $T=\emptyset$, $T_2=\{p\}$.
Then $T_2$ is consistent and more general wrt $Cl$ than $T$. However $T_2$ is not more general wrt $Cp$ than $T$ since $Cp(T) \not\subseteq Cp(T_2)$ as $\neg p \in Cp(T)$ but $p \in Cp(T_2)$.
\end{exmp}

An important property of a consequence operator is \emph{monotonicity}.

\section{Completeness by problem classes}
The task of a normal problem setting for inductive logic programming is given background knowledge $B$, positive examples $E+$, negative examples $E-$ to find a hypothesis $H$ such that $B \cup H \models E+$,
$B \cup H \not\models E-$,
$B \cup H \not\models false$.
The logical theories $B$, $E+$, $E-$ specify a learning problem and determine the solution space $\mathcal{H}$ of the correct hypotheses.

\begin{defn}
An \emph{ILP problem} is a tuple $\langle B, E+, E- \rangle$.
\end{defn}

We learn about the nature of the problem by extracting the properties from the given theories. Learning problems with the identical properties are put in a class. Consequently, we can classify the ILP systems and methods based on the ability to solve a well-defined class of learning problems.

\subsection{Learning concepts}
\begin{defn}
A \emph{concept} on a domain $M$ is a relation $R \subseteq M^n$ for some $n \ge 0$.
The number $n$ is called \emph{arity of a concept}.
$M^n$ and $\emptyset$ are \emph{trivial concepts}.
\end{defn}

\begin{exmp}
Let the domain $M$ be a human family $M=\{adam, eva, kain, abel\}$ then define the concepts $male, female, father, mother, parent$ by
$male=\{adam, kain, abel\}\subseteq{M}$,
$female=\{eva\}\subseteq{M}$,
$father=\{(adam,kain), (adam,abel)\}\subseteq{M^2}$,
$mother=\{(eva,kain), (eva,abel)\}\subseteq{M^2}$,
$parent=father \cup mother$.
\end{exmp}
\begin{defn}
A concept $R\subseteq M^n$ is definable from the concepts $\mathcal{C}$ iff there is a formula $\phi$ using only the concepts from $\mathcal{C}$ such that
$\forall (x_1, ..., x_n) \in M^n. R(x_1, ..., x_n) \iff \phi(x_1, ..., x_n)$.
\end{defn}
\begin{corollary}
If $P\subseteq{M^n}$, $Q\subseteq{M^n}$ are concepts, then
1. $P \setminus Q$, 2. $P \cup Q$, 3. $P \cap Q$ are concepts.
\end{corollary}
\begin{proof}
1. $\forall x_1, ..., x_n. (P \setminus Q)(x_1, ..., x_n) \iff
P(x_1, ..., x_n) \land \neg Q(x_1, ..., x_n)$.
2. $\forall x_1, ..., x_n. (P \cup Q)(x_1, ..., x_n) \iff
P(x_1, ..., x_n) \lor \neg Q(x_1, ..., x_n)$.
3. $\forall x_1, ..., x_n. (P \cap Q)(x_1, ..., x_n) \iff
P(x_1, ..., x_n) \land Q(x_1, ..., x_n)$.
\end{proof}

\begin{exmp}
The concept $father$ is definable from the concepts $male$ and $parent$ by:
$\forall x_1, x_2 \in M. father(x_1, x_2) \iff parent(x_1, x_2) \land male(x_1)$. In addition using the trivial concepts one could define
$father=parent \cap (male \times M)$.
\end{exmp}

\subsubsection{Single concept problems}


\subsection{Complexity of a learning problem}
\subsection{Learning syntactic representations}
\subsection{Learning up to logical equivalence}
\section{Hypothesis selection}
In \nameref{ilp_problem_definition} we required that an ILP problem has a unique solution. When formalising a definition that allowed several possible solutions we defined that such solutions were partial and the unique solution to the problem was a set of partial solutions.

For some learning objectives a set of solutions may be acceptable, for other learning objectives several possible solutions indicate that the problem has not been completely solved. This section analyses properties of a hypothesis, what it means for a hypothesis to have some specific property and how this information can be used to make a choice.

\subsection{Hypothesis properties}
We think of a property on a hypothesis as boolean valued function
$P:\mathcal{H} \to \{true, false\}$. We seek to list the intuitive notions of possible properties a hypothesis may have and formalise the most relevant ones.

\subsubsection{Hypothesis form}
An induction of hypotheses of certain forms tends to be statistically more successful than an induction of hypotheses of other forms.
Although one may hide the problem by introducing the Occam's razor to prefer the hypotheses of the simpler form, Goodman cautions in \cite{goodman1965new} that the form of the hypothesis depends on the description language as paraphrased:

\begin{cite}{goodman1965new}
Let H1 be a hypothesis that all emeralds are green. Let H2 be a hypothesis that all emeralds are grue - that is green until some time T in the future and blue afterwards. Then both H1 and H2 are (correct in ILP sense) explanations of our observations of green emeralds. However, we prefer inducing that all emeralds are green, not grue. The preference cannot be directly attributed to the complexity of the hypothesis. For suppose that the initial concepts are grue and brue - blue until the time T and green afterwards. Then H1 has a complex definition in terms of grue and brue whereas H2 is defined trivially by the initial concept grue.
\end{cite}

\subsubsection{Syntactic properties}
A property $P:\mathcal{H} \to \{true, false\}$ is syntactic iff $P$ is invariant under the logical equivalence, i.e.
$\forall H_1, H_2 \in \mathcal{H}. H_1 \equiv H_2 \implies P(H_1)=P(H_2)$.
The syntactic properties of a hypothesis to be considered are the number of the literals in a body, definitness of a hypothesis, arities of defining predicates, etc. TODO


\subsubsection{Semantic properties}

\subsection{Information content of a hypothesis}

\subsection{Inducing preferences over hypotheses space}
\subsection{Hypothesis sufficiency}
\subsection{Hypothesis approximation}

\section{Hypothesis search}
\begin{itemize}
\item completeness by hypothesis finding: how to find every possible (or possibly good) hypothesis $H$ explaining $E$ from $B$,
\item efficiency: how to find a good explanation efficiently. 
\end{itemize}
\subsection{Postulates of inductive inference}
\subsection{Inductive inference rules}
\subsection{Control}
\subsection{Automation}
\section{Research in ILP}
\section{Classification of ILP systems}