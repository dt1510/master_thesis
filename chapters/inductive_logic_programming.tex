\chapter{Inductive logic programming}
The objective of this chapter is to present concepts from Inductive logic programming (ILP) and their extensions that are going to be used for the classification of ILP systems.

\section{Inductive logic programming concepts}

\subsection{ILP task definition}
\section{ILP task definition}\label{sec:ilp_task_definition}
We present a definition of an ILP task of explanatory induction with positive examples, negative examples and a bias.

\subsubsection{Explanatory induction\cite{yamamoto2012inverse}}
\begin{defn}\cite{flach1996rationality} A learning problem of \emph{explanatory induction} is given logical theories background knowledge $B$, examples $E$ to find a hypothesis $H$ satisfying $E \subseteq Cn(B \cup H)$ and $false \not\in Cn(B \cup H)$ for some consequence operator $Cn$.
\end{defn}

This learning problem is alternatively called \emph{learning from entailment}\cite{muggleton1995inverse}\cite{de1997logical}. The examples $E$ can be conceptually divided into positive examples and negative examples $E=E^{+} \cup \neg E^{-}$ where $E, E^{+}, \neg E^{-}$ are conjunctions of logical statements, hence $E^{-}$ being a disjunction of logical statements.

\begin{exmp}\cite{explanatory_induction_example}
Let $E=\{man(adam), \neg man(alice), \neg man(susam)\}$. Then $E^{+}=\{man(adam)\}$,
$\neg E^{-} = \{\neg man(alice), \neg man(susam)\}=\neg man(alice) \land \neg man(susan)$, hence
$E^{-} = man(alice) \lor man(susan)$.
\end{exmp}

Therefore one can define explanatory induction with the negative examples.
\begin{defn}\label{explanatory_induction_with_negative_examples_definition}
A learning problem of \emph{explanatory induction with negative examples} is given logical theories background knowledge $B$, positive examples $E^{+}$, negative examples $E^{-}$ to find a hypothesis $H$ satisfying\\
1)$E^{+} \subseteq Cn(B \cup H)$\\
2)$E^{-} \not\in Cn(B \cup H)$.
\end{defn}
The consistency condition has become redundant since $false \in Cn(B \cup H$ implies $E^{-} \in Cn(B \cup H)$.

ILP systems we aim to classify restrict their hypothesis space by a bias. Consequently, we extend the definition of an ILP task further:
\begin{defn}\label{definition_explanatory_induction_with_bias}
A learning problem of \emph{explanatory induction with negative examples and a bias} is given logical theories background knowledge $B$, positive examples $E^{+}$, negative examples $E^{-}$, a bias $\mathcal{H}$ to find a hypothesis $H$ satisfying\\
1)$E^{+} \subseteq Cn(B \cup H)$\\
2)$E^{-} \not\in Cn(B \cup H)$\\
3)$H \in \mathcal{H}$.
\end{defn}

We refer to any of 3 possible ILP tasks as an ILP task of explanatory induction. Which definition is intended should be clear from the context depending on what input from $B, E^+, E^-, \mathcal{H}$ is given.

\begin{exmp}\label{explanatory_induction_example}
Let $E=\{mortal(aristotle)\}$, $B=\{man(aristotle)\}$,
$H_1=E$, $H_2=\{mortal(aristotle) \leftObjectImplies man(aristotle)\}$.
Then both $H_1$, $H_2$ are correct hypotheses explaining $E$ from $B$ for a bias $\mathcal{H}_{1,2}=\{H_1, H_2\}$, but only $H_1$ is a correct hypothesis for a bias $\mathcal{H}_1=\{H_1\}$.
\end{exmp}

\subsubsection{Correct hypothesis and its deviations\cite{nienhuys1997foundations}}\label{correct_hypothesis}
A hypothesis $H \in \mathcal{H}$ satisfying the conditions in explanatory induction is called a correct hypothesis.
\begin{defn}
A hypothesis $H$ is \emph{correct} wrt background knowledge $B$, positive examples $E^+$, negative examples $E^-$ iff\\
1) $E^+ \subseteq Cn(B \land H)$,\\
2) $E^- \not\in Cn(B \land H)$.
\end{defn}

When a hypothesis does not satisfy the two conditions it deviates from correctness.

\begin{defn}
Let $H$ be a theory, and $E^+$ and $E^-$ be sets of clauses.
$H$ is \emph{too strong} wrt $B$, $E^-$ iff
$E^- \in Cn(B \land H)$.
$H$ is \emph{too weak} wrt
$E^+$ iff $E^+ \not\subseteq Cn(B \land H)$.
$H$ is \emph{overly general} wrt $B$, $E^+$, $E^-$ iff
$E^+ \subseteq Cn(B \land H)$, $E^- \in Cn(B \land H)$.
$H$ is \emph{overly specific} wrt $B$, $E^+$, $E^-$ iff
$E^+ \not\subseteq Cn(B \land H)$, $E^- \not\in Cn(B \land H)$.
\end{defn}

\subsection{Inverse Entailment}
Inverse Entailment is a correspondence between an induction and a deduction:
\begin{thm}\cite{kimber2012learning}
Let $B$ be a Horn program, and let $h$ and $e$ be
Horn clauses. Then $B \wedge h \models e \iff B \wedge \neg e \models \neg h$.
\end{thm}

\subsubsection{IE algorithm\cite{yamamoto2012inverse}}\label{inverse_entailement_algorithm}
Progol, Xhail, Imparo are based on the principle of the inverse entailment. By the principle of the inverse entailment $B \cup H \models E$ iff
$B \cup \neg E \models \neg H$. A hypothesis $H \in \mathcal{H}_2$ is a solution to the problem of the explanatory induction $(B,E^{+},E^{-},\mathcal{H})$ iff
$B \cup H \subseteq Cn(E^{+} \union \overline{E^{-}})$,
 $false \not\in Cn(E^{+} \cup E^{-})$, $\mathcal{H}_2 \subseteq \mathcal{H}$
iff
$B \cup H \subseteq Cn(E^{+} \union \overline{E^{-}})$,
 $false \not\in Cn(E^{+} \cup E^{-})$, $\mathcal{H}_2 \subseteq \mathcal{H}$
 
They compute the hypothesis $H$ in two steps:
1. constructing an intermediate theory, 2. generalizing its negation into the hypothesis with the inverse of the entailment relation.

\subsection{Input and output of ILP system}
An input to an ILP system is a logic program that can be conceptually divided into 3 parts:
\begin{itemize}
\item objectlevel statements: background knowledge, examples: positive and negative,
\item effective metalevel statements,
\item non-effective metalevel statements.
\end{itemize}

Objectlevel statements provide the main setting of a learning problem and concern with what is known: the language of enquiry $L$, a set of observations, etc.

Effective metalevel statements provide a refinement of the main setting and concern with how the learning problem may be solved and what form the solution is expected to take. Their specification converts a learning problem into a potentially easier, more directed and concrete \emph{ILP task}.
\emph{Mode declarations} and \emph{determinations} are effective metalevel statements specifying a constraint on a language $L$ to determine the search space $\mathcal{H}$ of possible hypotheses. The other effective metalevel statements may impose further \emph{metalevel constraints} on $L$. The set of constraints on $L$ determining the search space $\mathcal{H}$ is called a \emph{language bias}.
Apart from the specification of a language bias, effective metalevel statements contain search control statements that affect the form the computation of a hypothesis takes place and contain decision elements: preferential bias - which hypothesis should be preferred over the other, which parts of the background knowledge should be considered, etc.

Non-effective metalevel statements do not affect the result of the computation of a hypothesis. They are either required syntactic sugar or their primary purpose concerns different aspects of an ILP system, e.g. to provide the statistics about the computation to the user.

The primary output of an ILP system is a solution (a hypothesis or their set) to a refined learning problem.

\section{Bias\cite{nienhuys1997foundations}}
Considering all theories $H \subseteq L$ as possible hypotheses is computationally infeasible. When one knows the properties that a hypothesis may take, one may introduce a bias $\mathcal{H} \subseteq \powerset{L}$. For example, $\mathcal{H}$ being a set of all finite Horn theories.

Two main types of a bias are:
\begin{itemize}
\item language bias,
\item search bias.
\end{itemize}

In the context of a language bias we refer to the bias as a hypothesis space $\mathcal{H}$, in the context of a search bias we refer to it as a search space $\mathcal{H}$.
These different terminologies refer to the same bias $\mathcal{H}$, however emphasise a different local focus on a bias in the text.

\subsection{Language bias}\label{subsec:background_language_bias}
The space of all possible hypotheses $H \in \mathcal{H}$ is restricted by a language bias $\mathcal{H}$. The main types of the language biases in ILP are:
\begin{itemize}
\item mode declarations,
\item determinations,
\item metaconstrains,
\item induction and production field.
\end{itemize}

\subsubsection{Mode declarations}\label{background_mode_declarations}
Mode declarations specify what predicates are allowed in a head and a body of a hypothesis.
Suppose you would like your ILP system explain observations
$woman(alice), woman(susan)$, then you know that a hypothesis you are looking for has to explain or define the concept $woman$. Therefore you declare a statement to an ILP system: ``In your search consider only hypotheses that have the predicate $woman$ in their head." A statement specifying what predicates should be in a head is called a modeh declaration. A stament specifying the allowed predicates in a body is a modeb declaration.

\paragraph{Mode declaration in Progol}
Muggleton defines the hypothesis bias $\mathcal{H}$ with mode language and mode declarations.
\begin{defn}\cite{muggleton1995inverse}
A \emph{mode declaration} has either the form
\tc{modeh(n,atom)} or \tc{modeb(n,atom)} where $n$, the \emph{recall}, is either an integer, $n \ge 1$,
or \tc{*} and atom is a ground atom. Terms in the atom are either normal or placemarker. A normal term is either a constant or a function symbol followed by a
bracketed tuple of terms. A placemarker is either \tc{+type}, \tc{-type} or \tc{\#type}, where
type is a constant. If $m$ is a mode declaration then $a(m)$ denotes the atom of m
with place-markers replaced by distinct variables. The sign of $m$ is positive if $m$
is a \tc{modeh} and negative if $m$ is a \tc{modeb}.
\end{defn}

The recall is used to bound the number of alternative solutions for instantiating
the atom. For simplicity, we assume in the following that all the modes have the
recall \tc{*}, meaning all solutions. The following defines when a clause is within Progol's definite mode language $L$.

\begin{defn}\label{definition_definite_mode_language}\cite{muggleton1995inverse}
Let $C$ be a definite clause with a
defined total ordering over the literals and $M$ be a set of mode declarations.
$C = h \leftObjectImplies b1, ..., bn$ is in the \emph{definite mode language} $L(M)$ iff 1) $h$ is the atom
of a \tc{modeh} declaration in $M$ with every place-marker \tc{+type} and \tc{-type} replaced by
variables and every placemarker \tc{\#type} replaced by a ground term and 2) every
atom $b_i$ in the body of $C$ is the atom of a \tc{modeb} declaration in $M$ with every
place-marker \tc{+type} and \tc{-type} replaced by variables and every place-marker \tc{\#type}
replaced by a ground term and 3) every variable of \tc{+type} in any atom $b_i$ is either
of \tc{+type} in $h$ or of \tc{-type} in some atom $b_j$, $1 \le j < i$.
\end{defn}

\begin{exmp}
The mode declarations $M=\{$
\begin{lstlisting}
modeh(1,woman(+person)),
modeb(*,female(+person)),
modeb(*,male(+person))
\end{lstlisting}
$\}$
specify the possible hypotheses
$H_1=woman(X)$,
$H_2=woman(X) \leftObjectImplies female(X)$,
$H_3=woman(X) \leftObjectImplies male(X)$,
$H_4=woman(X) \leftObjectImplies female(X), male(X)$.
Thefore the hypothesis bias created by the mode declarations 
is $\mathcal{H}=L(M)=\{H_1, H_2, H_3, H_4\}$, but $H_5 \not\in L(M)$ for
$H_5=woman(X) \leftObjectImplies cook(X)$.
\end{exmp}

The mode declarations can have a richer syntactic structure where \tc{?} in the last modeb declaration stands for an arithmetical operator:
\begin{exmp}\cite{muggleton1995inverse}
\begin{lstlisting}
modeh(1,plus(+int,+int,-int))
modeb(*,append(-list,+list,+list)
modeb(1,append(+list,[+any],-list))
modeb(4,(+int ? #int))
\end{lstlisting}
\end{exmp}

\subsubsection{Determinations}
Some ILP systems like Aleph in addition to mode declarations use determination statements\cite{aleph2007} to impose a finer control on a search space.

\begin{defn}\cite{aleph2007}
A \emph{determination} is a logic statement of the form\\
\tc{determination(TargetName/TargetArity, BackgroundName/BackgroundArity).}
where \tc{TargetName}, \tc{BackgroundName} are predicate symbols and
\tc{TargetArity}, \tc{BackgroundArity} are their arities (non-negative integers) respectively.
\end{defn}

\begin{defn}\label{definition_determination_language}
Let $D=\langle d_1(tn_1/ta_1, bn_1/ba_1), ..., d_n(tn_n/ta_n, bn_n/ba_n)\ \rangle$ be an ordered list of determinations. A formula $H=h \leftObjectImplies b_1, ..., b_m$ is in \emph{determination language} $L(D)$ iff\\
1) $D \not=\emptyset$,\\
2) $\forall b_i (1 \le i \le m) \exists j (1 \le j \le n) tn_j=h \land bn_j=b_i$.
\end{defn}

\begin{exmp}
Let $D$ be the list of the determinations
\begin{lstlisting}
:− determination(man/ 1, male/1).
:− determination(man/ 1, bridegroom/1).
:− determination(woman/ 1, female/1).
\end{lstlisting}
Let the hypotheses be

\begin{minipage}[t]{.50\textwidth}
\begin{lstlisting}
H1=man(X) :- male(X).
H2=man(X) :- bridegroom(X).
H3=man(X) :- male(X), bridegroom(X).
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{.50\textwidth}
\begin{lstlisting}
H4=woman(X) :- female(X).
H5=man(X) :- male(X), female(X).
\end{lstlisting}
\end{minipage}

Then for the hypothesis bias (determination language) $\mathcal{H}=L(D)$,
$\{H_1, H_2, H_3, H_4, H_5\} \cap \mathcal{H}=\{H_1, H_2, H_3\}$.
\end{exmp}

\subsubsection{Metaconstraints}
Metaconstraints specify the additional constraints on the hypothesis space $\mathcal{H}$ not definable with the mode declarations and determinations.
An example being a metaconstraint $Cond$ specifying the maximum number of the literals allowed in a formula $H \in L$.

\begin{defn}
A \emph{metaconstraint} on a language $L$ is any formula $Cond$ defining some subset $L(Cond)$ (\emph{metaconstraint language}) of the language $L$, i.e. $L(Cond)=\{x \in L | Cond(x)\}\subseteq L$.
\end{defn}

Each ILP system has its own set of allowed metaconstrains.
\begin{exmp}
In Toplog \tc{set(maximum\_literals\_in\_hypothesis, 5)} defines clausal theories whose clauses consist of at most 5 literals.
In Imparo \tc{set\_max\_clauses(2)} defines clausal theories with at most 2 clauses.
In Tal \tc{option(max\_body\_literals, 5)} defines definite theories whose clauses have at most 5 positive literals.
\end{exmp}

\begin{proposition}
Let $Cond_1, Cond_2$ be metaconstrains on a language $L$.
Then the following are metaconstrains on the language $L$:
$\neg Cond_1$, $Cond_1 \land Cond_2$, $Cond_1 \lor Cond_2$.
\end{proposition}

\begin{proof}
Follows trivially from the definition of a metaconstraint.
\end{proof}

Therefore we will often think of a set of the metaconstraints to be a metaconstraint created from their conjunction.

\begin{defn}
Let $C=\{Cond_1, ..., Cond_n\}$ be a set of metaconstraints defining subsets of the language $L$.
A \emph{metaconstraint language} $L(C)$ is the intersection of the subsets defined by $Cond_1, ..., Cond_n$,
i.e. $L(C)=\{x \in L | Cond_1(x) \land ... \land Cond_n(x) \}$.
\end{defn}

\subsubsection{Induction and production field}
Inoue defines a production field as another form of a language bias.

\begin{defn}\cite{inoue2004induction}
A \emph{production field} $\mathcal{P}$ is a represented by a pair,
$\langle Lit, Cond\rangle$, where $Lit$
(\emph{the characteristic literals} of $\mathcal{P}$) is a subset of literals in a language $L$ and is closed under instantiation (that is, if a literal containing variables is in $Lit$, then all its instances are also in $Lit$), and $Cond$ is a certain condition to be satisfied. 
A clause $C$ is said to belong to a production field $\mathcal{P} = \langle Lit, Cond \rangle$ if every literal in $C$ belongs to $Lit$ and $C$ satisfies $Cond$.
\end{defn}

A special case of the production field when a condition $Cond \equiv true$ is called an induction field.

\begin{defn}\cite{yamamoto2012inverse}\label{induction_field_definition}
An \emph{induction field} is denoted by $\mathcal{I}_H = \langle Lit \rangle$,
where $Lit$ is a finite
set of literals to appear in ground hypotheses.
A ground hypothesis $H_g$ \emph{belongs to} $\mathcal{I}_H$ if
every literal in $H_g$ is included in $Lit$.
\end{defn}

\subsubsection{Correspondence between mode declarations, determinations, meta constraints and production field}
A production field is a generalization of mode declarations, determinations and metaconstraints.

\begin{proposition}\label{md_d_pf_correspondence_proposition}
Let $M$ be a set of mode declarations, $D$ an ordered list of determinations. Then there exist production fields
$\mathcal{P}_M$,
$\mathcal{P}_D$,
$\mathcal{P}_{M,D}$
such that for any clause $H \in L$:\\
1) $H \in L(M) \iff H \in \mathcal{P}_M$,\\
2) $H \in L(D) \iff H \in \mathcal{P}_D$,\\
3) $H \in L(M) \cap L(D) \iff H \in \mathcal{P}_{M,D}$.\\
\end{proposition}
The following proof is non-constructive.
\begin{proof}
Let $L$ be a logic language. By the definitios of definite mode language\ref{definition_definite_mode_language} and determination language\ref{definition_determination_language}, $L(M)$ and $L(D)$ are definable.
Let $Cond_M$, $Cond_D$ be defining formulas for $L(M)$ and $L(D)$, i.e. $L(M)=\{x \in L | Cond_M(x)\}$, $L(D)=\{x \in L | Cond_D(x)\}$.
Let $Cond_{M,D}=Cond_M \land Cond_D$. Let $Lit$ be a set of all literals of $L$ and let
$Lit_M=L(M) \cap Lit$,
$Lit_D=L(D) \cap Lit$,
$Lit_{M,D}=Lit_M \cap Lit_D$.
Then for every clause $H \in L$ the production fields
$\mathcal{P}_M=\langle Lit_M, Cond_M \rangle$,
$\mathcal{P}_D=\langle Lit_D, Cond_D \rangle$,
$\mathcal{P}_{M,D}=\langle Lit_{M,D}, Cond_{M,D} \rangle$
satisfy the conditions 1, 2, 3 respectively trivially.
\end{proof}

Given production fields defining the bias of mode declarations and determinations, productions fields with the additional metaconstrains can be constructed.

\begin{proposition}\label{proposition_metaconstraints_production_field}
Let $\mathcal{P}$ be any production field, $C$ a set of metaconstraints.
Then there exists a production field $\mathcal{P}_C$ such that
$\mathcal{P}_C=\mathcal{P} \cap L(C)$, i.e.
$\forall H \in L. H \in \mathcal{P}_C \iff (H \in \mathcal{P}) \land (H \in L(C))$.
\end{proposition}
\begin{proof}
Let $\mathcal{P}=\langle Lit, Cond \rangle$,
$C=\{Cond_1, ..., Cond_n\}$.
Define $\mathcal{P}_C=\langle Lit, Cond \land Cond_1 \land ... \land Cond_n \rangle$. Take an arbitrary $H \in L$. Denote $Lit(H)$ to be the set of the literals of the clausal form of $H$.
Then $H \in \mathcal{P}_C$ iff $Lit(H) \subseteq Lit$ and
$(Cond \land Cond_1 \land ... \land Cond_n)(H)$ iff
$Lit(H) \subseteq Lit$ and $Cond(H)$ and $Cond_1(H) \land ... \land Cond_n(H)$
iff $H \in \langle Lit, Cond \rangle = \mathcal{P}$ and $H \in L(Cond_1 \land ... \land Cond_n) = L(C)$. Thus $\mathcal{P}_C=\mathcal{P} \cap L(C)$.
\end{proof}

\begin{corollary}
Let $M$ be a set of mode declarations, $D$ an ordered list of determinations, $C$ a set of the metaconstraints.
Then there exist production fields
$\mathcal{P}_{M,C}$,
$\mathcal{P}_{M,D,C}$
such that for any clause $H \in L$:\\
1) $H \in L(M) \cap L(C) \iff H \in \mathcal{P}_{M,C}$,\\
2) $H \in L(M) \cap L(D) \cap L(C) \iff H \in \mathcal{P}_{M,D,C}$.\\
\end{corollary}
\begin{proof}
Follows from \ref{md_d_pf_correspondence_proposition} and \ref{proposition_metaconstraints_production_field}.
\end{proof}

The converse depends on the possible metaconstraints. If one allowed a metaconstraint for every possible definable condition $Cond$, then one could express any production field $\mathcal{P}=\langle Lit, Cond \rangle$ with mode declarations, determinations and metaconstraints.

\subsection{Search bias\cite{nienhuys1997foundations}}
Search bias is defined by the properties of an algorithm that searches a hypothesis: heuritics and direction of a search.

\section{ILP systems}
An ILP system is a program that takes as an input background knowledge $B$ and examples $E$ and produces as an output an explanation called a hypothesis $H$ explaining the examples in terms of the background knowledge $B$, i.e. $E \subseteq Cn(B \cup H)$.
We give a list of ILP systems Progol, Aleph, Toplog, Xhail, Imparo, Tal.

\subsection{Progol}
We paraphrase the introduction to Progol from its official website\cite{muggleton1999progolWebsite}:
\begin{quote}
Progol combines Inverse Entailment with general-to-specific search through a refinement graph. Inverse Entailment is used with mode declarations to derive the most-specific clause within the mode language which entails a given example. This clause is used to guide a refinement-graph search by performing an admissible A*-like search, guided by compression, over clauses which subsume the most specific clause. Progol allows arbitrary Prolog programs as background knowledge and arbitrary definite clauses as examples.
\end{quote}

\subsection{Aleph\cite{aleph2007}}
Aleph is an ILP system based on Progol developed in order to understand the concepts behind the inverse entailment\cite{muggleton1995inverse} influenced by the ideas from other ILP systems: CProgol, FOIL, FORS, Indlog, MIDOS, SRT, Tilde, and WARMR. 

\subsection{Toplog\cite{santos2008toplogWebsite}\cite{muggleton2008toplog}}
Toplog is an ILP system extending the use of Inverse Entailment by a top theory $\top$ bias.
The search space is restricted by requiring that each hypothetised clause of a hypothesis $H$ must be entailed by the top theory $\top$.

\subsubsection{MC-Toplog\cite{muggleton2012mc}}
MC-Toplog is a sequel of Toplog, derives hypotheses like Toplog, in addition allows multiple clauses in a hypothesis. It restricts a hypotheses space to clauses entailing generalization of multiple examples as opposesed to Toplog that could generalizing only a single example.

\subsection{Hail\cite{ray2003hybrid}\cite{ray2005phdHybrid}}
A Hail (Hybrid abductive inductive learning) ILP system extending the Progol's incomplete method of bottom generalization to a more complete method of \emph{kernel set generalization} with the methods of abductive logic programming.

\subsection{Xhail\cite{ray2009nonmonotonic}}
Xhail standing for eXtended Hybrid abductive learning is an ILP system extending Hail's methodology from Horn theories to normal logic programs.
Xhail version 2 was used for the experiments presented in this thesis.

\subsection{Imparo\cite{kimber2012learning}}
Imparo is an ILP system based on a general Induction on Failure theoretical framework extending the Hail's incomplete method of kernel set generalization to the complete method of connected theory generalization.

\subsection{Tal\cite{corapi2010inductive}\cite{corapi2011tal}}
Tal (Top-directed Abductive Learning) is a non-monotonic top-down ILP system complete for background theories and hypotheses as normal logic programs. 
Tal relies on mapping an ILP problem into an equivalent ALP one. This enables the
use of established ALP proof procedures and the specification of richer language bias with integrity constraints. The mapping provides a principled search space for an ILP problem, over which an abductive search is used to compute inductive solutions.

\subsection{Other systems}
Other ILP systems include Metagol, Golem, Spectre, EBG, Alecto, FOIL, Linus, Marvin, Mis, Confucius, Quinlan, ASPAL, Hyper,  Tilde, CF-induction.

\subsection{Selecting ILP systems for classification}
Classification of all ILP systems would be out of the scope of this project and would not provide much more value than a classification of well chosen ILP systems.
For selection of ILP systems for classification the following criteria were taken into the consideration:

\begin{itemize}
\item a popularity of an ILP system,
\item a novelty and contribution of an ILP system,
\item an access: an availability of a general purpose implementation, documentation, research findings, easiness of communication (in person vs. over distance) with the original authors and respective experts.
\end{itemize}

In the end the author has decided to classify Progol, Aleph, Toplog, Xhail, Imparo, Tal; Aleph being the best documented working system and others having been developed in the author's department with a general purpose implementation. The author would like to note based on the private communication with the respective experts that MC-Toplog does not have a general purpose implementation, a user needs to write a specific top theory for each learning problem and adapt the current implementation to support it. Metagol\cite{muggleton2014meta}, being a very interesting and novel system, similarly, as of time of classification did not have a general purpose implementation.

\section{Inverse subsumption for complete explanatory induction\cite{yamamoto2012inverse}}\label{inverse_subsumption_for_complete_explanatory_induction}
The subsections \ref{subsec:preliminaries} and \ref{subsec:inverse_subsumption_with_minimal_complements} paraphrase work by Yamamoto et al. \cite{yamamoto2012inverse} unless stated otherwise. For more examples, explanations and results, due to space constrains the reader is encouraged to consult the original source shall the need arise. The main result is a complete algorithm \emph{inverse subsumption with minimal complements} for deriving a hypothesis wrt $B, E$ by the method of the inverse subsumption in lieu of the antientailment.

The remaining sections concern with the possible extensions of the inverse subsumption to first order theories, to support negative examples, the simplification of the algorithm and prove the completeness of Imparo in inverse subsumption.

\subsection{Preliminaries}\label{subsec:preliminaries}
\begin{defn}
Let $C$, $D$ be clauses, then $C$ \emph{(theta-)subsumes} $D$ iff there is a substitution $\theta$ such that $C \theta \subseteq D$. We denote the relation by $C \subsumes D$.
\end{defn}

\begin{defn}\label{definition_theory_subsumption}
Let $S$ and $T$ be two clausal theories. Then, $S$ \emph{(theory-)subsumes} $T$, denoted by $S \subsumes T$, if for any clause $D \in T$, there is a clause $C \in S$ such
that $C \subsumes D$. We denote by the inverse relation $\subsumed$ of the (theory-) subsumption, called \emph{anti-subsumption}.
\end{defn}

\begin{defn}
Let $S$ be a ground clausal theory $\{C_1, C_2,... ,C_n\}$ where each clause $C_i$ $(1 \le i \le n)= l_{i,1} \lor l_{i,2} \lor... \lor l_{i,m_i}$.
The \emph{complement} of $S$ is defined as
\\$\bar{S} = \{\neg l_{1,k_1} \lor \neg l_{2,k_2} \lor... \lor \neg l_{n,k_n} |
1 \le k_1 \le m_1 , 1 \le k_2 \le m_2,..., 1 \le k_n \le m_n\}$.
In case that $S$ is empty, $\bar{S}$ is defined as the set $\{\bot\}$ where $\bot$ is the empty clause.
\end{defn}

\begin{remark}
$S$ is a CNF formula such that $\bar{S} \equiv \neg S$.
\end{remark}

\begin{defn}
\emph{$\tau(S)$} denotes the clausal theory obtained by removing all the tautologies from a logic theory $S$.
\end{defn}

\begin{defn}
\emph{$\mu(S)$} denotes the clausal theory obtained by removing from $S$ all clauses that are properly subsumed by clauses in a logic theory $S$.
The \emph{minimal complement} of $S$ is $M(S)=\mu(\bar{S})$.
\end{defn}

\begin{exmp}
Let $S=\{a \lor b, b \lor c, \neg c\}$. Then
$\bar{S}=\{\neg a \lor \neg b \lor c, \neg a \lor \neg c \lor c, \neg b \lor c, \neg b \lor \neg c \lor c\}$.
$M(S)=\{\neg a \lor \neg c \lor c, \neg b \lor c\}$,
$\tau(\bar{S})=\{\neg a \lor \neg b \lor c, \neg b \lor c\}$.
\end{exmp}

The inverse subsumption with minimal complements computes a hypothesis $H$ wrt to the bridge theory $F$ and the induction field $\mathcal{I}_H$.

\begin{defn}\label{definition_bridge_theory}
Let $B$ and $E$ be a background theory and examples, respectively.
Let $F$ be a ground clausal theory. Then $F$ is a \emph{bridge theory} wrt $B$ and $E$ if
$B \land \bar{E} \models F$ holds. If no confusion arises, a bridge theory wrt $B$ and $E$ will simply be called a bridge theory.
\end{defn}

The author finds the notion of the maximal bridge theory useful in later chapters.
\begin{defn}\label{maximal_bridge_theory_definition}
A \emph{maximal bridge theory} wrt $B$ and $E$ is a theory $F = B \cup \bar{E}$.
\end{defn}
\begin{remark}
Every bridge theory is entailed by a maximal bridge theory.
\end{remark}

We next define the target hypotheses using the notion of an induction field $\mathcal{I}_H$\ref{induction_field_definition}, together with a bridge theory $F$ as follows:

\begin{defn}\label{definition_hypothesis_wrt_induction_field_bridge_theory}
Let $H$ be a hypothesis. $H$ is a \emph{hypothesis wrt $\mathcal{I}_H$ and $F$} if there is a ground hypothesis $H_g$ such that $H_g$ consists of instances from $H$,
$F \models \neg H_g$ and $H_g$ belongs to $\mathcal{I}_H$.
\end{defn}

\begin{defn}\emph{Tautologies of an induction field.}
Given an induction field $\mathcal{I}_H = \langle Lit \rangle$, $Taut(\mathcal{I}_H)$ is defined
as the set of tautologies $\{\neg A \land A | A \in Lit, \neg A \in Lit\}$.
\end{defn}

\subsection{Inverse subsumption with minimal complements}\label{subsec:inverse_subsumption_with_minimal_complements}
The generalization procedure based on inverse subsumption with minimal complements is as follows:

\begin{defn}\label{inverse_subsumption_with_minimal_complements_algorithm}
Let $B, E$ and $\mathcal{I}_H = \langle L \rangle$ be a background theory, examples and an induction
field, respectively. Let $F$ be a bridge theory wrt $B$ and $E$. A clausal theory $H$ is derived
by \emph{inverse subsumption with minimal complements} from $F$ wrt $\mathcal{I}_H$ if $H$ is constructed as follows.
\begin{itemize}
\item Step 1. Compute $Taut(\mathcal{I}_H)$;
\item Step 2. Compute $\tau(M(F \cup Taut(\mathcal{I}_H)))$;
\item Step 3. Construct a clausal theory $H$ satisfying the condition:
$H \subsumes \tau(M(F \cup Taut(\mathcal{I}_H)))$.
\end{itemize}
\end{defn}

Inverse subsumption with minimal complements ensures the completeness for finding
hypotheses wrt $\mathcal{I}_H$ and $F$.

\begin{lemma}\label{yamamoto2012inverseLemma2}\cite{yamamoto2012inverse}
Let $B$, $E$ and $I_H$ be a background theory, examples and an induction field,
respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$, $H$ satisfies the following condition:

$H \subsumes \tau(M(F \cup Taut(I_H))$.
\end{lemma}

\begin{thm}\emph{Completeness of inverse subsumption with minimal complements} Let $B$, $E$ and $\mathcal{I}_H$ be a background theory, examples and an induction field,
respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$,
$H$ is derived by inverse subsumption with minimal complements from $F$ wrt $\mathcal{I}_H$.
\end{thm}
\begin{proof}
Follows from \fullref{yamamoto2012inverseLemma2}.
\end{proof}

\subsection{Extension to first-order theories}
A generalization of the antisubsumption results to the first-order clausal theories enables a more efficient computation of anti-subsumption and of the minimal complement, especially if the number of potential tautologies is large since there is less need to search for a subsumed hypothesis as the hypothesis is already not ground. We prove the soundness of the first-order extension and leave the completeness as an open question.

\begin{proposition}\label{soundness_first_order_extension}(Soundness of the first-order extension)
Let $B$, $E$ and $I_H$ be a first-order background theory, examples and an induction field, respectively. Let $F$ be a bridge theory wrt $B$ and $E$. Suppose that $H \subsumes \tau(M(F \cup Taut(I_H)))$, then $B \cup H \models E$.
\end{proposition}
\begin{proof}
By subsumption $H \models \tau(M(F \cup Taut(I_H))) \equiv \neg F$. By the principle of the inverse entailment $B \cup \neg F \models E$, hence
$B \cup H \models E$.
\end{proof}

\subsection{Relaxation of minimal complements to a maximal bridge theory}
We relax the procedure of the minimal complements by using a maximal bridge theory\fullref{maximal_bridge_theory_definition} while preserving the completeness for finding correct hypotheses wrt to $B, E, \mathcal{I}_H$.

\begin{defn}
Let $B$, $E$, $\mathcal{I}_H$ be background knowledge, examples, an induction field. Then a \emph{hypothesis subsumer} wrt $B, E, \mathcal{I}_H, F$ is a theory
$T=\tau(M(F \cup Taut(\mathcal{I}_H))$ (recall $M(S)=\mu(\bar{S})$) for a bridge theory $F$ wrt $B, E$.
\end{defn}

\begin{defn}
Let $B$, $E$, $\mathcal{I}_H$ be background knowledge, examples, an induction field. Then a \emph{maximal hypothesis antisubsumer} wrt $B, E, \mathcal{I}_H$ is a theory
$T=\tau(M(F \cup Taut(\mathcal{I}_H))$ for a maximal bridge theory $F=B \land \bar{E}$ wrt $B, E$.
\end{defn}

By the completeness of inverse subsumption with minimal complements, for every hypothesis $H$ wrt $F$ and $\mathcal{I}_H$ it holds $H \subsumes \tau(M(F \cup Taut(\mathcal{I}_H))$. Since a maximal bridge theory $F$ is used, if a hypothesis $H$ wrt $\mathcal{I}_H$ is correct wrt $B, E$, then it can be derived by inverse subsumption with minimal complements with a maximal bridge theory.

\begin{defn}
A hypothesis $H$ is derived by \emph{inverse subsumption with minimal complements with a maximal bridge theory} wrt $B, E, \mathcal{I}_H$ iff
$H \subsumes \tau(M(B \cup \bar{E} \cup Taut(\mathcal{I}_H))$.
\end{defn}

\begin{lemma}\label{lemma_hypothesis_wrt_bridge_theory_correspondence}
Suppose $H$ is a hypothesis wrt an induction field $\mathcal{I}_H$ and a bridge theory $F_i$ wrt $B, E$. Then $H$ is a hypothesis wrt an induction field $\mathcal{I}_H$ and a maximal bridge theory $F=B \cup \bar{E}$ wrt $B, E$.
\end{lemma}
\begin{proof}
By \fullref{definition_bridge_theory} $F_i$ is a bridge theory wrt $B, E$ iff $B \cup \bar{E} \models F_i$ by \fullref{maximal_bridge_theory_definition} iff $F \models F_i$.
$H$ is a hypothesis wrt $\mathcal{I_H}$, $F_i$. Hence by \fullref{definition_hypothesis_wrt_induction_field_bridge_theory} there is a ground hypothesis $H_g$ such that $H_g$ consists of instances from $H$,
$F_i \models \neg H_g$ and $H_g$ belongs to $\mathcal{I}_H$. Hence $F \models F_i \models \neg H_g$ and thus by \fullref{definition_hypothesis_wrt_induction_field_bridge_theory} $H$ is a hypothesis wrt $\mathcal{I}_H$ and a maximal bridge theory $F$.
\end{proof}

\begin{proposition}
\label{completeness_of_inverse_subsumption_with_maximal_bridge_theory}
\emph{Completeness of inverse subsumption with minimal complements with a maximal bridge theory}.
Let $H$ be a hypothesis wrt an induction field $\mathcal{I}_H$ and a bridge theory $F_i$ wrt $B, E$, then $H$ can be derived by inverse subsumption with minimal complements with a maximal bridge theory.
\end{proposition}

\begin{proof}
By \fullref{lemma_hypothesis_wrt_bridge_theory_correspondence} $H$ is a hypothesis wrt $\mathcal{I}_H$ and a maximal bridge theory $F=B \cup \bar{E}$.
Hence by \fullref{yamamoto2012inverseLemma2} $H \subsumes \tau(M(B \cup \bar{E} \cup Taut(\mathcal{I}_H))$.
\end{proof}

\subsubsection{Comparison with the original algorithm}
Minimal complements with a maximal bridge theory is a special case of the original minimal complements algorithm where the bridge theory $F$ is required to be maximal, i.e. $F=B \cup \bar{E}$. The requirement for a maximal bridge theory $B \cup \bar{E}$ results in an in efficiency if the background knowledge $B$ is large. The advantage of the restricted algorithm is its simplicity and the preservation of completeness for finding a hypothesis $H$ wrt $\mathcal{I}_H, F_i$ wrt $B,E$, therefore providing an opportunity to improve the efficiency in a different direction.

\subsection{Inverse subsumption with minimal complements and connected theory}
There may be a direct correspondence between the theory subsuming a target hypothesis constructed by minimal complements and the connected theory.

\begin{conjecture}
Let $P=\langle B, U, I \rangle$ be a definite open program, let $H$ be a definite, and $e$ an atom with $E^+=\{e\}$. Suppose that $H$ is an inductive solution for $P$ and $E^+$ wrt an induction field $\mathcal{I}_H$ and a bridge theory $F$.
Then $T=\tau(M(F \cup Taut(\mathcal{I}_H)))$ is a connected theory for $P$ and $E$ and $H \subsumes T$.
\end{conjecture}

\subsection{Negative examples}
For a non-monotonic consequence opearator $\models$, there are two possible constrains the negative examples may introduce on a hypothesis $H$ wrt $B, E^+$, $E^-$.
\begin{enumerate}
\item\label{first_negative_examples_constraint} $B \cup H \models \neg E^-$,
\item\label{second_negative_examples_constraint} $B \cup H \not\models E^-$.
\end{enumerate}
\ref{first_negative_examples_constraint} constraint implies \ref{second_negative_examples_constraint} assuming that $B \cup H \not\models false$, but not vice versa. Inverse subsumption with minimal complements algorithm covers the \ref{first_negative_examples_constraint} case since $B \cup H \models E = E^+ \cup \neg E^- = E^+ \land \neg E^-$ and a bridge theory used is a bridge theory wrt $B$ and both positive and negative examples. For the \ref{second_negative_examples_constraint} case, one has to construct a bridge theory wrt $B$, $E^+$ only, then check the constraint imposed by the negative examples $E^-$ separately. However, it is possible to construct a bridge theory $F_i$ wrt $B$ and $\{e\}$ for every ground example $e \in E^-$. Then one can still use the inverse subsumption algorithm to check \ref{second_negative_examples_constraint}.

\begin{proposition}\label{proposition_negative_examples_subsumption}
Let $H$ be a hypothesis wrt an induction field $\mathcal{I}_H$ and any bridge theory wrt $B, E+$. Then $B \cup H \not\models E^-$ iff $\forall e \in E^-. H \not\subsumes \tau(M(B \cup \{\neg e\} \cup Taut(\mathcal{I}_H)))$.
\end{proposition}
\begin{proof}
As $E^-$ is a disjunction of negative examples $e$, it is sufficient to prove
$B \cup H \not\models e$ iff
$H \not\subsumes \tau(M(B \cup \{\neg e\} \cup Taut(\mathcal{I}_H)))$ for an arbitrary negative example $e$ which is equivalent to proving
$B \cup H \models e$ iff $H \subsumes \tau(M(B \cup \{\neg e\} \cup Taut(\mathcal{I}_H)))$. But this follows from \ref{yamamoto2012inverseLemma2} and \ref{soundness_first_order_extension}.
\end{proof}

\subsubsection{Complete inverse subsumption operators}\label{complete_inverse_subsumption_operators}
Based on the work by Yamamoto et al. on the generalization operators\cite{yamamoto2008towards} it is proved that every theory $S$ subsuming a theory $T$ can be derived by antisumbsumption using two generalization operators dropping and antiinstantiation.

\begin{defn}
A logic theory $S$ is derivable from the logic theory $T$ by \emph{dropping} denoted $T \vdash_d S$ iff
$\forall D \in T. \exists C \in S. C \subseteq D$.
\end{defn}

\begin{exmp}
$\{p \lor q\} \vdash_d \{p, q\}$.
\end{exmp}

\begin{defn}
A logic theory $S$ is derivable from the logic theory $T$ by \emph{antiinstantiation} denoted $T \vdash_a S$ iff
$\forall D \in T. \exists$ a substitution $\theta$, $\exists C \in S. C\theta = D$.
\end{defn}

\begin{exmp}
If $T=\{p(X) \lor q(X), r(X)\}$, then $T\vdash_a\{p(a) \lor q(a), r(X)\}$,
but $T\not\vdash_a\{p(a) \lor q(b), r(X)\}$.
\end{exmp}

\begin{proposition}(Completeness of dropping and antiinstantiation for antisumption).
Suppose that a logic theory $S$ theory-subsumes a logic theory $T$, then $S$ is derivable by dropping and antiinstantiation from $T$:
$S \subsumes T \implies \exists U. T \vdash_d U \vdash_a S$.
\end{proposition}
\begin{proof}
Suppose that $S$ theory-subsumes $T$, then by \ref{definition_theory_subsumption} for any clause $D \in T$, there is a clause $C_D \in S$ such
that $C_D \subsumes D$, i.e. $\exists \theta_{D}. C_D \theta_D \subseteq D$.
Then construct a theory $U=\{C_D \theta_D | D \in T \}$.
Since $\forall D \in T \exists C_D \theta_D \in U. C_D \theta_D \subseteq D$, thus $T \vdash_d U$.
Since $\forall C_D \theta_D \in U \exists C_D \in S. C_D \theta_D=C_D \theta_D$, thus $U \vdash_d S$. Hence $T \vdash_d U \vdash_a S$.
\end{proof}

\subsection{Imparo is complete in inverse subsumption}
The use of the antientailment\cite{yamamoto2012inverse} in the computation of a hypothesis is an expensive operation as compared to the inverse subsumption. The use of the antientailment instead of the antisubsumption may not preserve the completeness. However, we prove an open problem from \cite{yamamoto2012inverse} that for every correct hypothesis $H$ wrt $B, E^{+}, E^{-}$ there is a connected theory $T$ wrt $B, E^{+}, E^{-}$ subsumed by it.
\begin{defn}
Let $P=\langle B, U, I \rangle$ be an open definite program, $H$ be a correct hypothesis wrt $P$ and a ground example $E$, then $H$ is derivable by
\emph{connected theory inverse subsumption}
iff there exists a connected theory $T$ for $P$ and $E$ such that $H \subsumes T$.
We denote the statement by $P, E \vdash_{CTIS} H$.
\end{defn}

\begin{thm}\label{yamamoto2012inverseTheorem4}\cite{yamamoto2012inverse}
Let $S$ be a ground clausal theory. Then, $M^2(S) = \mu(S)$ holds.
\end{thm}

\begin{proposition}
Let $H \in I_H$ be an inductive solution for $P$ and $E$ such that $Taut(I_H)=\emptyset$, then $H$ is derivable by connected theory inverse subsumption from $P, E$.
\end{proposition}
\begin{proof}
By completeness of connected theory generalization\ref{completeness_ctg} there exists a connected theory $T$ such that $H \models T$. $M(T)$ is a bridge formula.
Hence by \ref{yam1amoto2012inverseLemma2}
$H \subsumes \tau(M(M(T) \cup Taut(I_H))$. From the definition of the connected theory $\tau(T)=T$ and $\mu (T)=T$.
Therefore $\tau(M(M(T) \cup Taut(I_H))=\tau(M(M(T))=\tau(T)=T$ using
\ref{yamamoto2012inverseTheorem4}. Therefore $H \subsumes T$ as required.
\end{proof}
\begin{remark}
The previous proposition is stated and proved only to provide an alternative proof of a statement superceded by the stronger result \ref{completeness_ctis}.
\end{remark}

\begin{thm}\label{implicationByGroundClauses}
(Implication by Ground Clauses \cite{nienhuys1997foundations}). Let $\Sigma$ be a non-empty set of clauses,
and $C$ be a ground clause. Then $\Sigma \models C$ if and only if there is a finite set $\Sigma_g$ of ground
instances of clauses from $\Sigma$, such that $\Sigma_g \models C$.
\end{thm}

\begin{thm}\label{completeness_ctg}
\emph{Completeness of connected theory generalization}(Theorem4.6 in \cite{kimber2012learning})
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory generalisation.
\end{thm}
\begin{proof}\cite{kimber2012learning}
For the full proof, an interested reader is encouraged to read \cite{kimber2012learning}.
Since $H$ is a correct hypothesis for $P$ and $E$,
then $B \cup H \models E$ by definition.
Therefore, by \ref{implicationByGroundClauses}, there is a finite set $S$ of ground instances of clauses in $B \cup H$,
such that $S \models E$. Let $T = S \cap ground(H)$.
Since $T \subseteq S$, then $T$ is ground and finite, and
since $T \subseteq ground(H)$ then $H \models T$. 
Then Kimber proves that $T$ is a connected theory for $P$ and $E$.
\end{proof}

\begin{thm}
\emph{Completeness of connected theory inverse subsumption}.
\label{completeness_ctis}
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory inverse subsumption.
\end{thm}
\begin{proof}
Construct a connected theory $T=S \cap ground(H)$ for $P$ and $E$ as in the proof of \ref{completeness_ctg}.
Then $H \subsumes ground(H) \subsumes S \cap ground(H) = T$,
hence $H \subsumes T$ by transitivity as required.
\end{proof}