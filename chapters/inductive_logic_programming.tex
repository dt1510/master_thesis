\chapter{Inductive logic programming}\label{ch:inductive_logic_programming}
The objective of this chapter is to present concepts from Inductive logic programming (ILP) and their extensions that are going to be used for the classification of ILP systems.

\section{Inductive logic programming concepts}

\subsection{ILP task definition}
We present a definition of an ILP task of explanatory induction with positive examples, negative examples and a bias.

\subsubsection{Explanatory induction\cite{yamamoto2012inverse}}
\begin{defn}\cite{flach1996rationality} A learning problem of \emph{explanatory induction} is given logical theories background knowledge $B$, examples $E$ to find a hypothesis $H$ satisfying $E \subseteq Cn(B \cup H)$ and $false \not\in Cn(B \cup H)$ for some consequence operator $Cn$.
\end{defn}

This learning problem is alternatively called \emph{learning from entailment}\cite{muggleton1995inverse}\cite{de1997logical}. The examples $E$ can be conceptually divided into positive examples and negative examples $E=E^{+} \cup \neg E^{-}$ where $E, E^{+}, \neg E^{-}$ are conjunctions of logical statements, hence $E^{-}$ being a disjunction of logical statements.

\begin{exmp}\cite{explanatory_induction_example}
Let $E=\{man(adam), \neg man(alice), \neg man(susam)\}$. Then $E^{+}=\{man(adam)\}$,
$\neg E^{-} = \{\neg man(alice), \neg man(susam)\}=\neg man(alice) \land \neg man(susan)$, hence
$E^{-} = man(alice) \lor man(susan)$.
\end{exmp}

Therefore one can define explanatory induction with the negative examples.
\begin{defn}\label{explanatory_induction_with_negative_examples_definition}
A learning problem of \emph{explanatory induction with negative examples} is given logical theories background knowledge $B$, positive examples $E^{+}$, negative examples $E^{-}$ to find a hypothesis $H$ satisfying\\
1)$E^{+} \subseteq Cn(B \cup H)$\\
2)$E^{-} \not\in Cn(B \cup H)$.
\end{defn}
The consistency condition has become redundant since $false \in Cn(B \cup H$ implies $E^{-} \in Cn(B \cup H)$.

ILP systems we aim to classify restrict their hypothesis space by a bias. Consequently, we extend the definition of an ILP task further:
\begin{defn}\label{definition_explanatory_induction_with_bias}
A learning problem of \emph{explanatory induction with negative examples and a bias} is given logical theories background knowledge $B$, positive examples $E^{+}$, negative examples $E^{-}$, a bias $\mathcal{H}$ to find a hypothesis $H$ satisfying\\
1)$E^{+} \subseteq Cn(B \cup H)$\\
2)$E^{-} \not\in Cn(B \cup H)$\\
3)$H \in \mathcal{H}$.
\end{defn}

We refer to any of 3 possible ILP tasks as an ILP task of explanatory induction. Which definition is intended should be clear from the context depending on what input from $B, E^+, E^-, \mathcal{H}$ is given.

\begin{exmp}\label{explanatory_induction_example}
Let $E=\{mortal(aristotle)\}$, $B=\{man(aristotle)\}$,
$H_1=E$, $H_2=\{mortal(aristotle) \leftObjectImplies man(aristotle)\}$.
Then both $H_1$, $H_2$ are correct hypotheses explaining $E$ from $B$ for a bias $\mathcal{H}_{1,2}=\{H_1, H_2\}$, but only $H_1$ is a correct hypothesis for a bias $\mathcal{H}_1=\{H_1\}$.
\end{exmp}

\subsubsection{Correct hypothesis and its deviations\cite{nienhuys1997foundations}}\label{correct_hypothesis}
A hypothesis $H \in \mathcal{H}$ satisfying the conditions in explanatory induction is called a correct hypothesis.
\begin{defn}
A hypothesis $H$ is \emph{correct} wrt background knowledge $B$, positive examples $E^+$, negative examples $E^-$ iff\\
1) $E^+ \subseteq Cn(B \land H)$,\\
2) $E^- \not\in Cn(B \land H)$.
\end{defn}

When a hypothesis does not satisfy the two conditions it deviates from correctness.

\begin{defn}
Let $H$ be a theory, and $E^+$ and $E^-$ be sets of clauses.
$H$ is \emph{too strong} wrt $B$, $E^-$ iff
$E^- \in Cn(B \land H)$.
$H$ is \emph{too weak} wrt
$E^+$ iff $E^+ \not\subseteq Cn(B \land H)$.
$H$ is \emph{overly general} wrt $B$, $E^+$, $E^-$ iff
$E^+ \subseteq Cn(B \land H)$, $E^- \in Cn(B \land H)$.
$H$ is \emph{overly specific} wrt $B$, $E^+$, $E^-$ iff
$E^+ \not\subseteq Cn(B \land H)$, $E^- \not\in Cn(B \land H)$.
\end{defn}

\subsection{Inverse Entailment}
Inverse Entailment is a correspondence between an induction and a deduction:
\begin{thm}\cite{kimber2012learning}
Let $B$ be a Horn program, and let $h$ and $e$ be
Horn clauses. Then $B \wedge h \models e \iff B \wedge \neg e \models \neg h$.
\end{thm}

\subsubsection{IE algorithm\cite{yamamoto2012inverse}}\label{inverse_entailement_algorithm}
Progol, Xhail, Imparo are based on the principle of the inverse entailment. By the principle of the inverse entailment $B \cup H \models E$ iff
$B \cup \neg E \models \neg H$. A hypothesis $H \in \mathcal{H}_2$ is a solution to the problem of the explanatory induction $(B,E^{+},E^{-},\mathcal{H})$ iff
$B \cup H \subseteq Cn(E^{+} \union \overline{E^{-}})$,
 $false \not\in Cn(E^{+} \cup E^{-})$, $\mathcal{H}_2 \subseteq \mathcal{H}$
iff
$B \cup H \subseteq Cn(E^{+} \union \overline{E^{-}})$,
 $false \not\in Cn(E^{+} \cup E^{-})$, $\mathcal{H}_2 \subseteq \mathcal{H}$
 
They compute the hypothesis $H$ in two steps:
1. constructing an intermediate theory, 2. generalizing its negation into the hypothesis with the inverse of the entailment relation.

\subsection{Input and output of ILP system}
An ILP system is a program that solves an ILP task.
An input to an ILP system is a logic program that can be conceptually divided into 3 parts:
\begin{itemize}
\item objectlevel statements: background knowledge, examples: positive and negative,
\item effective metalevel statements,
\item non-effective metalevel statements.
\end{itemize}

Objectlevel statements provide the main setting of a learning problem and concern with what is known: the language of enquiry $L$, a set of observations, etc.

Effective metalevel statements provide a refinement of the main setting and concern with how the learning problem may be solved and what form the solution is expected to take. Their specification converts a learning problem into a potentially easier, more directed and concrete \emph{ILP task}.
\emph{Mode declarations} and \emph{determinations} are effective metalevel statements specifying a constraint on a language $L$ to determine the search space $\mathcal{H}$ of possible hypotheses. The other effective metalevel statements may impose further \emph{metalevel constraints} on $L$. The set of constraints on $L$ determining the search space $\mathcal{H}$ is called a \emph{language bias}.
Apart from the specification of a language bias, effective metalevel statements contain search control statements that affect the form the computation of a hypothesis takes place and contain decision elements: preferential bias - which hypothesis should be preferred over the other, which parts of the background knowledge should be considered, etc.

Non-effective metalevel statements do not affect the result of the computation of a hypothesis. They are either required syntactic sugar or their primary purpose concerns different aspects of an ILP system, e.g. to provide the statistics about the computation to the user.

The primary output of an ILP system is a solution (a hypothesis or their set) to a refined learning problem.

\section{ILP systems}
An ILP system is a program that takes as an input background knowledge $B$ and examples $E$ and produces as an output an explanation called a hypothesis $H$ explaining the examples in terms of the background knowledge $B$, i.e. $E \subseteq Cn(B \cup H)$.
We give a list of ILP systems Progol, Aleph, Toplog, Xhail, Imparo, Tal.

\subsection{Progol}
We paraphrase the introduction to Progol from its official website\cite{muggleton1999progolWebsite}:
\begin{quote}
Progol combines Inverse Entailment with general-to-specific search through a refinement graph. Inverse Entailment is used with mode declarations to derive the most-specific clause within the mode language which entails a given example. This clause is used to guide a refinement-graph search by performing an admissible A*-like search, guided by compression, over clauses which subsume the most specific clause. Progol allows arbitrary Prolog programs as background knowledge and arbitrary definite clauses as examples.
\end{quote}

\subsection{Aleph\cite{aleph2007}}
Aleph is an ILP system based on Progol developed in order to understand the concepts behind the inverse entailment\cite{muggleton1995inverse} influenced by the ideas from other ILP systems: CProgol, FOIL, FORS, Indlog, MIDOS, SRT, Tilde, and WARMR. 

\subsection{Toplog\cite{santos2008toplogWebsite}\cite{muggleton2008toplog}}
Toplog is an ILP system extending the use of Inverse Entailment by a top theory $\top$ bias.
The search space is restricted by requiring that each hypothetised clause of a hypothesis $H$ must be entailed by the top theory $\top$.

\subsubsection{MC-Toplog\cite{muggleton2012mc}}
MC-Toplog is a sequel of Toplog, derives hypotheses like Toplog, in addition allows multiple clauses in a hypothesis. It restricts a hypotheses space to clauses entailing generalization of multiple examples as opposesed to Toplog that could generalizing only a single example.

\subsection{Hail\cite{ray2003hybrid}\cite{ray2005phdHybrid}}
A Hail (Hybrid abductive inductive learning) ILP system extending the Progol's incomplete method of bottom generalization to a more complete method of \emph{kernel set generalization} with the methods of abductive logic programming.

\subsection{Xhail\cite{ray2009nonmonotonic}}
Xhail standing for eXtended Hybrid abductive learning is an ILP system extending Hail's methodology from Horn theories to normal logic programs.
Xhail version 2 was used for the experiments presented in this thesis.

\subsection{Imparo\cite{kimber2012learning}}
Imparo is an ILP system based on a general Induction on Failure theoretical framework extending the Hail's incomplete method of kernel set generalization to the complete method of connected theory generalization.

\subsection{Tal\cite{corapi2010inductive}\cite{corapi2011tal}}
Tal (Top-directed Abductive Learning) is a non-monotonic top-down ILP system complete for background theories and hypotheses as normal logic programs. 
Tal relies on mapping an ILP problem into an equivalent ALP one. This enables the
use of established ALP proof procedures and the specification of richer language bias with integrity constraints. The mapping provides a principled search space for an ILP problem, over which an abductive search is used to compute inductive solutions.

\subsection{Other systems}
Other ILP systems include Metagol, Golem, Spectre, EBG, Alecto, FOIL, Linus, Marvin, Mis, Confucius, Quinlan, ASPAL, Hyper,  Tilde, CF-induction.

\subsection{Selecting ILP systems for classification}
Classification of all ILP systems would be out of the scope of this project and would not provide much more value than a classification of well chosen ILP systems.
For selection of ILP systems for classification the following criteria were taken into the consideration:

\begin{itemize}
\item a popularity of an ILP system,
\item a novelty and contribution of an ILP system,
\item an access: an availability of a general purpose implementation, documentation, research findings, easiness of communication (in person vs. over distance) with the original authors and respective experts.
\end{itemize}

In the end the author has decided to classify Progol, Aleph, Toplog, Xhail, Imparo, Tal; Aleph being the best documented working system and others having been developed in the author's department with a general purpose implementation. The author would like to note based on the private communication with the respective experts that MC-Toplog does not have a general purpose implementation, a user needs to write a specific top theory for each learning problem and adapt the current implementation to support it. Metagol\cite{muggleton2014meta}, being a very interesting and novel system, similarly, as of time of classification did not have a general purpose implementation.