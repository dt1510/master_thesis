\chapter{Classification of ILP systems}\label{chap:classification_of_ilp_systems}

In the chapter \nameref{chap:issues_in_ilp} we analysed the central issues in ILP by providing a brief explanation of an importance of an issue and a list of the properties defining an issue. In this chapter we classify ILP systems and theoretical frameworks based on the use of the defining properties to approach solving a particar issue.

We will define an ILP task $I \mapsto O$ by its space of possible inputs $I$ and the decription of the desired output $O$. Thus an ILP task is a \emph{function problem} as opposed to a decision problem. We think of an ILP system solving such an ILP task as an \emph{algorithm} computing the function $I \mapsto O$.

\section{ILP task definition}
In \nameref{sec:ilp_task_definition} of the chapter \nameref{chap:issues_in_inductive_logic_programming} we presented machine learning problems: generalization and explanatory induction; together with the properties expressivity, positive examples, negative examples, semantics.

Although on the surface there may seem to be a standard definition of an ILP task, in this section we demonstrate that when specifying an ILP task completely, each ILP system classified solves a different learning problem. This mutual incompatibility has a major impact how we can assess, compare and classify ILP systems.

We explain the importance of defining an ILP task completely, present building blocks for a definition of an ILP task, construct an ILP definition for each ILP system, classify the ILP systems based on an ILP task they solve and lastly provide a generalized unifying definition of an ILP task for all assessed ILP systems to lay the ground for their comparisons and classification in the subsequent chapters.

\subsection{Importance of complete definitions}
Since an ILP task is a function problem $f:I \mapsto O$ for a specified input $I$ and a corresponding output $O$, its incomplete definition may not specify a \emph{unique deterministic function} $f$. An example of an incomplete definition is explanatory induction: given $I=\langle B, E \rangle$ find a correct $O=H$, i.e. satisfying
$E \in Cn(B \cup H)$ and $false \not\in Cn(B \cup H)$.
\begin{exmp}\label{explanatory_induction_definition_incompleteness}
Let $E=\{mortal(aristotle)\}$, $B=\{man(aristotle)\}$,
$H_1=E$, $H_2=\{mortal(aristotle) \leftObjectImplies man(aristotle)\}$.
Then both $H_1$, $H_2$ are correct hypotheses explaining $E$ from $B$. Therefore there are at least 2 different functions $\langle B, E \rangle \mapsto H_1$ and
$\langle B, E \rangle \mapsto H_2$ specifying the output $O=H$.
\end{exmp}

The importance of an issue of an incomplete definition becomes apparent when defining a non-ILP function problem as our mind is not biased towards the acceptance of the incompleteness of a definition encountered in the ILP literature.

\begin{exmp}
Define a function problem $f:I \mapsto O$ by given an odd number, find an even number. Then constant functions $f_2:I \mapsto 2$, $f_4:I \mapsto 4$, ..., functons $g_1:I \mapsto I+1$, $g_3:I \mapsto I+3$ and many others are compatible with an incomplete definition of a function $f$. However, when the definition of a function problem is completed to given an odd number $I$ to find smallest even number greater than $I$, then unambiguously $f=g_1$.
\end{exmp}

Not specifying an ILP task completely may result in designing an ILP system solving a different function problem than the one intended.

\subsection{Buildings blocks of ILP task definition}\label{subsec:building_blocks_of_ilp_task_definition}
In this subsection we define an ILP task definition, instantiate the definition with an ILP task of generalization, extend the definition of generalization by the notions encountered in ILP tasks of ILP systems we classify. The objective of this subsection is to enumerate the building blocks of an ILP task definition and the rules that make the definition more complete from which we can construct definitions of ILP tasks and classify their respective ILP systems later.

\begin{defn}\label{ilp_task_definition}
An \emph{ILP task definition} is a tuple $D=\langle f, dom(f), cod(f), R \rangle$ where a (possibly non-deterministic) function $f$ is called an \emph{ILP task}, $dom(f)$ is a set of \emph{inputs} vectors
$I=\langle \overrightarrow{i} \rangle$, $cod(f)$ is a set of \emph{output} vectors $O=\langle \overrightarrow{o} \rangle$ and $R$ is a set of rules (logical sentences) defining $f$,
i.e. $O \in f(I) \iff R \cup \{O \in f(I)\} \not\models false$ for a classical consequence operator $\models$. A vector component of an input(output) vector $\overrightarrow{i}$($\overrightarrow{o}$) is called an \emph{input component}(\emph{output component}) of an ILP task $f$. A \emph{structure component} of an ILP task is its input or output component. A \emph{rule} of an ILP task is a formula $\phi \in R$.
\end{defn}

\begin{remark}
We will often specify $R$ only by its subset of sentences and the rest will be specified informally or expected from the reader to be deduced from the context in order to avoid unnecessarily long specifications of $R$ and the lost of the focus. We may specify directly a possible input vector $I$ without specifying $dom(f)$ explicitly, it is be understood that $dom(f)$ is a set of all such input vectors $I$. Similarly, $cod(f)$ is a set of all possible output vectors $O$. The notation is abused for vectors with one element, e.g. $H=\langle H \rangle$.
\end{remark}

\begin{exmp}
The definition of explanatory induction is $D=\langle f, dom(f), cod(f), R \rangle$ where $I=\langle B, E \rangle$, $O=H$, $R=\{E \subseteq Cn(B \cup H), false \not\in Cn(B \cup H)\}$ for some consequence operator $Cn$. Input components of explanatory inductions are $B, E$. A hypothesis $H$ is an output component of explanatory induction.
\end{exmp}

\subsubsection{Completeness of definitions of ILP task}
By the example \fullref{explanatory_induction_definition_incompleteness} the definition of explanatory induction does not specify a unique deterministic ILP task, we say such a definition is incomplete.

\begin{defn}
A definition $D=\langle f, dom(f), cod(f), R \rangle$ is a \emph{complete ILP task definition} iff $f:I \mapsto O$ is deterministic. $D$ is \emph{incomplete} iff $D$ is not complete.
\end{defn}

Since an incomplete definition results in an ambiguous ILP task definition it will be in our interest to make the definitions more complete by introducing additional rules defining an ILP task.

\begin{defn}
Let $f:X \mapsto Y$ be a (possibly non-deterministic) function, we say that a (possibly non-deterministic) function $g:X \mapsto Y$ is a \emph{possible function} of $f$ iff
$\forall x \in X. g(x) \subseteq f(x)$. We denote the set of all possible functions of $f$ by $\Box f$.
\end{defn}

\begin{remark}
The only possible function of a deterministic function is the function itself, i.e. $\Box f = \{f\}$.
\end{remark}

\begin{defn}
A definition $D_1=\langle f_1, dom(f_2), cod(f_2), R \rangle$ is \emph{more complete} than the definition $D_2=\langle f_1, dom(f_2), cod(f_2), R \rangle$ iff
$f_2$ is a possible function of $f_1$.
\end{defn}

\begin{corollary}\label{corollary_more_complete_1}
Let $D_1=\langle f_1, \mathcal{I}, \mathcal{O}, R_1 \rangle$, $D_2=\langle f_2, \mathcal{I}, \mathcal{O}, R_2 \rangle$ be definitions of an ILP task.
If $R_2 \models R_1$, then $D_2$ is more complete than $D_1$.
\end{corollary}

\begin{proof}
Follows directly from the definitions.
\end{proof}

\subsubsection{Extension of ILP task definition}
To provide a more accurate and complete specification of an ILP task, its definition can be extended.

\begin{defn}\label{definition_ilp_task_definition_extension}
Let $D_1=\langle f_1, \mathcal{I}_1, \mathcal{O}_1, R_1 \rangle$ be a definition of an ILP task. A definition $D_2=\langle \mathcal{I}_2, \mathcal{O}_2, R_2 \rangle$ is an \emph{input component extension}(\emph{output component extension}) of a definition $D_1$ iff $\langle \overrightarrow{i_1} \rangle \subset \langle \overrightarrow{i_2} \rangle$. $D_2$ is a \emph{component extension} of $D_1$ iff $D_2$ is an input or output component extension of $D_1$. $D_2$ is a \emph{rule extension} $D_1$ iff $R_1 \subset D_2$. $D_2$ is an \emph{extension}(or an \emph{extended definition}) of $D_1$ iff $D_2$ is a component or rule extension of $D_1$.
A component $i \in \langle \overrightarrow{i_2} \rangle \setminus \langle \overrightarrow{i_1}$ with its set is an \emph{extending input component} of $D_2$ for $D_1$.
A component $o \in \langle \overrightarrow{o_2} \rangle \setminus \langle \overrightarrow{o_1}$ is an \emph{extending output component} of $D_2$ for $D_1$.
A component is an \emph{extending component} iff it is an extending input component or an extending output component.
A rule $r \in R_2 \setminus R_1$ is an \emph{extending rule} of $D_2$ for $D_1$.
\end{defn}

\begin{remark}
An extension of an ILP task definition defines a different ILP task.
\end{remark}

\begin{remark}
\emph{Extending rules of a component} are the rules which define the component (i.e. rules containing the symbols present in the denotation of the of the component). When a definition is extended by an extending component, the corresponding extending rules of an extending component need to be added as extending rules to the new definition.
\end{remark}

A correspondence between an ILP task defined by $D$ and an ILP task defined by an extension of $D$ is shown.

\begin{corollary}
Let $D_2$, $D_1$ be ILP task definitions. If $D_2$ is a rule extension of $D_1$, then $D_2$ is more complete than $D_1$.
\end{corollary}
\begin{proof}
Follows from \fullref{definition_ilp_task_definition_extension} and \fullref{corollary_more_complete_1}.
\end{proof}

\begin{proposition}
Let $D_2=\langle f_2, dom(f_2), cod(f_2), R_2 \rangle$,
$D_1=\langle f_1, dom(f_1), cod(f_1), R_1 \rangle$ be ILP task definitions. Suppose that $D_2$ with an input $I_2$ is an input extension of $D_1$ with an input $I_1$, but not an output extension, not a rule extension. Then $f_2(I_2)=f_1(I_1)$.
\end{proposition}

\begin{proof}
$O \in f_1(I_1) \iff R_1 \cup \{O \in f_1(I_1)\} \not\models false \iff  R_2 \cup \{O \in f_2(I_2)\} \not\models false \iff O \in f_2(I_2)$ since $O \in f_2(I_2)$ is defined in terms of $I_1 \subseteq I_2$ and for the defining rules $R_1(f_1 \mapsto f_2) = R_2$, $R_2(f_2 \mapsto f_1)=R_1$ for substitutions $f_1 \mapsto f_2$, $f_2 \mapsto f_1$.
\end{proof}

\subsubsection{Generalization}
Recall \fullref{generalization}.
\begin{defn}
An ILP task of \emph{generalization} is defined by $D=\langle f, L, L, R \rangle$ where $R=\{\forall I \in L. Cn(I) \subseteq Cn(f(I))\}$ for some logic language $L$ and a consequence operator $Cn:L \to L$.
\end{defn}

An example of an ILP system that puts background knowledge and examples into one theory which is then generalized is Progol\cite{muggleton1995inverse}. An interested reader should consult Progol implementation referenced in the cited paper.

\subsubsection{Hypothesis}
The properties arising from an ILP task with a hypothesis provide significant means of comparison. A hypothesis is part of the definition of explanatory induction, but it is not present in the definition of generalisation. To be able to compare ILP systems like Progol whose ILP task is an extension of generalisation with ILP systems like Toplog, Imparo, Tal whose ILP task is an extension of explanatory induction we define a hypothesis for the definition of generalization.

\begin{defn}
Let $D=\langle f, L, L, R \rangle$ be a definition of generalization.
Then $H \subseteq L$ is a \emph{hypothesis} for $I \in dom(f)$ iff $\exists O \in f(I). H=O \setminus I$.
\end{defn}

\begin{exmp}
Let $B, E, H$ be background knowledge, examples, a correct hypothesis wrt $B, E$ such that $E \cap H=\emptyset$.
Then $Cn(B \cup E) \subseteq Cn(B \cup H)$, hence $O=B \cup H$ is a generalization of $I=B \cup E$.
$I, O$ define the hypothesis $H'=O \setminus I=(B \cup H) \setminus (B \cup E)=H \setminus E=H$ which is the original hypothesis we started with.
\end{exmp}

\subsubsection{Explanatory induction}
Most of the ILP systems solve an ILP task of an extended form of explanatory induction.\cite{kimber2012learning}\cite{aleph2007}\cite{nienhuys1997foundations}

\begin{defn}\label{defn_explanatory_induction}
An ILP task of \emph{explanatory induction} is defined by $D=\langle f, dom(f), cod(f), R \rangle$ where $I=\langle B, E \rangle$, $O=H$,
$R=\{Cn(B \cup E) \subseteq Cn(B \cup H), false \not\in Cn(B \cup H)\}$ for some consequence operator $Cn$.
\end{defn}
A division of examples $E=E^{+} \cup \neg E^{-}$ into positive examples $E^{+}$ and negative examples $E^{-}$ results in an alternative definition.

\begin{defn}
An ILP task of \emph{explanatory induction} is defined by $D=\langle f, dom(f), cod(f), R \rangle$ where $I=\langle B, E^{+}, E^{-} \rangle$, $O=H$,
$R=\{Cn(B \cup E^{+}) \subseteq Cn(B \cup H), E^{-} \not\in Cn(B \cup H)\}$ for some consequence operator $Cn$.
\end{defn}

See an example \fullref{explanatory_induction_example}.

\subsubsection{Multiexplanatory induction}
While the output of explanatory induction is an explanation $H$ for $E$ in terms of $B$, we define multiexplanatory induction as an ILP task allowing multiple explanations for $E$ in terms of $B$ on the output.

\begin{defn}\label{defn_multiexplanatory_induction}
An ILP task of \emph{multiexplanatory induction} is defined by $D=\langle f, dom(f), cod(f), R \rangle$ where $I=\langle B, E \rangle$, $H \in O$,
$R=\{Cn(B \cup E) \subseteq Cn(B \cup H), false \not\in Cn(B \cup H)\}$ for some consequence operator $Cn$.
\end{defn}

An example of an ILP system solving an extension of an ILP task of multiexplanatory induction is Tal.\fullref{subsec:tal_multiple_solutions} There is an obvious correspondence between explanatory induction and multiexplanatory induction.

\begin{proposition}
Let $f_1:I_1 \mapsto O_2$ be an ILP task of explanatory induction, $f_2:I_2 \mapsto O_2$ an ILP task of multiexplanatory induction. If $I_1=I_2$, then
$f_1(I_1) \in f_2(I_2)$.
\end{proposition}

\begin{proof}
Follows from the definitions \fullref{defn_explanatory_induction}, \fullref{defn_multiexplanatory_induction}.
\end{proof}

\subsubsection{Hypothesis bias}
The non-determinism of an ILP task is reduced by an introduction of additional constrains. A set of the constrains on a hypothesis for an ILP task are a hypothesis bias.

\begin{defn}
An ILP task of \emph{explanatory induction with a hypothesis bias} is defined by $D=\langle f, dom(f), cod(f), R \rangle$ where $I=\langle B, E, \mathcal{H}\rangle$, $O=H$,
$R=\{Cn(B \cup E) \subseteq Cn(B \cup H), false \not\in Cn(B \cup H), H \in \mathcal{H}\}$ for some consequence operator $Cn$. A set $\mathcal{H} \subseteq \powerset{L}$ for a logic language $L$ is called a \emph{hypothesis bias} on a language $L$.
\end{defn}

\begin{remark}
A hypothesis bias $\mathcal{H}$ is an input extension of explanatory induction with its extending rules $R$.
\end{remark}

\begin{exmp}\label{explanatory_induction_hypothesis_bias}
Let $B = \{cook(alice), female(alice)\}$, $E=\{woman(alice)\}$,
$H_1=\{woman(alice) \leftObjectImplies cook(alice)\}$,
$H_2=\{woman(alice) \leftObjectImplies female(alice)\}$,
$\mathcal{H}=\{H_1\}$.
Then $H_1$ and $H_2$ are possible outputs for an input $I=\langle B, E\rangle$ of explanatory induction. However, only $H_2$ is an output for an input $I'=\langle B, E, \mathcal{H}\rangle$ of explanatory induction with a hypothesis bias.
\end{exmp}

\begin{exmp}
Let $\mathcal{H} \subseteq \powerset{L}$ be a set of definite clauses. Then $\mathcal{H}$ is a hypothesis bias.
\end{exmp}

\begin{exmp}
Recall \fullref{subsec:background_language_bias}.
Let $M$ be a set of mode declarations, $D$ an ordered list of determinations, $C$ a set of the metaconstraints.
Then $L(M)$, $L(D)$, $L(C)$ are hypothesis biases.
\end{exmp}

Since the hypothesis bias is a set of the possible hypotheses, we have the obvious proposition.
\begin{proposition}
Let $\mathcal{H}_1$, $\mathcal{H}_2$ be hypothesis biases on a language $L$, then the following are hypothesis biases:
1) $\powerset{L} \setminus \mathcal{H}_1$, 2) $\mathcal{H}_1 \cap \mathcal{H}_2$,
3) $\mathcal{H}_1 \cup \mathcal{H}_2$.
\end{proposition}

\begin{proof}
Clear.
\end{proof}

\paragraph{Language bias}
One may define a hypothesis bias $\mathcal{H}$ by enumerating all the hypotheses $H \in \mathcal{H}$. If $\#\mathcal{H}$ is large, it may be impractical. ILP systems specify constraints on a hypothesis space with alternative constructions\fullref{subsec:background_language_bias}:
mode declarations, determinations and metaconstraints.
By \ref{proposition_metaconstraints_production_field}, \ref{md_d_pf_correspondence_proposition} these constraints are expressible with a production field. Therefore we define an ILP task with a production field where the production field can be replaced by a more specific construction.

\begin{defn}
An ILP task of \emph{explanatory induction with a production field} is defined by $D=\langle f, dom(f), cod(f), R \rangle$ where $I=\langle B, E, \mathcal{P}\rangle$, $O=H$,
$R=\{Cn(B \cup E) \subseteq Cn(B \cup H), false \not\in Cn(B \cup H), H \in \mathcal{H}, Cond(H)\}$ for some consequence operator $Cn$ where a production field $\mathcal{P}=\langle Lit, Cond \rangle$.
\end{defn}

\subsubsection{Preferential bias}
Instead of introducing constrains on a hypothesis, one may introduce constrains as a relation on pairs of hypotheses.

\begin{defn}
An ILP task of \emph{explanatory induction with a preferential hypothesis bias} is defined by $D=\langle f, dom(f), cod(f), R \rangle$ where $I=\langle B, E, <_H\rangle$, $O=H$,
$R=\{Cn(B \cup E) \subseteq Cn(B \cup H), false \not\in Cn(B \cup H), \forall H_2. (Cn(B \cup E) \subseteq Cn(B \cup H_2), false \not\in Cn(B \cup H_2) \implies H_2 <_H H \lor H_2 =_H H \}$ for some consequence operator $Cn$. A relation $<_H \subseteq \powerset{L} \times \powerset{L}$ is called a \emph{preferential hypothesis bias}.
\end{defn}

\begin{exmp}
Let $B, E, H_1, H_2$ be as in an example \fullref{explanatory_induction_hypothesis_bias}.
Let $<_H=\{(H_1, H_2)\}$ be a preferential hypothesis bias.
Then $H_2$ is a possible output for an input $I=\langle B, E, <_H \rangle$ of explanatory induction with a preferential hypothesis bias, but $H_1$ is not.
\end{exmp}

\subsubsection{Integrity constraints}

\paragraph{Progol}
\begin{lstlisting}
:- op(40,xfx,left_of)?
:- op(40,xfx,supports)?
:- op(40,xfx,touches)?
\end{lstlisting}

\paragraph{Toplog}
\begin{lstlisting}
:-set(maximum_literals_in_hypothesis)
:-set(example_inflation, 10).
\end{lstlisting}

\paragraph{Imparo}
\begin{lstlisting}
:-set_max_clause_length(5).
:-set_max_clauses(1).
:-set_verbose(1).
:-set_connected(1).
:-set_max_var_depth(3).
\end{lstlisting}

\paragraph{Tal}
\begin{lstlisting}
%Maximum number of body literals
option(max_body_literals, 5).

%Maximum number of rules
option(max_num_rules, 5).

%Maximum depth of the proof
%option(max_depth, 400).
option(strategy, breadth).
option(ic_check, true).
\end{lstlisting}

\subsubsection{Probabilistic setting}

\subsubsection{Approximating requirements}
Toplog - hypothesis.

\subsubsection{Score function}

\subsubsection{Semantics}

\subsubsection{Other constructions}
The building blocks and the rules we have not considered are:
\begin{itemize}
\item types on the ground instances in a logic program as in the appendix,
\item induction field\cite{yamamoto2012inverse} and production field\cite{inoue2004induction},
\item episodes in metalearning framework\cite{muggleton2013meta}.
\end{itemize}

\subsubsection{Summary of ILP task definitions}

\subsection{Classification ILP systems by an ILP task definition}
The objective of this subsection is to summarise ILP task definitions for each ILP system classified from the building blocks of an ILP task definition in the preceeding subsection \fullref{subsec:building_blocks_of_ilp_task_definition} and to classify ILP systems by their correspondent ILP task definition.

\subsubsection{Definitions of an ILP task for classified ILP systems}

\subsubsection{Comparisons of ILP systems based on their ILP task definition}

\subsection{A unified definition of ILP task}
We are interested in comparing the ILP systems, considering the specifics of ILP systems, each does not try to solve the same ILP problem. We search a way to compare these mutually incompatible systems. One way to establish the possibility of the comparison would be to unify these incopabilities by generalizing and abstracting. However, this means we may lose some of the information about the specifics of each system. First we provide the problem settings for each ILP system.

Let $E$ denote examples, $E^{+}$ positive examples, $E^{-}$ negative examples, $B$ the background knowledge, $\mathcal{H} \subseteq \powerset{L}$ the language bias. Let $Cl:\powerset{L} \to \powerset{L}$ be the classical monotonic consequence operator, $\models$ a non-monotonic consequence operator. Here by the overline on negative examples $\overline{E^{-}}$ we denote the set of the negated sentences of $E^{-}$. When specifying the consistency condition for the negative examples by $B \cup H \not\models E^{-}$ we mean that no example $e^{-} \in E^{-}$ should be implied by $B \cup H$, therefore $E^{-}$ is meant to be a disjunction (rather than a conjunction like $E^{+}$) of examples. Therefore, $\overline{E^{-}} \equiv \neg E^{-}$.

\begin{center}
    \begin{tabular}{ | l | p{15cm} | l | p{5cm} |}
    \hline
    ILP system & \\ \hline
    Aleph & Given $E^{+}$, $E^{-}$, $B$, $\mathcal{H}$, $B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$\\ \hline
    Progol & Given $B$, $\mathcal{H}$, find the most general $B_2 = B' \land H$ where $B' \subseteq B$, $H \in \mathcal{H}$, $B_2 \models B, B_2 \not\models false$\\ \hline
    Toplog & Given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$ \\ \hline
    Imparo & Given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$ \\ \hline
    Tal & Given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $\mathcal{H}_2 \subset \mathcal{H}. \forall H \in \mathcal{H}_2. B \land H \models E^{+}, B \land E^{-} \not\models false$ \\ \hline
    Xhail & Given $E^{+}$, $E^{-}$, $B$, $\mathcal{H}$, find $H \in \mathcal{H}$. $E^{+} \land \overline{E^{-}} \subseteq Cl(B \land H), false \not\in Cl(B \land H)$\\ \hline
    \hline
    \end{tabular}
\end{center}

Aleph, Toplog and Imparo aim to solve the same ILP problem called \emph{explanatory induction}:
given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$

The Progol ILP problem is a generalization of the explanatory induction: given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$,
let $B_P=E^{+} \land \overline{E^{-}} \land B$ be the Progol background knowledge. Then Progol finds the most general $B_2 = B' \union H_P$ where $B' \subseteq B_P, H_P \in \mathcal{H}, B_2 \models B_P, B_2 \not\models false$.
Then $(B' \union H_P) \backslash B=E'^{+} \land \overline{E'^-} \land H_P = H$ where $E'^+ \subseteq E^+, E'^- \subseteq E^-$. Then $H$ is the solution of the problem of the explanatory induction since $B \land H \models E^{+}, B \land H \not\models E^{-}, B \land H \not\models false$. We could generalize the problem to Progol ILP problem level, however, as Progol is the only system with so significantly differing problem system, we rather proceed in the reverse direction, we specialize the Progol ILP problem to the level of the explanatory induction as explained above. Hence we will think of Progol as producing a hypothesis explaning the examples from the background knowledge. Some information about the true nature of the Progol will become invisible in the comparisons, on the other hand, more information about the other systems will remain available.

Let $Cn:\powerset{L} \to \powerset{L}$ be a consequence operator, then generalize the definition of an ILP problem for Aleph, Toplog, Imparo, Tal and Xhail:
given $E^{+}, E^{-}, B, \mathcal{H}, false \not\in Cn(B \land E^{+} \land E^{-})$,
find $\mathcal{H}_2 \subset \mathcal{H}. \forall H \in \mathcal{H}_2. E^{+} \land \overline{E^{-}} \subseteq Cn(B \land H), false \not\in Cn(B \land H)$. Call this generalization of the explanatory induction, Tal ILP problem, Xhail ILP problem to be \emph{multi-explanatory induction} as it provides possible several epxlanations $\mathcal{H}_2$ for the observations. Clearly, multi-explanatory induction encompasses the specialization of the Progol problem. For systems other than Tal, $\#\mathcal{H}_2=1$. For Xhail, $Cn=Cl$, for others $Cn=\models$.

\section{Hypothesis selection}

\section{Hypothesis search}

\section{Bias}
ILP systems use bias to refine their search space $\mathcal{H}$, to induce a preference relation on the space of hypotheses and to control their search. The objective of this section is to classify the ILP systems based on the biases they use and the capabilities of their biases of a certain type.

\subsection{Language bias}\label{subsec:classification_language_bias}
The objective of this subsection is to classify ILP systems based on their peculiarities related to the language bias: mode declarations, determinations and metaconstraints.

\subsubsection{Mode declarations}
ILP systems are classified here based on their adaptations of the mode declarations.
The following mode declarations are excerpts and modifications of the examples provided with an implementation of a specific ILP system.

\paragraph{Progol}
Progol supports the mode declarations as defined in \fullref{background_mode_declarations}.

\begin{exmp}\cite{muggleton1999progolWebsite}
\begin{lstlisting}
:- modeh(1,class(+animal,#class))?
:- modeb(1,has_gills(+animal))?
:- modeb(*,habitat(+animal,#habitat))?
:- modeh(1,append([+constant|+clist],+clist,[-constant|-clist]))?
:- modeh(1,append(+clist,+clist,-clist))?
\end{lstlisting}
\end{exmp}

\paragraph{Aleph}
Aleph's mode declarations are compatible\cite{aleph2007} with the mode declarations specified by Progol.
\begin{exmp}
\begin{lstlisting}

:-modeh(1, woman(+person)).
:-modeb(1, female(+person)).
:-modeb(*, english_couple(+person, -person)).
:-modeb(2, english(+person)).
\end{lstlisting}
\end{exmp}

\paragraph{Toplog}
Toplog's modeb declarations are compatible with Progol up to specification of the atom. Specification of the recall is optional, however, for modeh declarations recall cannot be specified\cite{santos2008toplogWebsite}.
\begin{exmp}\cite{santos2008toplogWebsite}
\begin{lstlisting}
:-modeh(append(+list,+list,-list)).
:-modet(2, l2ht(+list/[H|T], -any/H, -list/T)).
:-modeb(append(+list,+list,-list)).
:-modeb(5,append(+list,+list,-list)).
:-modeh(class(+animal,#class)).
:-modeh(*,uncle(+person, -person)).%invalid
\end{lstlisting}
\end{exmp}

\subparagraph{Modet declarations}
In addition to modeb, modeh mode declarations, Toplog uses modet declarations. Every modet declaration can be transformed to a modeb declaration. For example\\
\tc{modet(l2ht(+list/[H|T], -any/H, -list/T))} has its correspondent modeb declaration\\
\tc{modeb(+list=[-any|-list])}.
For references and further details, an interested reader should consult the Toplog's source code\cite{santos2008toplogWebsite}.

\paragraph{Xhail}\label{xhail_mode_declarations}
Xhail has extended mode declarations to include metaconstraint statements in order to refine its search space.

Xhail executable's help\cite{ray2007xhail} reads:
\begin{quote}
\emph{Zero or more head declarations} are of the form
\tc{modeh(1,3,min,p("\#q","+r","-s"))} meaning between 1 and 3 ground atoms
 of the form $p(a,b,c)$ should be assumed such that $q(a), r(b), s(c)$ hold 
 and where $a, b, c$ are constant, input, output terms, respectively;
 the third flag is either min="attempt to minimize" or all="do not minimize".
 
\emph{Zero or more body declarations} are of the form \tc{modeb(1,3,pos,p("\#q","+r","-s")).} meaning this scheme can be used between 
 depths 1 and 3.  The third flag is either pos="pos. literal" or neg="neg. literal" 
\end{quote}

\begin{exmp}\ref{xhail_fine_search_space_control}
\begin{lstlisting}
modeh(0,1,all,woman("+person")).
modeb(0,1,pos,female("+person")).
\end{lstlisting}
\end{exmp}

Whereas Progol specifies the recall by its upper bound, Xhail enables its specification for both its \emph{lower bound} and upper bound. Metaconstraint statements
\tc{min}, \tc{all}, \tc{pos}, \tc{neg} constraint the hypothesis bias further.

\paragraph{Imparo}\cite{kimber2013imparo}
Imparo does not support recall specification in its mode declarations. The specification of the atom is supported.
\begin{exmp}
\begin{lstlisting}
head_modes([
    woman(+number)
]).
body_modes([
    female(+number)
]).
\end{lstlisting}
\end{exmp}

\paragraph{Tal}
Tal does not support recall specification for modeh declarations, however the recall can be specified for the modeb declarations\cite{corapi2011tal}.
\begin{exmp}
\begin{lstlisting}
modeh(happens("#event","#time","#scenario")).
modeh(woman(+person), [name(wh)]).
modeb(female(+person), [name(fb)]).
modeb(2,female(+person), [name(fb)]).
\end{lstlisting}
\end{exmp}

\paragraph{Summary}
The use of the mode declarations in an ILP system and their defining role is summarised. We say that a system of mode declarations is Prolog compatible iff any hypothesis bias definable with Prolog mode declarations can be defined within that system.

\captionof{table}{Classification by mode declaration bias} \label{tab:classification_by_mode_declaration_bias} 
 \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Prolog compatible & yes & yes & no &  yes & no & no \\ \hline
    Recall modeb support & yes & yes & yes & yes & no & yes \\ \hline
    Recall modeh support & yes & yes & no & yes & no & no \\ \hline
    Recall lower bound support & no & no & no & yes & no & no \\ \hline
  \end{tabular}

\subsubsection{Determinations}
By reading the manuals of the corresponding ILP systems, one may find that
Toplog, Xhail, Imparo, Tal do not support the determinations.
Progol supports the use of the determinations, cf. \ref{progol_multiclausal_learning}, but it does not require it, cf. \ref{progol_specialization_in_arguments}.
Aleph requires the determinations to be specified\fullref{subsec:aleph_determination_declaration_requirement}.

\subsubsection{Metaconstraints}
Metaconstraints supplement mode declarations and determinations in specifying the the language bias. Several metaconstraints are selected and their supported is evaluated accross the ILP systems classified:

\begin{itemize}
\item Maximum number of literals in a clause of a hypothesis,
\item Maximum number of clauses in a hypothesis - some incomplete ILP systems do not support multiclausal hypotheses, therefore they do not need this metaconstraint,
\item Maximum clause length in a hypothesis,
\item Maximum variable depth\ref{definition_variable_depth} of a hypothesis,
\item Maximum number of abducibles - the maximum number of ground atoms within any abductive explanation for an observation
\end{itemize}

\begin{defn}\label{definition_variable_depth}\cite{nienhuys1997foundations}
The \emph{variable depth of a variable} $x$ in an ordered definite program clause
$A \leftObjectImplies  B_1, . . . , B_n$ is defined as follows. If $x$ occurs in $A$, then its variable depth is $0$. Suppose $x$ first occurs in $B_i$.
If none of the other variables
in $B_i$ already occurred in $A \leftObjectImplies B_1,... ,B_{i-1}$,
then $x$ has variable depth $\infty$.
Otherwise, the variable depth of $x$ is $1$ plus the variable depth of the variable in $B_i$ with greatest variable depth occurring in
$A \leftObjectImplies B_1,... ,B_{i-1}$.
The \emph{variable-depth of a clause} in an ordered definite program
is the largest variable depth of its variables. Note that such a
clause is constrained iff it has variable-depth $0$.
\end{defn}

\begin{exmp}
Let $H=path(X,Y) \leftObjectImplies path(X,X_1), arc(X_1,X_2), path(X_2,Y)$.
Then the variable depths of the variables $X,Y,X_1,X_2$ in $H$ are
$0,0,1,2$ respectively.
\end{exmp}

\paragraph{Progol}
Prolog manual does not provide information about its metalevel constraints. The metalevel constraints used in the provided files\cite{muggleton1999progolWebsite} were of the following format:
\begin{lstlisting}
./pole.pl::- op(10, xfx, ...)?
./pole.pl::- op(30,xfy,:)?
./numbers.pl::- set(h,100)?
./numbers.pl::- set(r,1000)?
./numbers.pl::- set(inflate,99)?
./numbers.pl::- set(nodes,100)?
./numbers.pl::- set(i,5), set(c,5)?
\end{lstlisting}
Their change did not affect the properties examined, hence the author assumes Progol does not support any of the metaconstraints.

\paragraph{Aleph}
The author consulted the section 18 of the Aleph manual\cite{aleph2007}.
\begin{quote}
\begin{itemize}
\item set(clauselength,+V)
V is a positive integer (default 4). Sets upper bound on number of literals in an acceptable clause.
\item \emph{set(clauses,+V)}
V is a positive integer. Sets upper bound on the number of clauses in a theory when performing theory-level search (see section Theory-level search).
\item set(depth,+V)
V is a positive integer (default 10). Sets an upper bound on the proof depth to which theorem-proving proceeds.
\item Variable depth.
set(i,+V) V is a positive integer (default 2). Set upper bound on layers of new variables.
\item set(language,+V)
V is an integer >= 1 or inf (default inf). Specifies the number of occurences of a predicate symbol in any clause.
\item set(max\_abducibles,+V)
V is a positive integer (default 2). Sets an upper bound on the maximum number of ground atoms within any abductive explanation for an observation. See section Abductive learning.
\end{itemize}
\end{quote}

\paragraph{Toplog}
\begin{lstlisting}
%:-set(cross_validation_folds, 5).%dsstox.pl
%:-set(evalfn, laplace).%dsstox.pl
%:-set(evalfn, coverage).%dsstox.pl
:-set(maximum_singletons_in_hypothesis,3)%carcinogenesis.pl
:-set(maximum_literals_in_hypothesis,4).%carcinogenesis.pl
:-set(example_inflation, 10).
:-set(maximum_hypothesis_interpretations, 10).%animals.pl
:-set(maximum_proof_depth, 10).%fib.pl
:-set(maximum_proof_unifications, 500).%fib.pl
\end{lstlisting}

\paragraph{Xhail}
In \ref{xhail_mode_declarations} we found out that Xhail has the most advanced system of mode declarations. On the other hand it does not support any of the chosen metaconstraints\cite{ray2007xhail}. The author consulted the Xhail's help and example files provided with Xhail as there was no access to the source code.

\paragraph{Imparo}
\begin{lstlisting}
:-set_max_clause_length(5).
:-set_max_clauses(1).
:-set_verbose(1).
:-set_connected(1).
:-set_max_var_depth(3).
\end{lstlisting}

\paragraph{Tal}
\begin{lstlisting}
%Maximum number of body literals
option(max_body_literals, 5).

%Maximum number of rules
option(max_num_rules, 5).

%Maximum depth of the proof
%option(max_depth, 400).
option(strategy, breadth).
option(ic_check, true).
\end{lstlisting}

\paragraph{Summary}
The support for the metaconstraints of interest is summarised.

\captionof{table}{Classification by metaconstraints} \label{tab:classification_by_metaconstraints} 
 \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Max. no. of literals in a clause & & & & & & \\ \hline
    Max. no. of clauses in $H$ & & & & & & \\ \hline
    Max. clause length in $H$ & & & & & & \\ \hline
    Max. variable depth in $H$ & & & & & & \\ \hline
    Max. no of abducibles & & & & & &  \\ \hline
  \end{tabular}

\subsubsection{Top theory}
ILP systems Toplog and MC-Toplog use Top theory to constraint their hypothesis space.

\subsection{Search bias}

\subsubsection{Search bias metastatements}
Metalevel statements specify both the language bias and the search bias. ILP systems are classified based on their support of the metalevel constraints on the search.

\paragraph{Progol}
\paragraph{Aleph}
\paragraph{Toplog}
\paragraph{Xhail}
\paragraph{Imparo}
\paragraph{Tal}
\paragraph{Summary}

\section{Inverse Subsumption for Complete Explanatory Induction}\cite{yamamoto2012inverse}

\subsection{IE algorithm\cite{yamamoto2012inverse}}
Progol, Xhail, Imparo are based on the principle of the inverse entailment. By the principle of the inverse entailment $B \cup H \models E$ iff
$B \cup \neg E \models \neg H$. A hypothesis $H \in \mathcal{H}_2$ is a solution to the problem of the explanatory induction $(B,E^{+},E^{-},\mathcal{H})$ iff
$B \cup H \subseteq Cn(E^{+} \union \overline{E^{-}})$,
 $false \not\in Cn(E^{+} \cup E^{-})$, $\mathcal{H}_2 \subseteq \mathcal{H}$
iff
$B \cup H \subseteq Cn(E^{+} \union \overline{E^{-}})$,
 $false \not\in Cn(E^{+} \cup E^{-})$, $\mathcal{H}_2 \subseteq \mathcal{H}$
 
They compute the hypothesis $H$ in two steps:
1. constructing an intermediate theory, 2. generalizing its negation into the hypothesis with the inverse of the entailment relation.

The use of the inverse entailment in the computation of a hypothesis is an expensive operation as compared to the inverse subsumption. We classify the ILP systems based on whether the use of their induction technique with the inverse subsumption instead of the inverse entailment preserves the completeness for finding all the correct hypotheses.

\subsection{Imparo is complete in inverse subsumption}
We solve an open problem in \cite{yamamoto2012inverse} proving that for every correct hypothesis $H$ wrt $B, E^{+}, E^{-}$ there is a connected theory $T$ wrt $B, E^{+}, E^{-}$ subsumed by it.
\begin{defn}
Let $P=\langle B, U, I \rangle$ be an open definite program, $H$ be a correct hypothesis wrt $P$ and a ground example $E$, then $H$ is derivable by
\emph{connected theory inverse subsumption}
iff there exists a connected theory $T$ for $P$ and $E$ such that $H \subsumes T$.
We denote the statement by $P, E \vdash_{CTIS} H$.
\end{defn}

\begin{thm}\label{yamamoto2012inverseTheorem4}\cite{yamamoto2012inverse}
Let $S$ be a ground clausal theory. Then, $M^2(S) = \mu(S)$ holds.
\end{thm}

\begin{proposition}
Let $H \in I_H$ be an inductive solution for $P$ and $E$ such that $Taut(I_H)=\emptyset$, then $H$ is derivable by connected theory inverse subsumption from $P, E$.
\end{proposition}
\begin{proof}
By completeness of connected theory generalization\ref{completeness_ctg} there exists a connected theory $T$ such that $H \models T$. $M(T)$ is a bridge formula.
Hence by \ref{yam1amoto2012inverseLemma2}
$H \subsumes \tau(M(M(T) \cup Taut(I_H))$. From the definition of the connected theory $\tau(T)=T$ and $\mu (T)=T$.
Therefore $\tau(M(M(T) \cup Taut(I_H))=\tau(M(M(T))=\tau(T)=T$ using
\ref{yamamoto2012inverseTheorem4}. Therefore $H \subsumes T$ as required.
\end{proof}
\begin{remark}
The previous proposition is stated and proved only to provide an alternative proof of a statement superceded by the stronger result \ref{completeness_ctis}.
\end{remark}

\begin{thm}\label{implicationByGroundClauses}
(Implication by Ground Clauses \cite{nienhuys1997foundations}). Let $\Sigma$ be a non-empty set of clauses,
and $C$ be a ground clause. Then $\Sigma \models C$ if and only if there is a finite set $\Sigma_g$ of ground
instances of clauses from $\Sigma$, such that $\Sigma_g \models C$.
\end{thm}

\begin{thm}\label{completeness_ctg}
\emph{Completeness of connected theory generalization}(Theorem4.6 in \cite{kimber2012learning})
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory generalisation.
\end{thm}
\begin{proof}\cite{kimber2012learning}
For the full proof, an interested reader is encouraged to read \cite{kimber2012learning}.
Since $H$ is a correct hypothesis for $P$ and $E$,
then $B \cup H \models E$ by definition.
Therefore, by \ref{implicationByGroundClauses}, there is a finite set $S$ of ground instances of clauses in $B \cup H$,
such that $S \models E$. Let $T = S \cap ground(H)$.
Since $T \subseteq S$, then $T$ is ground and finite, and
since $T \subseteq ground(H)$ then $H \models T$. 
Then Kimber proves that $T$ is a connected theory for $P$ and $E$.
\end{proof}

\begin{thm}
\emph{Completeness of connected theory inverse subsumption}.
\label{completeness_ctis}
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory inverse subsumption.
\end{thm}
\begin{proof}
Construct a connected theory $T=S \cap ground(H)$ for $P$ and $E$ as in the proof of \ref{completeness_ctg}.
Then $H \subsumes ground(H) \subsumes S \cap ground(H) = T$,
hence $H \subsumes T$ by transitivity as required.
\end{proof}

\section{Completeness by problem classes}

\section{Summary}

\subsection{Completeness in generalization\cite{yamamoto2012inverse}}
\begin{center}
    \begin{tabular}{ | l | p{5cm} |}
    \hline
    ILP system &  Complete in generalization \\ \hline
    Aleph & no\\ \hline
    Progol & no\\ \hline
    Toplog & no\\ \hline
    Imparo & no\\ \hline
    Tal & no*\\ \hline
    Metagol & yes*\\ \hline
    Xhail & no\\ \hline
    Cf-induction & yes\\
    \hline
    \end{tabular}
\end{center}


\subsection{Example learning}
None of the tested ILP systems can explain the negative examples. All the systems can induce hypotheses that aim to explain the positive examples. All the tested ILP systems accept ground instances of literals for examples.
\begin{center}
    \begin{tabular}{ | l | p{5cm} |}
    \hline
    ILP system &  Clausal examples \\ \hline
    Aleph & no\\ \hline
    Progol & yes\\ \hline
    Toplog & no\\ \hline
    Imparo & no\\ \hline
    Tal & no\\ \hline
    Metagol & \\ \hline
    \hline
    \end{tabular}
\end{center}

\subsection{Hypotheses space}
All systems can learn only the Horn clauses.
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & Function symbols & Multi-clausal hypotheses & Infinite Herbrand models\\ \hline
    Aleph & yes & yes & yes\\ \hline
    Progol & yes & yes & yes*\\ \hline
    Toplog & no & no & *\\ \hline
    Imparo & yes & yes & yes\\ \hline
    Tal & yes & yes & yes\\ \hline
    Xhail & yes & yes & no\\ \hline
    \hline
    \end{tabular}
\end{center}

\subsection{Completeness}
Double Kleene star test assesses the ability of an ILP system to learn a regular language \tc{(ss)*}.
\begin{center}
    \begin{tabular}{ | p{2cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} |}
    \hline
    ILP system & Multi-clausal concepts & Generalization downwards &
    Double Kleene star & Argument Specialization &
    Non-observational concepts\\ \hline
    Aleph & no & yes & no & no & no\\ \hline
    Progol & yes & no & no & yes & no\\ \hline
    Toplog & no & no* & no* & no & no\\ \hline
    Imparo & yes & no & yes* & yes & no\\ \hline
    Tal & yes & no & no & yes & yes\\ \hline
    \hline
    \end{tabular}
\end{center}

\subsection{Argument Specialization}
Argument specialization test assesses the ILP system's ability to learn a hypothesis which require ground terms to be in a head predicate.
\begin{lstlisting}
modeh(learns(+person, +subject)).modeh(learns(#person, +subject)).
%background,male(adam).male(bob).female(alice).female(mary).
%positive examples,learns(polymath, mathematics).learns(polymath, physics).
learns(polymath, chemistry).learns(jack, mathematics).learns(susan, physics).
%negative examples,:- learns(jack, physics).:- learns(susan, chemistry).
\end{lstlisting}
has its target concept
\begin{lstlisting}
learns(polymath,A).
\end{lstlisting}
which has a constant \tc{polymath} as a first argument and a variable as a second argument.

\subsection{Non-observational concepts}
The ILP systems were tested if they could learn hypotheses that require multiple hypotheses in order to explain an example, however these hypotheses are not learnable with the cover loop algorithm. A cover loop algorithm learns hypotheses, adds these hypotheses to the background knowledge and tries to learn new hypotheses with the extended background knowledge that would explain not yet covered examples. The cover loop terminates if no new hypothesis is learnt.

An adapted sibling example from Kimber's thesis was used.
\begin{lstlisting}
modeh(brother(+person,+person)).modeh(parent(+person,+person)).
modeb(sibling(+person,+person)).modeb(male(+person)).
modeb(father(+person,+person)).
determination(brother/2, sibling/2).determination(brother/2, male/1).
determination(parent/2, father/2).
%background knowledge
male(bart).male(rod).male(todd).parent(homer,bart).
parent(homer,maggie).father(ned,rod).father(ned,todd).father(homer,lisa).
male(bart2).male(rod2).male(todd2).parent(homer2,bart2).
parent(homer2,maggie2).father(ned2,rod2).father(ned2,todd2).father(homer2,lisa2).
male(bart3).male(rod3).male(todd3).parent(homer3,bart3).
parent(homer3,maggie3).father(ned3,rod3).father(ned3,todd3).father(homer3,lisa3).
sibling(X,Y):-parent(Z,X),parent(Z,Y).
%positive examples
brother(bart,lisa).brother(rod,todd).
brother(bart2,lisa2).brother(rod2,todd2).
brother(bart3,lisa3).brother(rod3,todd3).
%negative examples, brother(lisa,bart).brother(rod,bart).
brother(maggie, maggie).parent(lisa,bart).
\end{lstlisting}
where we expect to learn the hypothesis
\begin{lstlisting}
brother(X,Y):-sibling(X,Y), male(X).parent(X,Y):-father(X,Y).
\end{lstlisting}
The second hypothesis \tc{parent(X,Y):-father(X,Y).} generalizes the existing knowledge, however it does not explain any examples. Such learning is called a non-observational learning. In the case of this sibling problem, a general hypothesis consists of an observational part - the first hypothesis and a non-observational part - the second hypothesis.
Other solutions consisting of purely observational hypotheses are longer and hence depending on our definition of generality, arguably less general.

\subsubsection{Imparo}
Imparo could not learn the expected hypothesis of the example, it learnt instead
\begin{lstlisting}
brother(A,B):-sibling(A,A),male(A)
brother(rod,todd):-true
brother(rod3,todd3):-true
brother(rod2,todd2):-true
\end{lstlisting}

\subsubsection{Aleph}
Aleph produces the hypothesis
\begin{lstlisting}
brother(A,B) :- sibling(A,A), male(A).
brother(rod,todd).brother(rod2,todd2).brother(rod3,todd3).
\end{lstlisting}

\subsubsection{Other systems}
Progol produces an inconsistent hypothesis \tc{parent(A,B).}. Toplog does not produce any hypothesis. Tal has amongst its solutions the expected hypothesis. However, depending on the depth limit we set, Tal can produce an arbitrarily large number of hypotheses. Thefore, an important work on Tal includes hypothesis selection.

\subsection{Biases}
\begin{center}
    \begin{tabular}{ | l | l | l | l | p{5cm} |}
    \hline
    ILP system & Mode declarations & Determinations & Declaration order bias & Alphabetical bias \\ \hline
    Aleph & yes & yes & yes & no\\ \hline
    Progol & yes & yes & yes & no\\ \hline
    Toplog & yes & no & yes & no\\ \hline
    Imparo & yes & no & yes & no\\ \hline
    Tal & yes & no & no & yes\\ \hline
    \hline
    \end{tabular}
\end{center}

\subsection{Violations}
All tested systems suffer from weak-head mode declaration bias.
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & Correct example bias\\ \hline
    Aleph & yes\\ \hline
    Progol & yes\\ \hline
    Toplog & no\\ \hline
    Imparo & no\\ \hline
    Tal & yes\\ \hline
    \hline
    \end{tabular}
\end{center}

\subsection{Robustness}
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & Consistency check & Always terminated\\ \hline
    Aleph & no & yes\\ \hline
    Progol & yes & yes\\ \hline
    Toplog & no & yes\\ \hline
    Imparo & no & no\\ \hline
    Tal & no & no\\ \hline
    \hline
    \end{tabular}
\end{center}

\subsection{Completeness of theoretical frameworks}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP framework & General Clause Hypothesis\\ \hline
    Bottom Generalization & no\\ \hline
    Induction on Failure & no\\ \hline
    CF-induction & yes\\ \hline
    TDHD & no\\ \hline
    MIL & \\ \hline
    \hline
    \end{tabular}

\subsection{Other}
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & \\ \hline
    Aleph & \\ \hline
    Progol & \\ \hline
    Toplog & \\ \hline
    Imparo & \\ \hline
    Tal & \\ \hline
    \hline
    \end{tabular}
\end{center}