\chapter{Experimental classification of ILP systems}\label{chap:classification_of_ilp_systems}
In this chapter we derive a unified definition of an ILP task for each ILP system and then using this definition experimentally classify ILP systems based on the problems they can solve and on their robustness as programs.

This chapter is a summary of our comparision of
ILP systems on about 100 new examples with annotations present in the appendix where the prefixes of their references in the text are denoted by letters
\ref{appendix_progol}, \ref{appendix_aleph}, \ref{appendix_toplog}, \ref{appendix_xhail}, \ref{appendix_imparo}, \ref{appendix_tal} standing for
Progol, Aleph, Toplog, Xhail, Imparo and Tal respectively.

Each ILP system uses a different syntax and writing a learning problem for each ILP system manually can be time consuming. We wrote a program \tc{generate.php} for generating input files for ILP systems Progol, Aleph, Toplog, Imparo and Tal. It is accessible with example learning files from Github at 
\url{https://github.com/dt1510/generate_ilp_input}.

\subsection{Unified definition of ILP task}
We are interested in comparing the ILP systems. Considering their specifics, each does not try to solve the same ILP task. One way to establish the possibility of the comparison of these mutually incompatible systems would be to unify the incompatibilities by generalizing and abstracting. However, this means we may lose some of the information about the specifics of each system. First we list a definition of an ILP task for each ILP system, subsequently a unifying definition is derived.

Let $\mathcal{H} \subseteq \powerset{L}$ be a bias, $Cl:\powerset{L} \to \powerset{L}$ be the classical monotonic consequence operator, $\models$ a non-monotonic consequence operator.

\begin{center}
    \begin{tabular}{ | l | p{15cm} | l | p{5cm} |}
    \hline
    ILP system & ILP task\\ \hline
    Aleph & Given $E^{+}$, $E^{-}$, $B$, $\mathcal{H}$, $B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$\\ \hline
    Progol & Given $B$, $\mathcal{H}$, find the most general $B_2 = B' \land H$ where $B' \subseteq B$, $H \in \mathcal{H}$, $B_2 \models B, B_2 \not\models false$\\ \hline
    Toplog & Given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$ \\ \hline
    Imparo & Given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$ \\ \hline
    Tal & Given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $\mathcal{H}_2 \subset \mathcal{H}. \forall H \in \mathcal{H}_2. B \land H \models E^{+}, B \land E^{-} \not\models false$ \\ \hline
    Xhail & Given $E^{+}$, $E^{-}$, $B$, $\mathcal{H}$, find $H \in \mathcal{H}$. $E^{+} \land \neg E^{-} \subseteq Cl(B \land H), false \not\in Cl(B \land H)$\\ \hline
    \end{tabular}
\end{center}

Aleph, Toplog and Imparo aim to solve the same ILP task called explanatory induction \ref{definition_explanatory_induction_with_bias}:
given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$, find $H \in \mathcal{H}. B \land H \models E^{+}, B \land E^{-} \not\models false$

The Progol ILP task is a generalization of the explanatory induction: given $E^{+}, E^{-}, B, \mathcal{H}, B \land E^{+} \land E^{-} \not\models false$,
let $B_P=E^{+} \land \neg E^{-} \land B$ be the Progol background knowledge. Then Progol finds the most general $B_2 = B' \union H_P$ where $B' \subseteq B_P, H_P \in \mathcal{H}, B_2 \models B_P, B_2 \not\models false$.
Then $(B' \union H_P) \backslash B=E'^{+} \land \neg E'^- \land H_P = H$ where $E'^+ \subseteq E^+, E'^- \subseteq E^-$. Then $H$ is the solution of an ILP task of the explanatory induction since $B \land H \models E^{+}, B \land H \not\models E^{-}, B \land H \not\models false$. We could generalize the ILP task to Progol ILP task level, however, as Progol is the only system with so significantly differing ILP task, we rather proceed in the reverse direction, we specialize the Progol ILP task to the level of the explanatory induction as explained above. Hence we will think of Progol as producing a hypothesis explaining the examples from the background knowledge. Some information about the true nature of the Progol will become invisible in the comparisons, on the other hand, more information about the other systems will remain available.

Generalize the definition of an ILP task for Aleph, Toplog, Imparo, Tal and Xhail:

\begin{defn}A learning problem of \emph{multi-explanatory induction} is
given $E^{+}, E^{-}, B, \mathcal{H} \subseteq \powerset{L}$ such that $false \not\in Cn(B \land E^{+} \land E^{-})$ for some consequence operator $Cn:\powerset{L} \to \powerset{L}$ and a language $L$,
find $\mathcal{H}_2$ called \emph{solutions} wrt $B$, $E^+$, $E^-$, $\mathcal{H}$ satisfying:\\
1)$\forall H \in \mathcal{H}_2. E^{+} \subseteq Cn(B \land H)$,\\
2)$\forall H \in \mathcal{H}_2. \neg E^{-} \not\in Cn(B \land H)$,\\
3)$\mathcal{H}_2 \subseteq \mathcal{H}$.

\end{defn}
Call this generalization of the explanatory induction, Tal ILP task, Xhail ILP task to be multi-explanatory induction as it provides several possible  explanations $\mathcal{H}_2$ for the observations. Clearly, multi-explanatory induction encompasses the specialization of the Progol task. For systems other than Tal, $\#\mathcal{H}_2=01$. For Xhail, $Cn=Cl$, for others $Cn=\models$.

We refer in the text to multi-explanatory induction simply as explanatory induction having in mind a unified definition unless a need arises to emphasise the multiplicity of possible solutions or a distinction from explanatory induction with one possible solution. Similarly, instead of discussing a set of hypotheses $\mathcal{H}_2$, we reason about its members individually referring to a hypothesis $H \in \mathcal{H}_2$.
\section{Completeness by problem classes}\label{completeness_by_problem_classes}
We classify ILP systems based on whether they can find a hypothesis for the same class of inputs $B, E^+, E^-, \mathcal{H}$ and their correct hypothesis $H$ where a class is defined by some property on $B, E^+, E^-, \mathcal{H}$, $H$. For that purpose we introduce a definition of a problem and of a class:

\begin{defn}
A problem is a tuple $\langle B, E^+, E^-, \mathcal{H}, \mathcal{H}_2 \rangle$ where
$B, E^+, E^-, \mathcal{H}$ is an input to an ILP task of explanatory induction and $\mathcal{H}_2$ is a set of all correct hypotheses wrt $B, E^+, E^-, \mathcal{H}$.
\end{defn}

\begin{defn}
A \emph{problem class} is a set of problems.
\end{defn}

Examples of problem classes as well as the classification of ILP system based on their ability to solve problems in a class are in the subsequent subsections.

\subsection{Multiplicity of clauses in a hypothesis}
A hypothesis as a clausal theory, can consist of one clause or multiple clauses. 
We recognize two classes of a learning problem based on the number of clauses in a possible hypothesis $H$.

\begin{defn}
A learning problem $\langle B, E^+, E^-, \mathcal{H}, \mathcal{H}_2 \rangle$ is a 
\emph{single clausal} problem iff $\exists H \in \mathcal{H}_2. \#H = 1$. That is $B, E^+, E^-, \mathcal{H}$ have a some correct hypothesis consisting of one clause.
A learning problem that is not single clausal is called a \emph{multiclausal} problem.
\end{defn}

The reader should consult for an example of an input to a single clausal problem \ref{progol_term_structure_learnability}, for an example of an input to a multi clausal problem \ref{progol_multiclausal_learning}.

We classify the ILP systems based on their ability to learn some (not necessarily complete) subclass of multiclausal problems.

\begin{center}
\captionof{table}{Classification by learning multiclausal hypotheses} \label{classification_multiclausal_hypotheses} 
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Multiclausal& no & yes & no & 
    yes & yes & yes \\ 
     hypotheses & \cite{muggleton2012mc}&\ref{aleph_multiclausal_learning}&
     \cite{muggleton2012mc}&\cite{muggleton2012mc}&\cite{muggleton2012mc}&
     \cite{muggleton2012mc}\\ 
    \hline
\end{tabular}
\end{center}

\subsection{Expressivity of examples}
We classify ILP systems based on whether they can learn explanations for the clausal examples or their observations are limited to atoms.

\begin{center}
\captionof{table}{Classification by learning clausal examples} \label{classification_clausal_examples} 
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Clausal& yes & no & no & 
    no & no & no \\
     examples & \ref{progol_clausal_examples} 
     &\cite{aleph2007}&\ref{toplog_clausal_examples}
     &
     &\ref{imparo_clausal_examples}&\ref{tal_clausal_examples}\\
    \hline
\end{tabular}
\end{center}

Xhail cannot learn clausal examples as all examples have to be in a body of a goal to be proved as seen in the appendix, therefore examples need to be literals.

\subsection{Generalization downwards}
Generalization downwards test assess whether an ILP system can learn a hypothesis of the form $P(x) \leftObjectImplies P(s(x))$.
\begin{center}
\captionof{table}{Classification by generalization downwards} \label{classification_generalization_downwards} 
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Generalization downwards & no & yes & no & yes & no & no \\ 
    & \ref{progol_no_generalization_downwards}
    & \ref{aleph_generalization_downwards}
    & \ref{toplog_unlearnability_of_term_structure}
    & \ref{xhail_generalization_downwards}
    & \ref{imparo_no_generalization_downwards}
    & \ref{tal_no_generalization_downwards}\\ \hline
\end{tabular}
\end{center}

The test gives an insight into an implementations of Tal which did not return the correct solution because it started looping.

\subsection{Double Kleene star}
Double Kleene star test assesses the capabilities of an ILP system to learn a regular language expressed in the term structure with the function symbols.
We classify ILP systems based on their ability to learn a hypothesis of the form
$P(s(s(X)) \leftObjectImplies P(x)$ representing a regular language \tc{(ss)*}.
\begin{center}
\captionof{table}{Classification by double Kleene star operator} \label{classification_double_kleene_star} 
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Double Kleene star& no & no & no & yes & yes & no \\
    & \ref{progol_unlearnability_of_regular_languages}
    & \ref{aleph_unlearnability_of_regular_languages}
    &  \ref{toplog_unlearnability_of_term_structure}
    & \ref{xhail_learnability_regular_languages}
    & \ref{imparo_learnability_of_nested_term_structure}
    & \ref{tal_loop_on_learning_regular_languages}\\ \hline
\end{tabular}
\end{center}

\section{Robustness}
We classify ILP systems based on whether they check for the consistency of the input theories, whether they always terminated and did not return duplicate hypotheses.

\begin{center}
\captionof{table}{Classification by robustness} \label{classification_robustness} 
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Consistency check & yes & no & no & yes & no & no \\
	& \ref{progol_consistency_check}
	& \ref{aleph_consistency_assumption}
	& \ref{toplog_consistency_assumption}
	 & \ref{xhail_implicit_consistency_check}
	 & \ref{imparo_consistency_assumption}
	 & \ref{tal_consistency_assumption} \\ \hline
    Always terminated & yes & yes & yes & yes & no & no \\ 
	&  & & & & \ref{imparo_clausal_examples} & \ref{tal_loop_on_learning_regular_languages} \\ \hline
	
	    Duplication detected & no & no & no & yes & no & yes \\ 
	&  & & 
	&\ref{xhail_redundant_hypotheses}
	& 
	&\ref{tal_solution_redundancy}\\ \hline

\end{tabular}
\end{center} 

The property ``Always terminated'' evaluates whether an ILP system terminated on all the experiments in the appendix. However, it may be possible for a system that withstood our experiments that one could find an input on which a system would not terminate. Therefore this test is only indicative.

The property ``Duplication detected'' assesses whether a duplication of the solutions
(an output with multiple equivalent hypotheses)
during the experiments for an ILP system was detected.
A case of the duplication is an ILP system returning two equivalent solutions as in
\begin{lstlisting}
1. woman_cook(X) :- female(X), cook(X).
2. woman_cook(X) :- cook(X), female(X).
\end{lstlisting}
Although this property does not strictly violate the specification of ILP systems, we would argue, that in the real applications it is desirable to return only one hypothesis up to its logical equivalence. The violation of returning a unique solution up to a logical equivalence exposes limitations of an ILP system on checking and adapting its output of solutions.

\section{Summary of classification of ILP systems}\label{classification_summary}
\begin{center}
\captionof{table}{Summary of classification}
 \begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    \hline
    \multicolumn{7}{|l|}{Mode declarations \ref{classification_mode_declarations}} \\ \hline
    Progol compatible & yes & yes & no &  yes & no & no \\ \hline
    Recall modeb support & yes & yes & yes & yes & no & yes \\ \hline
    Recall modeh support & yes & yes & no & yes & no & no \\ \hline
    Recall lower bound support & no & no & no & yes & no & no \\ \hline
    \hline
    Determinations \ref{classification_determinations} & optional & required & no & no & no & no \\ \hline
    \hline
    \multicolumn{7}{|l|}{Meta-constraints \ref{classification_meta-constraints}} \\ \hline
    Max. no. of literals in a clause & no & yes & yes & no & yes & yes\\ \hline
    Max. no. of clauses in $H$ & no & yes & no & no & yes & yes\\ \hline
    Max. variable depth in $H$ & no & yes & no & no & yes & no\\ \hline
    Number of singletons in $H$ & no & no & yes & no & no & no\\ \hline    
    \hline
    
    \multicolumn{7}{|l|}{Problem classes \ref{completeness_by_problem_classes}} \\ \hline
    Multiclausal hypotheses \ref{classification_multiclausal_hypotheses}& no & yes & no & 
    yes & yes & yes \\ \hline
    
        \hline
    Clausal examples \ref{classification_clausal_examples}& yes & no & no & 
    no & no & no \\ \hline        
  	
  	Generalization downwards \ref{classification_generalization_downwards}
    & no & yes & no & yes & no & no \\ \hline

    Double Kleene star \ref{classification_double_kleene_star} & no & no & no & yes & yes & yes \\ \hline
        
    \hline
	\multicolumn{7}{|l|}{Robustness \ref{classification_robustness}} \\ \hline
    Consistency check & yes & no & no & yes & no & no \\ \hline
    Always terminated & yes & yes & yes & yes & no & no \\ \hline  
    Duplication detected & no & no & no & yes & no & yes \\  \hline
  \end{tabular}
  \end{center}