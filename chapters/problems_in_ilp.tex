\chapter{Problems in inductive logic programming}

\section{ILP problem definition}
\subsection{Learning problem}
\subsection{Learning objective}
\subsection{Learning new information from given data}
\subsection{Generalization}
\subsection{Negative examples}
\subsection{Sematics}
If one would like to deduce the observations $E$ from the background knowledge $B$ and the hypothesis $H$, in notation $E \subseteq Cn(B \cup H)$, one has to define the consequence operator $Cn$ in order to specify the ILP problem setting.

The choice of the consequence operator specifies the interpretation of the learning problem, impacts what concepts $H$ are learnable from the background knowledge $B$ and the observations $E$ and what algorithm should be used to solve a specified inductive learning problem.

\subsubsection{Allowing several possibilities}


\subsubsection{Expressing incompleteness of information}
Solving a learning problem may require that we make a distinction in our knowledge of information: what is true, what is false and what we do not know.
Solving certain learning problems requires dealing with the knowledge of the information we do not know.

\subsection{Assumption of the probable information}

\subsubsection{Ability to withdraw conclusions}


\section{Completeness by problem classes}
\subsection{Learning concepts}
\subsection{Complexity of a learning problem}
\subsection{Learning syntactic representations}
\subsection{Learning up to logical equivalence}
\section{Hypothesis selection}
\subsection{Information content of a hypothesis}
\subsection{Inducing preferences over hypotheses space}
\subsection{Hypothesis sufficiency}
\subsection{Hypothesis approximation}
\section{Hypothesis search}
\subsection{Postulates of inductive inference}
\subsection{Inductive inference rules}
\subsection{Control}
\subsection{Automation}
\section{Research in ILP}
\section{Classification of ILP systems}