\chapter{Problems in inductive logic programming}

\section{ILP problem definition}
\subsection{Learning problem}
\subsection{Learning objective}
\subsection{Learning new information from given data}
\subsection{Generalization}
\subsection{Negative examples}
\subsection{Sematics}
If one would like to deduce the observations $E$ from the background knowledge $B$ and the hypothesis $H$, in notation $E \subseteq Cn(B \cup H)$, one has to define the consequence operator $Cn$ in order to specify the ILP problem setting.

The choice of the consequence operator specifies the interpretation of the learning problem, impacts what concepts $H$ are learnable from the background knowledge $B$ and the observations $E$ and what algorithm should be used to solve a specified inductive learning problem.

\subsubsection{Allowing several possibilities}


\subsubsection{Expressing incompleteness of information}
Solving a learning problem may require that we make a distinction in our knowledge of information: what is true, what is false and what we do not know.
Solving certain learning problems requires dealing with the knowledge of the information we do not know.

\subsection{Assumption of the probable information}

\subsubsection{Ability to withdraw conclusions}


\section{Completeness by problem classes}
The task of a normal problem setting for inductive logic programming is given background knowledge $B$, positive examples $E+$, negative examples $E-$ to find a hypothesis $H$ such that $B \cup H \models E+$,
$B \cup H \not\models E-$,
$B \cup H \not\models false$.
The logical theories $B$, $E+$, $E-$ specify a learning problem and determine the solution space $\mathcal{H}$ of the correct hypotheses. We learn about the nature of the problem by extracting the properties from the given theories. Learning problems with the identical properties are put in a class. Consequently, we can classify the ILP systems and methods based on the ability to solve a well-defined class of learning problems.

\subsection{Learning concepts}
\begin{defn}
A concept on a domain $M$ is a relation $R \subseteq M^n$ for $n \ge 0$.
\end{defn}
\begin{exmp}
Let the domain $M$ be a human family $M=\{adam, eva, kain, abel\}$ then define the concepts $male, female, father, mother, parent$ by
$male=\{adam, kain, abel\}\subseteq{M}$,
$female=\{eva\}\subseteq{M}$,
$father=\{(adam,kain), (adam,abel)\}\subseteq{M^2}$,
$mother=\{(eva,kain), (eva,abel)\}\subseteq{M^2}$,
$parent=father \cup mother$.
\end{exmp}
\begin{defn}
A concept $R\subseteq M^n$ is definable in terms of the concepts $\mathcal{C}$ iff there is a formula $\phi$ using only the concepts from $\mathcal{C}$ such that
$\forall (x_1, ..., x_n) \in M^n. R(x_1, ..., x_n) \iff \phi(x_1, ..., x_n)$.
\end{defn}
\begin{corollary}
If $P\subseteq{M^n}$, $Q\subseteq{M^n}$ are concepts, then
1. $P \setminus Q$, 2. $P \cup Q$, 3. $P \cap Q$ are concepts.
\end{corollary}
\begin{proof}
1. $\forall x_1, ..., x_n. (P \setminus Q)(x_1, ..., x_n) \iff
P(x_1, ..., x_n) \land \neg Q(x_1, ..., x_n)$.
2. $\forall x_1, ..., x_n. (P \cup Q)(x_1, ..., x_n) \iff
P(x_1, ..., x_n) \lor \neg Q(x_1, ..., x_n)$.
3. $\forall x_1, ..., x_n. (P \cap Q)(x_1, ..., x_n) \iff
P(x_1, ..., x_n) \land Q(x_1, ..., x_n)$.
\end{proof}


\subsection{Complexity of a learning problem}
\subsection{Learning syntactic representations}
\subsection{Learning up to logical equivalence}
\section{Hypothesis selection}
\subsection{Information content of a hypothesis}
\subsection{Inducing preferences over hypotheses space}
\subsection{Hypothesis sufficiency}
\subsection{Hypothesis approximation}
\section{Hypothesis search}
\subsection{Postulates of inductive inference}
\subsection{Inductive inference rules}
\subsection{Control}
\subsection{Automation}
\section{Research in ILP}
\section{Classification of ILP systems}