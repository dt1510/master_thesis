\chapter{Evaluation}
We evaluate the achievements of the thesis relative to the initial goals and then assess the contribution of its main achievements:
\begin{itemize}
\item formalisation of a language bias and theoretical correspondence between mode declarations, determinations, metaconstraints and production field,
\item experimental classification of ILP systems,
\item proof of completeness of Imparo by inverse subsumption,
\item extension of the results Inverse subsumption for complete explanatory induction and their implementation Rationale ILP system.
\end{itemize}

\section{Evaluation relative to initial goals}
The project timeline is presented which is then subsequently used to reflect on the actual achievements relative to initial goals.

Being interested in artificial intelligence, the initial goal of the author was to conduct a classification of ILP systems as mathematical structures from which an insight could be born into how to design an ILP system that would be able to solve learning problems at a level of human intelligence and greater.


The author outlines the turning points of the project timeline which relate the initial goals to what has been achieved and provide a more accurate assement of the final achievements taking into consideration the difficulties faced:

\begin{itemize}
\item classification of ILP systems as mathematical objects,
\item foundations of ILP,
\item retreat to the experimental classification of ILP systems,
\item work on the theory of ILP,
\item extended classification of ILP systems.
\end{itemize}

The author analyses the turning points and evaluates the contributions towards the initial goal.

\subsection{Classification of ILP systems as mathematical objects}
\subsection{Foundations of ILP}
\subsection{Retreat to the experimental classification of ILP systems}
\subsection{Work on the theory of ILP}
\subsection{Extended classification of ILP systems}

\section{Formalisation of a language bias}
A good description of the language bias can be found in ILP theory and methods by Muggleton and De Raedt \cite{muggleton1994inductive} who list 4 notable frameworks for a language bias specification: the inductive logic programming language of
Bergadano \cite{bergadano1993interactive}, the antecedent description grammars of Cohen \cite{cohen1994grammatically}\cite{cohen1992compiling}, the schemata of
the BLIP-MOBAL team \cite{emde1983discovery}\cite{kietz1992controlling}, their variants \cite{de1992interactive}\cite{silverstein1991relational}
\cite{tausend1994representing} and the fourth framework parametric languages as defined by \cite{muggleton1992efficient}\cite{de1992interactive}
\cite{buntine1987induction}\cite{cohen1993learnability}.
These theoretical frameworks are designed to support a cetain feature of the language bias and its specification: Bergadano's framework aims at readability, the framework by Cohen targets generality and efficiency in computing power.
But most of these specification frameworks have been used only by a small number of ILP systems.

Another language bias specification framework has been developed by Inoue \cite{inoue1992linear} who defined a production field restricting the hypothesis space to the set of relevant clauses called characteristic clauses. However, this specification is implemetation independent and capture speficics of an ILP system.

We provide a specification carrying the information about the specifics of an ILP system while being general enough to be compatible with the language bias specifications of the classified systems.
Our work takes existing implementations of Progol, Aleph, Toplog, Xhail, Imparo, Tal; extracts their specifications of the language bias and designes a system of specification of a language bias by mode declarations, determinations and metaconstraints that is compatible with the specifications of ILP systems classified. An existing formalisation of the mode declarations by Muggleton \cite{muggleton1995inverse} is used. However, a bias specification for determinations \ref{bias_determinations} and metaconstraints \ref{bias_metaconstraints} is new to the author's knowledge. Finally, the validity of a good formalisation of these notions is confirmed by establishing a correspondence \ref{bias_correspondence} with the \emph{implementation idependent} language bias specification of a production field by Inoue \cite{inoue1992linear}. This correspondence can used to reason about a language bias of ILP systems as a production field in a more theoretical way.

Our specification of a language bias is compatible with specifications of all ILP systems using the mode declarations, determinations and metaconstraints. However, there are two problems.

The first being a strong abstraction of the metaconstraints which may encompass almost any form of a language bias specification, but for this reason it does not provide us with much understanding of the underlying specifics of the language specification. Therefore the correspondent classification of ILP systems by metaconstraints is either too general concerned with a bias $\mathcal{H}$ or is too specific concerned with an individual metaconstraint such as the number of clauses in a hypothesis $H$. The future work therefore may include grouping metaconstraints by properties into classes and comparing ILP systems based on whether they support a certain class of metaconstraints.

The second problem is that our formalisation of a langauge bias is specific to only 6 ILP systems classified and extends only to ILP systems with a compatible language bias specification such as Golem \cite{muggleton1992efficient}, CF-induction \cite{yamamoto2014cfInductionWebsite}, but does not extend to other systems like Metagol \cite{muggleton2013meta}\cite{muggleton2014meta}.

\section{Experimental classification}
To the author's knowledge most the experimental evaluations of ILP systems have been performed on the following types of datasets:
\begin{itemize}
\item well-known examples from biology such as mutagenesis \cite{srinivasan1994mutagenesis}, carcinogenesis \cite{srinivasan1997carcinogenesis}, alzheimers-amine \cite{king1995relating}, DSSTox \cite{richard2002distributed} as in \cite{muggleton2008toplog}\cite{corapi2011nonmonotonic},
\item grammar learning problems as in \cite{muggleton1999progolWebsite} \cite{muggleton2012mc}, \cite{muggleton2013meta}, \cite{kimber2012learning},
\item examples on learning basic concepts such as family relations, types of animals, parity of integers as in \cite{muggleton1999progolWebsite}, \cite{santos2008toplogWebsite}, \cite{corapi2011tal}, \cite{aleph2007},
\item learning rules of in an environment: playing chess and nim \cite{muggleton1999progolWebsite}, building a stable wall as in Robot Strategy Learning \cite{muggleton2014meta}.
\end{itemize}

The advantage of these datasets is that they have been studied extensively by the ILP community and therefore are well-understood and provide a de facto standard for the comparison of new ILP systems with the existing ones.

However, the availability of these examples during the phase of a design of an ILP system makes the authors of ILP systems focus on making their systems work with these examples rather than with an arbitrary example from the real world. It is not clear to the author whether a set these well-known examples is representative of the learning problems encountered in real world applications.

Instead, the author has tried a vast number of existing and new datasets on ILP systems until datasets were found for which ILP systems produced unexpected results. The learning problems such as \ref{classification_generalization_downwards}, \ref{classification_double_kleene_star} revealed the incompleteness of algorithms the ILP systems use alghough their theoretical frameworks were complete with respect to these examples. Other examples manifested the hidden search bias of ILP systems (alphabetical term search bias \ref{tal_alphabetical_term_bias}, search bias arising from the order of mode declarations \ref{imparo_preference_over_later_mode_declarations} and determinations \ref{aleph_preference_over_earlier_determinations}), or their violations with the specification \ref{tal_no_generalization_downwards}. The author believes, some of the new examples have a potential to be a valuable contribution to the ILP community aiding in design of algorithms and practical implementations.

However, unlike a well-known Yamamoto's example \cite{yamamoto1997hypotheses} on learning a concept of an odd number from an even one, these datasets have only a limited value in a theory of ILP since they do not demonstrate the weaknesses and characteristics of the theoretical frameworks but rather of their implementations. 

\section{Proof of Imparo's completeness by inverse subsumption}
Yamamoto et al. investigate in \cite{yamamoto2012inverse} whether an inverse subsumption can be embedded in a complete inductive procedure since it is not clear whether a use of anti-subsumption instead of anti-entailment in an inductive logic programming technique preserves the completeness in ILP systems like Imparo.

The author's contribution is a proof \ref{completeness_ctis} that Imparo's algorithm can be extended to use anti-subsumption instead of anti-entailment while preserving a completeness for finding a correct hypothesis. In this regard, the author has solved an open problem.

However, the proof establishes a completeness of an extension for only one specific (although significant) theoretical framework and may not contribute to the general method of an extension sought by Yamamoto et al. In order to evaluate the result further, the author would need to investigate its extension in a more general setting.

\section{Inverse subsumption for complete explanatory induction extensions}
The author evaluates his extensions described in \ref{inverse_subsumption_extensions} of the inverse subsumption with minimal complements procedure by Yamamoto et al. \cite{yamamoto2012inverse}.

\paragraph{First-order extension}
The author has extended the method to the first-order theories and has proved its soundness. An extension to first-order theories is an important step in improvement of the computational efficiency of the procedure as it reduces the number of the tautological clauses $Taut(\mathcal{I}_H)$ arising from the ground literals of an induction field. This results in a potentially exponentially smaller complement $M(Taut(F \cup Taut(\mathcal{I}_H))$. Although the author investigated the completeness of a method, it remains an open problem.

\paragraph{Support for negative examples}
Yamamoto et al. devise their procedure for explanatory induction with positive examples only. The author extends their procedure to include negative examples, however the extension requires a construction of a theory subsumed by a hypothesis for every negative example and therefore if the number of the negative examples is large it may be inefficient. Indeed, a different method for checking the consistency with the negative examples has been used by an ILP system Rationale constructed by the author. However, the extension gives theoretical hints as to what a more optimal extension for negative examples may look like.

\paragraph{Relaxation of the algorithm while preserving its completeness}
The relaxation of the algorithm by Yamamoto et al. comes at the price of the potential inefficiency as discussed in \ref{inverse_subsumption_algorithm_comparison}. However, for extensions in a different direction, the simplicity of the relaxed while still complete algorithm may turn out to be valuable.

\paragraph{Defining inverse subsumption operators and proving their completeness}
The author proved that every formula $H$ subsuming some $T$ can be derived by the sequential application of two defined operators. The first value of the result is in an easier reasoning about the procedure of anti-subsumption in the theoretical context. The second value of the result is a practical one and the two-step procedure of inverse subsumption has been applied in an ILP system Rationale.