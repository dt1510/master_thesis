\section{Language bias}
One of the strategies ILP systems use for solving the problems of the hypothesis selection and hypothesis search is the introduction of a language bias.

\begin{defn}
Given a language $L$, the \emph{language bias} is a property (a boolean valued function) $P$ defined on all words of $L$.
\end{defn}

\begin{exmp}
Define the property $P$ by $\forall w \in L. P(w) \iff w \in L_h$. Then $P$ is a language bias of the hypotheses language $L_h$ in its superlanguage of enquiry $L$.
\end{exmp}

\begin{exmp}
The signature of $L$ is
$\mathcal{L}=\{TastesHot, IsWhite, ContainsSpice, ContainsSugar\}$,
the signature of $L_h$ is
$\mathcal{L}_h=\{TastesHot, IsWhite, ContainsSpice\}$.
Let $P(w) \iff w$ does not contain $ContainsSugar$. $P$ is the language bias of $L_h$ in $L$.
\end{exmp}

\begin{remark}
We will often think of a bias on $L$ as a subset $P$ of $L$ rather than a property on $L$ and use the notation interchangeably:
$P=\{w \in L : P(w)\} \subseteq L$.
\end{remark}

\begin{defn}
A bias $P$ of the language $L$ is \emph{sufficiently weak} with respect to the theory $T$ iff there exists an axiomatization $A$ of the theory $T$ such that $A \subseteq P$. If $P$ is not sufficiently weak with respect to the theory $T$ we say that $P$ is \emph{too strong} with respect to the theory $T$.
\end{defn}

\begin{defn}
Let $P_1$, $P_2$ be biases on the language $L$. $P_1$ is \emph{weaker} than $P_2$ and $P_2$ is \emph{stronger} than $P_1$ iff $P_2 \subseteq P_1$.
\end{defn}

\begin{exmp}
Define biases $P_k$ for $k \in \mathbb{Z}_{\ge 0}$ by $P_k(\phi)$ iff $\phi$ consists of at most $k$ distinct characters. Let $T$ be the theory of the elementary class of the partial orders. Let $A=\{\phi_1, \phi_2, \phi_3\}$ where

$\phi_1=\forall a. a \le a$,

$\phi_2=\forall a \forall b. a \le b \wedge b \le a \objectImplies a=b$,

$\phi_3=\forall a \forall b. a \le b \wedge b \le c \objectImplies a \le c$.

$A$ is an axiomatization of the theory $T$. Note $\forall \phi \in A. P_8(\phi)$ as every axiom of $A$ consists of less than $8$ distinct characters. Therefore $P_8$ is a sufficiently weak bias of the language $L$ with respect to the theory $T$. However there does not exist an axiomatization of the theory $T$ where every axiom consists of at most $3$ distinct characters, therefore $P_3$ is a too strong bias with respect to the theory $T$. The set of the biases is ordered by their strength:
$P_0 \subseteq P_1 \subseteq P_2 \subseteq P_3 \subseteq ... \subseteq P_8 \subseteq ...$. Thus $P_3$ is stronger than $P_8$.
\end{exmp}

\begin{remark}
Let $L$ be the language, $\mathcal{P}$ be the set of all the biases on $L$. Then $\mathcal{P}$ is a lattice ordered by the subset inclusion $\subseteq$ with its join operator the set union $\union$ and its meet operator the set intersection $\cap$.
\end{remark}

\subsection{Language bias benefits}
\begin{itemize}
\item Biases reduce the hypotheses space to focus on the relevant hypotheses,
\item the problem of finding the hypothesis $H$ can be solved by applying successively a stronger and stronger bias.
\end{itemize}
We want our hypothesis to have a specific syntactic form:
\begin{itemize}
\item If a hypothesis is a clause with a head $h$ we learn sufficient conditions for $h$ to be implied,
\item by specifying conditions in a body of a clause of a hypothesis we learn what concepts can be implied,
\item restricting a head $h$ of an implication $b \objectImplies h$ not to be a disjunction but only a conjuction, we avoid learning the rules implying uncertain facts.
\end{itemize}

\subsection{Language bias problems}
Consider the hypotheses space $\mathcal{H} \subseteq \powerset{L}$. An ILP problem is to find the axiomatization $A \in \mathcal{H}$ of $O$ given $B$ of the maximal score $s(A,B,O)$ where $O$ is a set of the observations, $B$ is the background knowledge.
A simple brute force algorithm may go over all $H \in \mathcal{H}$ and check if $H=A$. This poses problems:
1. $\mathcal{H}$ may not be finite,
2. determining if $H=A$ may not be decidable.
In order to disect the problem more closesly the score function will need to be given a more specific form.

\subsection{Language bias in ILP}
Different biases may be applied to each of the languages $L, L_o, L_h$. 

\subsection{Syntactic biases}

\begin{defn}
A bias $P$ on $L$ is syntactic iff there exists a computable function $f:L \to \{0,1\}$ such that $\forall x \in L.P(x) \iff f(x)=1$.
\end{defn}

\begin{remark}
A syntactic bias is computable from the language $L$ alone, no other information is available on its input.
\end{remark}

\begin{defn}
$P$ is a \emph{size bound bias} iff there exist numbers $min, max \in \mathbb{Z}$ such that $\forall \phi \in L. P(\phi) \iff min \le size(\phi) \le max$ where $size(\phi)$ is a length of a word $\phi \in L$.
\end{defn}

\begin{remark}
ILP systems with a score function involving a minimum compression length have size bound bias since they do not consider hypotheses of size greater than $\#B \union O$.
\end{remark}

\begin{remark}
Similarly, with a syntactic bias we could bound the number of the clauses in their conjunction $\phi \in L$ if necessary.
\end{remark}

\begin{defn}
A formula $\phi$ is $k$-adic iff its all predicate symbols (including an equals sign) and function symbols translated to predicate symbols are of the arity at most $k$. A formula $\phi$ is monadic iff $\phi$ is $1$-adic. A set or a theory is $k$-adic iff all its formulas are $k$-adic. The property of a formula $\phi$ being $k-adic$ is computable from $\phi$ and therefore it is a \emph{$k-adic$ bias}.
\end{defn}

\begin{defn}
A formula $\phi$ is a Horn formula iff $\phi$ is a clause with at most one positive literal. The property of a formula $\phi$ being a Horn formula is computable from $\phi$ and therefore it is a \emph{Horn bias}.
\end{defn}

\begin{defn}
A formula $\phi$ is a definite clause iff $\phi$ is a clause with exactly one positive literal. The property of a formula $\phi$ being a definite clause is computable from $\phi$ and therefore it is a \emph{definite clause bias}.
\end{defn}

\begin{remark}
A mode declaration bias is syntactic since there exists a computable function $g:L \times \mathfrak{M} \to \{0, 1\}$ (e.g. implemented in Progol\cite{muggleton1995inverse}) such that given a mode declaration $M$, then
$\forall \phi \in L. g(\phi, M)=1 \iff \phi \in L_{mode}$.
\end{remark}

\subsubsection{Syntactic biases list}
\begin{itemize}
\item Size bound bias.
\item $k$-adic bias.
\item Horn bias.
\item Definite clause bias.
\item Mode declaration bias.
\end{itemize}

\subsection{Semantic biases}

\begin{defn}
A bias $P$ is semantic iff $\forall \phi, \psi \in L. \phi \equiv \psi \implies (P(\phi) \iff P(\psi))$, i.e. $P$ is independent of a syntax of a formula $\phi \in L$.
\end{defn}

\subsubsection{Toplog top theory bias\cite{muggleton2008toplog}}
A top theory $\top$ is an extension of the background knowledge $B$. One could convert any ILP system to a TDHD(top-directed hypothesis derivation) ILP system by providing the input background knowledge $B \union \top$ instead of $B$ with the assumptions that the input satisfies the conditions of a quadruple $\langle NT, \top, B, E \rangle$ stated later.

A top theory bias is a mixture of a syntactic bias - e.g. $\top$ has to consist of Horn clauses and semantic bias which is of our interest.
\begin{defn}
A predicate symbol is \emph{non-terminal} iff TODO
\end{defn}

\begin{defn}
A predicate symbol is a \emph{target} iff TODO
\end{defn}

\subsubsection{Construction of a top theory from the mode declarations\cite{muggleton2008toplog}}
TODO - explain a construction of a top theory from an arbitrary set of mode declarations.
A top theory bias is more expressive than a mode declaration bias.
In a simplified way, given the mode declarations

$modeh(mammal(+animal)).$

$modeb(has milk(+animal)).$

$modeb(has eggs(+animal)).$

a top theory can be constructed to be $\top=\{\top_1, \top_2, \top_3, \top_4\}$:

$\top_1=mammal(X) \leftObjectImplies \$body(X).$

$\top_2=body(X) \leftObjectImplies .\%emptybody$

$\top_3=\$body(X) \leftObjectImplies has\_milk(X), \$body(X).$

$\top_4=\$body(X) \leftObjectImplies has\_eggs(X), \$body(X).$

The actual construction of a $\top$ theory has stricter control rules like: variables may only bind with others of the same type, a newly added literal must have its input variables already bound.

Both top theory bias and a mode declaration bias are specified at the input to an ILP system, for some input specifications as one above the biases are equal. Therefore one should distinguish further between a bias and a bias specification.

One could argue that a specification of a bias consists of an algorithm $A$ and an input $i$ where $A$ computes a bias $P$ from the given input $i$. As the input $i$ independent of an ILP system input $\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle$ may vary, an algorithm $A$ may compute (specify) multiple possible biases $P$. This can be seen from the ability to specify a mode declaration independent of the input
$\langle\mathcal{L}, \mathcal{L}_o, \mathcal{L}_h, O, B\rangle$.

\begin{exmp}
For the following sentence can be entailed by a top theory but there does not exist a mode declaration biased space that includes it:
TODO
\end{exmp}

\begin{itemize}
\item Bottom theory bias. An ILP system constructs a most specific hypothesis $\bot$ according to the bias criteria, then every possible hypothesis $H$ must entail $\bot$: $H \models \bot$.
\item Bottom theory subsumption bias. Every hypothesis must subsume the bottom theory.
\item Top theory subsumption bias. Every hypothesis must be subsumed by the top theory.
\item Solo-generalization bias. The hypotheses space $\mathcal{H}$ is restricted to the generalization of a single example. \cite{muggleton2012mc}
TODO: provide a formula of the restriction.
\item Solo-explanation bias. Every clause $h$ in a hypothesis $H$ has to explain at least \emph{one} example.
$\forall h \in H. \exists e \in E. \top \models h \& B, h \models e$
\item Inverse entailment bias.
$B \union H \models E \iff B \union \neg H \models \neg E$.
\item Negation on failure bias. The lack of an observation $example(x)$
causes an illusion that $\neg example(x)$ is true as $example(x)$ not provable.

\subsubsection{Heuristic biases}
\begin{defn}
A boolean property $P$ is a heuristic bias iff $P$ is computable from $\mathcal{H}, B, O$.
\end{defn}
\item Score function bias. Having two correct hypotheses $H_1$, $H_2$ such that $B \union H_1 \models E$ and $B \union H_2 \models E$ choose the hypothesis with the greater score by the (computable) function $s:\mathcal{H} \to \mathbb{R}$.
\item Search algorithm bias. A search algorithm uses heauristics to navigate via the search space $\mathcal{H}$ in addition to the score function applied after the algorithmic hypotheses generation.
\end{itemize}

\section{Biases in ILP systems}

\iffalse
\subsection{Progol}
\begin{itemize}
\item Inverse Entailment,
\item a bias provided by mode declaration,
\end{itemize}
\fi

\subsection{Toplog\cite{muggleton2008toplog}}
\begin{defn}
$B$ a Horn theory, $E$ a Horn clause. The bottom clause of $B$ and $E$ is $\bot(B,E)$=$\vee\{L | L $ a ground literal, $B \union {\neg E} \models \neg L\}$.
\end{defn}
An ILP system implementation Toplog is based on the theoretical framework Top Directed Hypothesis Derivation (TDHD) with the hypothesis space bias called top theory $\top$:
\begin{itemize}
\item Horn theory bias,
\item Bottom theory bias $\bot = \bot(B,E)$,
\item Solo-generalization bias,
\item Score function bias,
\item top theory $\top$ is specified in the input to an ILP system,
\item top theory $\top$ can be constructed from the mode declarations,
\item not every top theory bias can be expressed with the mode declarations,
\item a top theory consists of literals: terminals (in hypothesis language) and non-terminals (not allowed in hypothesis language and background knowledge)
\item a hypothesis clause in a hypothesis space consists of terminals and is derivable from the top theory by SLD-resolution and substitution, $\top \models h$
\item restriction on the predicates in the head/body of a hypothesis clause,
\item every constructed hypothesis must be subsumed by a top theory $\top$. If $H$ is a set of candidate hypotheses, then: $\forall h \in H. \exists e \in E. \top \models h \& B, h \models e$.
\item inverse entailment bias.
\end{itemize}
Toplog violates the properties in our first definition of an ILP system:
\begin{itemize}
\item a produced theory may not be complete due to too strong default bias:
\begin{itemize}
\item only what is defined by mode declarations can be in theory,
\item theory can contain only one clause,
\item only one head clause definable with mode declarations unlike two clauses
\begin{lstlisting}
:-modeh(woman(+person)).
:-modeh(man(+person)).
\end{lstlisting}

\item restrictions on the top theory related to $NT$ (non-terminals), $B$, $E$; what literals can occur where. Details in TopLog: ILP Using a Logic Program Declarative Bias by Muggleton etal.
\end{itemize}
\item a produced theory need not be consistent with the observations: given observations $human(susan)$ and $human(jack)$ a final theory can contain the hypothesis $woman(X) :- human(X)$. The accuracy of a theory is computed, a score function allows a degree of inaccuracy.
\end{itemize}

\iffalse
\subsection{ProGolem}
\begin{itemize}
\item Inverse Entailment,
\item co-generalization, 
\end{itemize}
\fi

\subsection{Imparo\cite{kimber2009induction}}
Bias:
\begin{itemize}
\item Horn theory bias (not present in a IoF framework),
\item Bottom theory bias, $\bot=$ a connected theory for $B$ and $e$,
\item Solo-generalization bias,
\item Score function bias,
\item Mode declaration bias,
\item a bias specifiable in an input program:
\begin{lstlisting}
%a restriction on the length of a clause in a theory
:-set_max_clause_length(N1).
%a restriction on the number of clauses in a theory
:-set_max_clauses(N2).
%TODO - explain further two
:-set_connected(N3).
:-set_max_var_depth(N41).
\end{lstlisting}
\end{itemize}
TODO: an algorithm for a connected theory generation.

\subsection{Metagol}
TODO

%Bias:
%Algorithm:
%Violations:

\subsection{Comparison of biases in ILP systems}
$\mathcal{H}_{Toplog} \subseteq \mathcal{H}_{Imparo}$

$\mathcal{H}_{bottom\_clause} \subseteq \mathcal{H}_{kernel\_set} \subseteq \mathcal{H}_{connected\_theory}$
