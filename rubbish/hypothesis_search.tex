\section{Hypothesis search}
\begin{itemize}
\item completeness by hypothesis finding: how to find every possible (or possibly good) hypothesis $H$ explaining $E$ from $B$,
\item efficiency: how to find a good explanation efficiently. 
\end{itemize}
\subsection{Postulates of inductive inference}
\subsection{Inductive inference rules}
\subsection{Control}
\subsection{Automation}

\section{Hypothesis search}
Hypothesis search concerns with the algorithms used for searching a hypothesis, their heuristics, search biases and other search control mechanisms.
The objective of this section is to present the search algorithms for each ILP system and then classify the ILP systems based on the representative properties of a search algorithm.

\subsection{Search direction}
Based on a direction of generalization and specializaton, hypothesis search algorithms of ILP systems fall into two categoies: bottom-up, top-down.

\subsubsection{Bottom-up}
Using the principle of the inverse entailment bottom-up search starts with a construction of the most general hypothesis, called a bottom clause, which is subsequently specialised until a required hypothesis is found.
%$B \and \neg E \models F_1 \models ... \models F_n \models \neg H$
\subsubsection{Top-down}
Top-down algorithms construct a specific top theory $\top$ which is then generalized until a required hypothesis $H$ is found.

$H=\top_n \models ... \models \top_2 \models \top_1=\top$

\subsection{Algorithms in ILP systems}

\subsubsection{Progol}

\subsubsection{Aleph}
In the Aleph manual\cite{aleph2007} a reader would find the description of the basic algorithm:
\begin{enumerate}
\item \emph{Select example.} Select an example to be generalised. If none exist, stop, otherwise proceed to the next step.
\item \emph{Build most-specific-clause.} Construct the most specific clause that entails the example selected, and is within language restrictions provided. This is usually a definite clause with many literals, and is called the "bottom clause." This step is sometimes called the "saturation" step. Details of constructing the bottom clause can be found in Stephen Muggleton's 1995 paper: Inverse Entailment and Progol\cite{muggleton1995inverse}.
\item \emph{Search.} Find a clause more general than the bottom clause. This is done by searching for some subset of the literals in the bottom clause that has the "best" score. Two points should be noted. First, confining the search to subsets of the bottom clause does not produce all the clauses more general than it, but is good enough for this thumbnail sketch. Second, the exact nature of the score of a clause is not really important here. This step is sometimes called the "reduction" step.
\item \emph{Remove redundant.} The clause with the best score is added to the current theory, and all examples made redundant are removed. This step is sometimes called the "cover removal" step. Note here that the best clause may make clauses other than the examples redundant. Again, this is ignored here. Return to Step 1.
\end{enumerate}

\subsubsection{Toplog\cite{muggleton2008toplog}}
The TopLog learning algorithm consists of three major steps: 1) hypotheses
derivation for each positive example, 2) coverage computation for all unique
hypotheses, $H$, derived in previous step, 3) construct the final theory, T , as the
subset of $H$ that maximizes a given score function (e.g. compression).


\subsubsection{Xhail}

\subsubsection{Imparo}

\subsubsection{Tal}

\subsection{Summary}
\captionof{table}{Classification by properties of a search algorithm} \label{tab:title} 
\begin{tabular}{| l | l | l | l | l | l | l |}
    \hline
    ILP system & Progol & Aleph & Toplog & Xhail & Imparo & Tal \\ \hline
    Search direction & bottom-up& bottom-up&top-down& & &top-down\\ \hline
\end{tabular}

