
\section{The role of a language in learning the models}
Consider an empty language. It has one possible interpretation and one model up to an isomorphism.
By extending the language we can distinguish the models further. The problem of predicate invention is to invent a predicate so that the right distinction between models is made based on the language.
Questions:
1. How can we measure an expessivity and a complexity of a language?
2. How much information can be captured by a $\mathcal{L}$-theory given a language $\mathcal{L}$?
3. What is the maximal complexity of the language $\mathcal{L}$ for which the $\mathcal{L}$-theory is decidable?
4. Complexity of learning the equivalence class of the reality given a language.
5. Is there a language for which the task of learning the equivalence class of the reality is undecidable?
6. What percentage of the reality can we recover from observing and reasoning only with the statements in the restricted language?
7. Can a language be partitioned (or expressed as posets of sublanguages) into sublanguages and the theories of the sublanguages be learnt, then joint into a theory of the original language?

\chapter{Rationale ILP system}
After having reviewed ILP systems, we aim to design a custom ILP system called Rationale that would address some of the problems we have encountered in the ILP systems.

\section{Suggested solutions}
Our suggested solutions are an extension of the work\cite{yamamoto2012inverse} by Yamamoto, et al. in which they provide a complete theoretical framework for solving the problem of an explanatory induction.
   
\subsection{Inverse Subsumption for Complete Explanatory Induction}
\begin{lemma}\label{yamamoto2012inverseLemma2}\cite{yamamoto2012inverse}
Let $B$, $E$ and $I_H$ be a background theory, examples and an induction field,
respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$, $H$ satisfies the following condition:

$H \subsumes \tau(M(F \cup Taut(I_H))$.
\end{lemma}

\subsection{Extension to first-order theories}
A generalization of the antisubsumption results to the first-order clausal theories enables a more efficient computation of the minimal complement, especially if the number of potential tautologies is large and a computation of anti-subsumption since there is less need to search for a subsumed hypothesis as the hypothesis is already not ground.

\begin{proposition}(Soundness of the first-order extension)
Let $B$, $E$ and $I_H$ be a first-order background theory, examples and an induction field, respectively. Let $F$ be a bridge theory wrt $B$ and $E$. Suppose that $H \subsumes \tau(M(F \cup Taut(I_H)))$, then $B \cup H \models E$.
\end{proposition}
\begin{proof}
By subsumption $H \models \tau(M(F \cup Taut(I_H))) \equiv \neg F$. By the principle of the inverse entailment $B \cup \neg F \models E$, hence
$B \cup H \models E$.
\end{proof}

However, it is not clear whether the completeness is preserved in the first order case.

$\tau'(S)$ denotes the clausal theory obtained by removing every clause from $S$ that has a ground instance that factors to a tautology. $I_H$ is an induction field of not necessarily ground literals.

\begin{conjecture}\label{subsumptionConjectureFirstOrder}
Let $B$, $E$ and $I_H$ be a background theory, examples and an induction field, respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$, $H$ satisfies the following condition:
$H \subsumes \tau'(M(F \cup Taut(I_H)))$.
\end{conjecture}

\begin{exmp}
Let $E=\{r(y)\}$,
$B=\{\neg p(x) \vee r(x)\}$,
$H=\{s(x), \neg s(x) \vee p(x)\}$,
$I_H=\{s(x), \neg s(x), p(x)\}$.
Then $F=B \cup \neg E=\{\neg p(x) \vee r(x), \neg r(y) \}$.
$Taut(I_H)=\{s(x) \vee \neg s(x)\}$.
$M(F \cup Taut(I_H))=$
$\{p(x) \vee r(y) \vee \neg s(z), p(x) \vee r(y) \vee s(z),$
$\neg r(x) \vee r(y) \vee \neg s(z),\neg r(x) \vee r(y) \vee s(z) \}$.

$\tau'(M(F \cup Taut(I_H))=\{p(x) \vee r(y) \vee \neg s(z), p(x) \vee r(y) \vee s(z)\}$ which is subsumed by the hypothesis $H$.
\end{exmp}

\begin{exmp}
All theories need not be grounded. Adapting the previous example, if the examples were
$E_2=\{r(a), r(b)\}$ with the same hypothesis
$H=\{s(x), \neg s(x) \vee p(x)\}$, then
$\tau'(M(F_2 \cup Taut(I_H))=\{p(x) \vee r(a) \vee \neg s(z), p(x) \vee r(a) \vee s(z), p(x) \vee r(b) \vee \neg s(z), p(x) \vee r(b) \vee s(z) \}$ which is subsumed by $H$. A careful reader notices that the adapted $\tau'(M(F_2 \cup Taut(I_H))$ is just a partial instantiation of the previous
$\tau'(M(F \cup Taut(I_H))$ with the same substitution
$theta=\{a / y, b / y\}$ as used for the examples:
$\tau'(M(F_2 \cup Taut(I_H)) \theta=\tau'(M(F_2 \cup Taut(I_H))$,
$E_2 \theta = E$.
\end{exmp}

\subsection{Extension to cover negative examples}
We extend the completeness results and algorithm in a more general problem of explanatory induction that includes the negative examples.
\begin{defn}General ILP Problem setting: given the clausal theories $B$, $H$, $E^+$, $E^-$ find a hypothesis $H$ such that $B \wedge H \models E+$, $B \wedge H$ consistent, $B \wedge H \not\models E-$.
\end{defn}

\begin{lemma}\label{yamamoto2012inverseLemma2Converse}
Let $B \cup \neg E \models F$. $H \subsumes \tau(M(F \cup Taut(I_H)))$ implies
$B \cup H \models E$.
\end{lemma}
\begin{proof}
By the principle of the inverse entailment and by $\tau(M(F \cup Taut(I_H))) \equiv \neg F$.
\end{proof}

\begin{lemma}
Let $E-=\{e_1,...,e_n\}$, $F_i-=B \cup \{\neg e_i\}$ and $H$ be
a hypothesis wrt $I_H, B, E+, E-$.
Then $B \cup H \not\models E-$ iff
$\forall i \in \{1,...,n\}.$ $H \not\subsumes \tau(M(F_i- \cup Taut(I_H)))$.
\end{lemma}

\begin{proof}
Use \ref{yamamoto2012inverseLemma2}, \ref{yamamoto2012inverseLemma2Converse} and the fact that $E-$ is in a disjunctive normal form.
\end{proof}

\begin{conjecture}
$\tau(M(F_i- \cup Taut(I_H))) \subsumes \tau(M(F- \cup Taut(I_H)))$
where $F-=F_1- \wedge ... \wedge F_n-$.
\end{conjecture}

By the principle of inverse entailment
$B \wedge H \not\models E-$ iff
$B \wedge \neg E- \not\models \neg H$.

\begin{defn}
(Negative bridge theory) Let $B$ and $E-$ be a background theory and negative examples, respectively. Let $F-$ be a ground clausal theory. Then $F-$ is a negative bridge theory wrt $B$ and $E$ iff $B \wedge \neg E- \models F-$ holds.
\end{defn}

\begin{conjecture}\label{subsumptionConjectureNegativeExamples}
(Note: probably false).
Let $B$, $E-$ and $I_H$ be a background theory, negative examples and an induction field, respectively. Let $F-$ be a negative bridge theory wrt $B$ and $E-$. For every hypothesis $H$ wrt $I_H$ and $F-$,

$B \wedge H \not\models E-$ iff $H$ does not subsume
$\tau(M(F- \cup Taut(I_H)))$.
\end{conjecture}

\begin{defn}
Let $B$, $E+$, $E-$ and $I_H = \langle L \rangle$ be a background theory, positive examples, negative examples and an induction field, respectively.
Let $F$ be a bridge theory wrt $B$ and $E+$. Let $F-$ be a maximal negative bridge theory wrt $B$ and $E-$. A clausal theory $H$ is derived
by extended inverse subsumption with minimal complements from $F$ and $F-$ wrt $I_H$ if $H$ is constructed as
follows:
\begin{itemize}
\item Step 1. Compute $Taut(I_H)$;
\item Step 2. Compute $\tau(M(F \cup Taut(I_H)))$;
\item Step 3. Compute $\tau(M(F- \cup Taut(I_H)))$;
\item Step 4. Construct a clausal theory $H$ satisfying the conditions:
$H \subsumes \tau(M(F \cup Taut(I_H)))$,
$H \not\subsumes \tau(M(F- \cup Taut(I_H)))$.
\end{itemize}
\end{defn}

\subsection{Imparo extension}
\begin{defn}
Let $P=\langle B, U, I \rangle$ be an open definite program, $H$ be a correct hypothesis wrt $P$ and a ground example $E$, then $H$ is derivable by
\emph{connected theory inverse subsumption}
iff there exists a connected theory $T$ for $P$ and $E$ such that $H \subsumes T$.
We denote the statement by $P, E \vdash_{CTIS} H$.
\end{defn}

\begin{thm}\label{yamamoto2012inverseTheorem4}\cite{yamamoto2012inverse}
Let $S$ be a ground clausal theory. Then, $M^2(S) = \mu(S)$ holds.
\end{thm}

\begin{proposition}
Let $H \in I_H$ be an inductive solution for $P$ and $E$ such that $Taut(I_H)=\emptyset$, then $H$ is derivable by connected theory inverse subsumption from $P, E$.
\end{proposition}
\begin{proof}
By completeness of connected theory generalization\ref{completeness_ctg} there exists a connected theory $T$ such that $H \models T$. $M(T)$ is a bridge formula.
Hence by \ref{yamamoto2012inverseLemma2}
$H \subsumes \tau(M(M(T) \cup Taut(I_H))$. From the definition of the connected theory $\tau(T)=T$ and $\mu (T)=T$.
Therefore $\tau(M(M(T) \cup Taut(I_H))=\tau(M(M(T))=\tau(T)=T$ using
\ref{yamamoto2012inverseTheorem4}. Therefore $H \subsumes T$ as required.
\end{proof}
\begin{remark}
The previous proposition is stated and proved only to provide an alternative proof of a statement superceded by the stronger result \ref{completeness_ctis}.
\end{remark}

\begin{thm}\label{implicationByGroundClauses}
(Implication by Ground Clauses \cite{nienhuys1997foundations}). Let $\Sigma$ be a non-empty set of clauses,
and $C$ be a ground clause. Then $\Sigma \models C$ if and only if there is a finite set $\Sigma_g$ of ground
instances of clauses from $\Sigma$, such that $\Sigma_g \models C$.
\end{thm}

\begin{thm}\label{completeness_ctg}
\emph{Completeness of connected theory generalization}(Theorem4.6 in \cite{kimber2012learning})
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory generalisation.
\end{thm}
\begin{proof}\cite{kimber2012learning}
For the full proof, an interested reader is encouraged to read \cite{kimber2012learning}.
Since $H$ is a correct hypothesis for $P$ and $E$,
then $B \cup H \models E$ by definition.
Therefore, by \ref{implicationByGroundClauses}, there is a finite set $S$ of ground instances of clauses in $B \cup H$,
such that $S \models E$. Let $T = S \cap ground(H)$.
Since $T \subseteq S$, then $T$ is ground and finite, and
since $T \subseteq ground(H)$ then $H \models T$. 
Then Kimber proves that $T$ is a connected theory for $P$ and $E$.
\end{proof}

\begin{thm}
\emph{Completeness of connected theory inverse subsumption}.
\label{completeness_ctis}
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory inverse subsumption.
\end{thm}
\begin{proof}
Construct a connected theory $T=S \cap ground(H)$ for $P$ and $E$ as in the proof of \ref{completeness_ctg}.
Then $H \subsumes ground(H) \subsumes S \cap ground(H) = T$,
hence $H \subsumes T$ by transitivity as required.
\end{proof}

\subsection{Faster consistency check\cite{yamamoto2012comparison}}
Checking of the consistency of $B \cup E$ in upward generalization can be done by finding a refutation for example, but in general this verification is expensive. However, using \ref{yamamoto2012inverseLemma2} one can reduce the consistency check problem into subsumption problem by verifying $\tau(M(B \cup \neg E \cup Taut(I_H))$ is subsumed by the hypothesis $H$. Computing $\tau(M(B \cup \neg E \cup Taut(I_H))$ is deterministic and with the application of \ref{subsumptionConjectureFirstOrder} tractable and potentially efficient. Checking the subsumption condition is trivial and efficient.

\subsection{Hypothesis selection}
Given clausal theories $E$, $B$, there may be several non-equivalent clausal theories $H$ such that $B \cup H \models E$ and $B \cup H \not\models false$. Denote the set of such (correct wrt $E$, $B$) hypotheses $\mathcal{H}$. The problem of hypothesis selection is to induce only one $H \in \mathcal{H}$.

We seek to resolve the hypothesis selection problem by providing a justification for the selected hypotheses. The problem may be reduced to the use of the argumentation frameworks such as Abstract Argumentation (AA) or Assumption Based Argumentation (ABA).
\subsection{Hypotheses enumeration}
Enumerating all the correct hypotheses can be done efficiently using the antisubsumption and the result \ref{yamamoto2012inverseLemma2}. First $\tau(M(B \cup \neg E \cup Taut(I_H))$ is computed, then antisubsumed theories $H$ can be enumerated trivially and efficiently.

\section{Unresolved problems}
\begin{itemize}
\item The background theory has to be Horn.
\end{itemize}

\chapter{Comparisons of ILP systems}
* denotes to be tested/proved.
\section{Definition of an ILP problem}
We are interested in comparing the ILP systems, considering the specifics of ILP systems, each does not try to solve the same ILP problem. We search a way to compare these mutually incompatible systems. One way to establish the possibility of the comparison would be to unify these incopabilities by generalizing and abstracting. However, this means we may lose some of the information about the specifics of each system. First we provide the problem settings for each ILP system.

Let $E$ denote examples, $E+$ positive examples, $E-$ negative examples, $B$ the background knowledge, $\mathcal{H} \subseteq \powerset{L}$ the language bias. Let $Cl:\powerset{L} \to \powerset{L}$ be the classical monotonic consequence operator, $\models$ a non-monotonic consequence operator. Here by the overline on negative examples $\overline{E-}$ we denote the set of the negated sentences of $E-$. When specifying the consistency condition for the negative examples by $B \cup H \not\models E-$ we mean that no example $e- \in E-$ should be implied by $B \cup H$, therefore $E-$ is meant to be a disjunction (rather than a conjunction like $E+$) of examples. Therefore, $\overline{E-} \equiv \neg E-$.

\begin{center}
    \begin{tabular}{ | l | p{15cm} | l | p{5cm} |}
    \hline
    ILP system & \\ \hline
    Aleph & Given $E+$, $E-$, $B$, $\mathcal{H}$, $B \cup E+ \cup E- \not\models false$, find $H \in \mathcal{H}. B \cup H \models E+, B \cup E- \not\models false$\\ \hline
    Progol & Given $B$, $\mathcal{H}$, find the most general $B_2 = B' \cup H$ where $B' \subseteq B$, $H \in \mathcal{H}$, $B_2 \models B, B_2 \not\models false$\\ \hline
    Toplog & Given $E+, E-, B, \mathcal{H}, B \cup E+ \cup E- \not\models false$, find $H \in \mathcal{H}. B \cup H \models E+, B \cup E- \not\models false$ \\ \hline
    Imparo & Given $E+, E-, B, \mathcal{H}, B \cup E+ \cup E- \not\models false$, find $H \in \mathcal{H}. B \cup H \models E+, B \cup E- \not\models false$ \\ \hline
    Tal & Given $E+, E-, B, \mathcal{H}, B \cup E+ \cup E- \not\models false$, find $\mathcal{H}_2 \subset \mathcal{H}. \forall H \in \mathcal{H}_2. B \cup H \models E+, B \cup E- \not\models false$ \\ \hline
    Xhail & Given $E+$, $E-$, $B$, $\mathcal{H}$, find $H \in \mathcal{H}$. $E+ \cup \overline{E-} \subseteq Cl(B \cup H), false \not\in Cl(B \cup H)$\\ \hline
    \hline
    \end{tabular}
\end{center}

Aleph, Toplog and Imparo aim to solve the same ILP problem called \emph{explanatory induction}:
given $E+, E-, B, \mathcal{H}, B \cup E+ \cup E- \not\models false$, find $H \in \mathcal{H}. B \cup H \models E+, B \cup E- \not\models false$

The Progol ILP problem is a generalization of the explanatory induction: given $E+, E-, B, \mathcal{H}, B \cup E+ \cup E- \not\models false$,
let $B_P=E+ \cup \overline{E-} \cup B$ be the Progol background knowledge. Then Progol finds the most general $B_2 = B' \union H_P$ where $B' \subseteq B_P, H_P \in \mathcal{H}, B_2 \models B_P, B_2 \not\models false$.
Then $(B' \union H_P) \backslash B=E+' \cup \overline{E-'} \cup H_P = H$ where $E+' \subseteq E+, E-' \subseteq E-$. Then $H$ is the solution of the problem of the explanatory induction since $B \cup H \models E+, B \cup H \not\models E-, B \cup H \not\models false$. We could generalize the problem to Progol ILP problem level, however, as Progol is the only system with so significantly differing problem system, we rather proceed in the reverse direction, we specialize the Progol ILP problem to the level of the explanatory induction as explained above. Hence we will think of Progol as producing a hypothesis explaning the examples from the background knowledge. Some information about the true nature of the Progol will become invisible in the comparisons, on the other hand, more information about the other systems will remain available.

Let $Cn:\powerset{L} \to \powerset{L}$ be a consequence operator, then generalize the definition of an ILP problem for Aleph, Toplog, Imparo, Tal and Xhail:
given $E+, E-, B, \mathcal{H}, false \not\in Cn(B \cup E+ \cup E-)$,
find $\mathcal{H}_2 \subset \mathcal{H}. \forall H \in \mathcal{H}_2. E+ \cup \overline{E-} \subseteq Cn(B \cup H), false \not\in Cn(B \cup H)$. Call this generalization of the explanatory induction, Tal ILP problem, Xhail ILP problem to be \emph{multi-explanatory induction} as it provides possible several epxlanations $\mathcal{H}_2$ for the observations. Clearly, multi-explanatory induction encompasses the specialization of the Progol problem. For systems other than Tal, $\#\mathcal{H}_2=1$. For Xhail, $Cn=Cl$, for others $Cn=\models$.

We define an ILP system:
\begin{defn}
An ILP system is a partial function $s:(E+,E-,B, \mathcal{H}) \mapsto \mathcal{H}_2$.
\end{defn}
Having defined an ILP problem, we define the correctness of an ILP system:
\begin{defn}
An ILP system $s:(E+,E-,B, \mathcal{H}) \mapsto \mathcal{H}_2$ gives a solution to the problem of the multi-explanatory induction on an input $(E+,E-,B, \mathcal{H})$ iff if $false \not\in Cn(B \cup E+ \cup E-)$,
then $\forall H \in \mathcal{H}_2. E+ \cup \overline{E-} \subseteq Cn(B \cup H), false \not\in Cn(B \cup H)$. We also say that $s$ is correct on an input $(E+,E-,B, \mathcal{H})$ wrt to the problem of the multi-explanatory induction.
\end{defn}

\section{Properties of an ILP system}
We further seek properties of ILP systems upon which we could base our comparison of different ILP systems. Recall $L$ is a set of all logical sentences for a given signature $\mathcal{L}$ and $B, E+, E-, H \subseteq L$, $\mathcal{H}, \mathcal{H}_2 \subseteq \powerset{L}$.

We ask:
\begin{enumerate}
\item On what inputs is $s$ defined?
\item What is the algorithm computing $s$?
\item What is the computational complexity of $s$?
\item On what inputs do different systems $s_1, s_2$ produce a different output?
\item If on a given input $i$ and $s_1(i) \not= s_2(i)$, is there a way to compare the systems based on their output solution set?
\item Is there a way to equip the input and the output with the semantics to provide an interpretation for the output produced from the input?
\end{enumerate}



\section{Algorithmic definition of an ILP system}
We provide a simplified algorithm for each of the systems in order to be able to extract properties of each system upon which a comparison could based.

\subsection{IE algorithm\cite{yamamoto2012inverse}}
Progol, Xhail, Imparo are based on the principle of the inverse entailment. By the principle of the inverse entailment $B \cup H \models E$ iff
$B \cup \neg E \models \neg H$. A hypothesis $H \in \mathcal{H}_2$ is a solution to the problem of the explanatory induction $(B,E+,E-,\mathcal{H})$ iff
$B \cup H \subseteq Cn(E+ \union \overline{E-})$,
 $false \not\in Cn(E+ \cup E-)$, $\mathcal{H}_2 \subseteq \mathcal{H}$
iff
$B \cup H \subseteq Cn(E+ \union \overline{E-})$,
 $false \not\in Cn(E+ \cup E-)$, $\mathcal{H}_2 \subseteq \mathcal{H}$
 
 They compute the hypothesis $H$ in two steps:
1. constructing an intermediate theory, 2. generalizing its negation into the hypothesis with the inverse of the entailment relation.


\subsection{Aleph}
\subsection{Progol}
\subsection{Toplog}
\subsubsection{Top theory construction}
\subsection{Imparo}
\subsection{Tal}
\subsection{Xhail}

\section{Completeness in generalization\cite{yamamoto2012inverse}}
\begin{center}
    \begin{tabular}{ | l | p{5cm} |}
    \hline
    ILP system &  Complete in generalization \\ \hline
    Aleph & no\\ \hline
    Progol & no\\ \hline
    Toplog & no\\ \hline
    Imparo & no\\ \hline
    Tal & no*\\ \hline
    Metagol & yes*\\ \hline
    Xhail & no\\ \hline
    Cf-induction & yes\\
    \hline
    \end{tabular}
\end{center}


\section{Example learning}
None of the tested ILP systems can explain the negative examples. All the systems can induce hypotheses that aim to explain the positive examples. All the tested ILP systems accept ground instances of literals for examples.
\begin{center}
    \begin{tabular}{ | l | p{5cm} |}
    \hline
    ILP system &  Clausal examples \\ \hline
    Aleph & no\\ \hline
    Progol & yes\\ \hline
    Toplog & no\\ \hline
    Imparo & no\\ \hline
    Tal & no\\ \hline
    Metagol & \\ \hline
    \hline
    \end{tabular}
\end{center}

\section{Hypotheses space}
All systems can learn only the Horn clauses.
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & Function symbols & Multi-clausal hypotheses & Infinite Herbrand models\\ \hline
    Aleph & yes & yes & yes\\ \hline
    Progol & yes & yes & yes*\\ \hline
    Toplog & no & no & *\\ \hline
    Imparo & yes & yes & yes\\ \hline
    Tal & yes & yes & yes\\ \hline
    Xhail & yes & yes & no\\ \hline
    \hline
    \end{tabular}
\end{center}

\section{Completeness}
Double Kleene star test assesses the ability of an ILP system to learn a regular language \tc{(ss)*}.
\begin{center}
    \begin{tabular}{ | p{2cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} |}
    \hline
    ILP system & Multi-clausal concepts & Generalization downwards &
    Double Kleene star & Argument Specialization &
    Non-observational concepts\\ \hline
    Aleph & no & yes & no & no & no\\ \hline
    Progol & yes & no & no & yes & no\\ \hline
    Toplog & no & no* & no* & no & no\\ \hline
    Imparo & yes & no & yes* & yes & no\\ \hline
    Tal & yes & no & no & yes & yes\\ \hline
    \hline
    \end{tabular}
\end{center}

\subsection{Argument Specialization}
Argument specialization test assesses the ILP system's ability to learn a hypothesis which require ground terms to be in a head predicate.
\begin{lstlisting}
modeh(learns(+person, +subject)).modeh(learns(#person, +subject)).
%background,male(adam).male(bob).female(alice).female(mary).
%positive examples,learns(polymath, mathematics).learns(polymath, physics).
learns(polymath, chemistry).learns(jack, mathematics).learns(susan, physics).
%negative examples,:- learns(jack, physics).:- learns(susan, chemistry).
\end{lstlisting}
has its target concept
\begin{lstlisting}
learns(polymath,A).
\end{lstlisting}
which has a constant \tc{polymath} as a first argument and a variable as a second argument.

\subsection{Non-observational concepts}
The ILP systems were tested if they could learn hypotheses that require multiple hypotheses in order to explain an example, however these hypotheses are not learnable with the cover loop algorithm. A cover loop algorithm learns hypotheses, adds these hypotheses to the background knowledge and tries to learn new hypotheses with the extended background knowledge that would explain not yet covered examples. The cover loop terminates if no new hypothesis is learnt.

An adapted sibling example from Kimber's thesis was used.
\begin{lstlisting}
modeh(brother(+person,+person)).modeh(parent(+person,+person)).
modeb(sibling(+person,+person)).modeb(male(+person)).
modeb(father(+person,+person)).
determination(brother/2, sibling/2).determination(brother/2, male/1).
determination(parent/2, father/2).
%background knowledge
male(bart).male(rod).male(todd).parent(homer,bart).
parent(homer,maggie).father(ned,rod).father(ned,todd).father(homer,lisa).
male(bart2).male(rod2).male(todd2).parent(homer2,bart2).
parent(homer2,maggie2).father(ned2,rod2).father(ned2,todd2).father(homer2,lisa2).
male(bart3).male(rod3).male(todd3).parent(homer3,bart3).
parent(homer3,maggie3).father(ned3,rod3).father(ned3,todd3).father(homer3,lisa3).
sibling(X,Y):-parent(Z,X),parent(Z,Y).
%positive examples
brother(bart,lisa).brother(rod,todd).
brother(bart2,lisa2).brother(rod2,todd2).
brother(bart3,lisa3).brother(rod3,todd3).
%negative examples, brother(lisa,bart).brother(rod,bart).
brother(maggie, maggie).parent(lisa,bart).
\end{lstlisting}
where we expect to learn the hypothesis
\begin{lstlisting}
brother(X,Y):-sibling(X,Y), male(X).parent(X,Y):-father(X,Y).
\end{lstlisting}
The second hypothesis \tc{parent(X,Y):-father(X,Y).} generalizes the existing knowledge, however it does not explain any examples. Such learning is called a non-observational learning. In the case of this sibling problem, a general hypothesis consists of an observational part - the first hypothesis and a non-observational part - the second hypothesis.
Other solutions consisting of purely observational hypotheses are longer and hence depending on our definition of generality, arguably less general.

\subsubsection{Imparo}
Imparo could not learn the expected hypothesis of the example, it learnt instead
\begin{lstlisting}
brother(A,B):-sibling(A,A),male(A)
brother(rod,todd):-true
brother(rod3,todd3):-true
brother(rod2,todd2):-true
\end{lstlisting}

\subsubsection{Aleph}
Aleph produces the hypothesis
\begin{lstlisting}
brother(A,B) :- sibling(A,A), male(A).
brother(rod,todd).brother(rod2,todd2).brother(rod3,todd3).
\end{lstlisting}

\subsubsection{Other systems}
Progol produces an inconsistent hypothesis \tc{parent(A,B).}. Toplog does not produce any hypothesis. Tal has amongst its solutions the expected hypothesis. However, depending on the depth limit we set, Tal can produce an arbitrarily large number of hypotheses. Thefore, an important work on Tal includes hypothesis selection.

\section{Biases}
\begin{center}
    \begin{tabular}{ | l | l | l | l | p{5cm} |}
    \hline
    ILP system & Mode declarations & Determinations & Declaration order bias & Alphabetical bias \\ \hline
    Aleph & yes & yes & yes & no\\ \hline
    Progol & yes & yes & yes & no\\ \hline
    Toplog & yes & no & yes & no\\ \hline
    Imparo & yes & no & yes & no\\ \hline
    Tal & yes & no & no & yes\\ \hline
    \hline
    \end{tabular}
\end{center}

\section{Violations}
All tested systems suffer from weak-head mode declaration bias.
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & Correct example bias\\ \hline
    Aleph & yes\\ \hline
    Progol & yes\\ \hline
    Toplog & no\\ \hline
    Imparo & no\\ \hline
    Tal & yes\\ \hline
    \hline
    \end{tabular}
\end{center}

\section{Robustness}
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & Consistency check & Always terminated\\ \hline
    Aleph & no & yes\\ \hline
    Progol & yes & yes\\ \hline
    Toplog & no & yes\\ \hline
    Imparo & no & no\\ \hline
    Tal & no & no\\ \hline
    \hline
    \end{tabular}
\end{center}

\section{Completeness of theoretical frameworks}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP framework & General Clause Hypothesis\\ \hline
    Bottom Generalization & no\\ \hline
    Induction on Failure & no\\ \hline
    CF-induction & yes\\ \hline
    TDHD & no\\ \hline
    MIL & \\ \hline
    \hline
    \end{tabular}

\section{Other}
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    ILP system & \\ \hline
    Aleph & \\ \hline
    Progol & \\ \hline
    Toplog & \\ \hline
    Imparo & \\ \hline
    Tal & \\ \hline
    \hline
    \end{tabular}
\end{center}
    
\chapter{Definition of an ILP problem - notes}
For a given set of examples often multiple sets of hypotheses can explain such a set of examples. However, the common definition of an ILP problem does not convey which hypotheses should be chosen in such a case. This chapter presents an investigation of an definition of an ILP problem and critique of the incompleteness of the definition of an ILP problem.

Related to the definition of an ILP problem Cheng and Wolf say in \cite{nienhuys1997foundations}
"If E + is finite, then E = E -e will be a correct theory, but a rather unin-
teresting one. In this case, we would not have learned anything beyond the
given examples: the induced theory has no predictive power. To avoid this,
we can put some constraints on the theory. For instance, we might demand
that E contains less clauses than the number of given positive examples. In
that case, E = E + is ruled out. Since constraints like these mainly depend
on the particular application at hand, we will not devote much attention to
them."
They acknowledge that their definition of an ILP problem does not specify what explanation $H$ should explain the examples. However, to the author of this thesis it is unclear how one would complete the definition with respect to the particular application.

Muggleton in his Inverse Entailment and Progol \cite{muggleton1995inverse} says:
"the association of probability values with hypotheses requires the assumption of a prior probability distribution over the hypothesis language. Occam's razor can be taken as an instance of a distribution which assigns higher prior probability to simpler hypotheses. It has been shown [4] that without such distributional assumptions the class of all logic programs is not even PAC-predictable. On the other hand, it has recently been demonstrated [42] that the class of all time-bounded logic programs is polynomial-time learnable (U-learnable) under fairly broad families of prior probability distributions. Appendix B gives more details of the relationship between data, posterior probabilities and U-learnability."

\section{ILP theory and methods}
\subsection{Completeness}
From ILP theory and methods \cite{muggleton1994inductive}:
An ILP system with a bias $\mathcal{H}$ is complete iff for all $E+, E-,B$ there exists $H \in \mathcal{H}$ such that $B \cup H \models E+$ and $B \cup H \not \models E-$.
\subsection{Non-monotonic setting for ILP}
Validity: all $h \in H$ are true in $M+(B)$,
Completeness: if general clause $g$ is true in $M+(B)$, then $H \models g$,	
Minimality: there is no proper subset $G$ of $H$ which is valid and complete.
Question: is $H$ unique from given the minimality condition?

\chapter{Learning hypotheses of the least Kolmogorov complexity}

Given a set of observations $O$, the background knowledge $B$, if the theory $O \union B$ is not complete, i.e. there is more than one model of $O \union B$ up to isomorphism, then there are several hypotheses $H \in \mathcal{H}$ that can explain the observations from the background knowledge, i.e. $B \union H \models O$, and are consistent with the background knowledge, i.e. $B \union H \not\models false$.

What quantifiable properties hypothesis should have can be captured by a score function. Arguing which score function is more desirable is a problem in its own right. We explore the possibility of a score function to be Kolmogorov complexity measure. We would like to find out what ILP systems induce their hypotheses of the least Kolmogorov complexity. The problem proves to be difficult as due to complex grammatical structure of the first order sentences of a logic program. We therefore provide an abstraction where a sentence is represented by an element of a set in hope that understanding the problem in its relaxed version will shed more light into its original version.

\subsection{Problem of induction - computability setting}

\begin{defn}
The Kolmogorov complexity $K(S)$ of a set $S$ is the Kolmogorov complexity
of a formula $\psi$ with the least Kolmogorov complexity that defines $S$.
\end{defn}

Given a set $P \subseteq \mathbb{N}$ of positive examples,
a set $N \subseteq \mathbb{N}$ of negative examples,
find a computable set $S = \{n \in \mathbb{N} : \mathbb{N} \models \phi(n) \} \subseteq \mathbb{N}$ that has the least Kolmogorov complexity.

In a specialized version of the problem, the criterion of Kolmogorov complexity can be replaced by other property definable by a real-valued score function on a triple $\langle P, N, \phi \rangle$. Similarly, we may require weaker conditions on a set $S$, e.g. computable with respect to some oracle set (background knowledge) $B$ or being of other Turing (or enumeration) degree.

\begin{defn}
TODO Herbrand Interpretation, Hebrand Model.
\end{defn}

We can think of the model of an environment as a Hebrand model. Notice that the powerset of natural numbers is equinumerous with the set of Herbrand models. Therefore we can think of a natural number as a ground atom.

\begin{defn}
An \emph{inductive inference system} IIS is a computable function
$f: \powerset{\mathbb{N}} \times \powerset{\mathbb{N}} \times \powerset{\powerset{\mathbb{N}}}
\to \powerset{\mathbb{N}},
\langle P, N, \mathcal{H} \rangle \mapsto S$
satisfying the conditions:

\begin{enumerate}
\item $S$ is a computable set,
\item $S \in \mathcal{H}$,
\item $P \subseteq S$,
\item $N \cap S = \emptyset$.
\end{enumerate}
\end{defn}

We call $P$ a set of positive examples, $N$ a set of negative examples, $\mathcal{H}$ a bias, $S$ a solution (a hypothesis) to a problem of induction.

\section{Bottom and top theories}
The following definitions are motivated by a top theory of TDHD framework and a bottom clause of Prolog systems.
\begin{defn}
A top theory $\top$ with respect to $\langle P, N \rangle$ is
$\{x \in \mathbb{N} : \psi_\top(x)\}$,
a set definable by $\psi_\top$, or implicitly $\psi$ satisfying
$P \subseteq \top$.
A set $\mathcal{H}=\powerset{\top}$ is called a top theory bias.
\end{defn}

\begin{defn}
A bottom theory $\bot$ with respect to $\langle P, N \rangle$ is
$\{x \in \mathbb{N} : \psi_\bot(x)\}$,
a set definable by $\psi_\bot$, or implicitly $\psi$ satisfying
$N \cap \bot = \emptyset$.
A set $\mathcal{H}=\powerset{\bot}$ is called a bottom theory bias.
\end{defn}

\section{Generalization}
In order to define a notion of generalization in an abstracted setting we should understand what a generalization is in various ILP contexts.

Inoue in \cite{inoue2012dnf} defines $\phi$ to be more general than $\psi$ iff
$M(\phi) \subseteq M(\psi)$ iff $\phi \models \psi$. An important observation is that as we generalize $\psi$ strictly the number of the possible models of $\phi$ shrinks, generalization increases certainty.

However, in systems like Toplog and Imparo whose logics use a Negation as Failure NAF, given a set of sentences $\Sigma$ it is complete since
$\forall \psi. \Sigma \models \psi \vee \Sigma \models \neg\psi$. Therefore every such consistent $\Sigma$ has only one Herbrand model. The notion of generalization cannot be defined in terms of models in logics with NAF. The intuition of a generalization comes from the $\theta$-subsumption.

\begin{defn}
Let a $C$, $D$ be sets of clauses, then $C$ $\theta$-subsumes $D$  written
$C \ge_\theta D$ iff $C \theta \subseteq D$.
\end{defn}

Notice that if $C \ge_\theta D$ then $C \models D$. Consider the case of $C=\{\neg female(x) \vee woman(x)\}$ and
$D=\{
\neg female(susan) \vee woman(susan),
\neg female(mary) \vee woman(mary),
\neg female(ann) \vee woman(ann) \}$.
Then $C$ $\theta$-subsumes $D$ with the substitution
$\theta=\{susan \backslash x, mary \backslash x, ann \backslash x\}$.
From the example we can see that generalization corresponds to deriving a more general rule from the specific instances of that rule. The theory $C$ is shorter than $D$ and less complex in a sense that it is easier to remember that every female is a woman than to remember that Susan is a woman, Mary is a woman, Ann is a woman given that we know that Susan, Mary, Ann are females.
Intuitively a set $S$ is simpler than a set $T$ if it can be defined in a simpler way:

\begin{defn}
Let $S$ and $T$ be subsets of $\mathbb{N}$, then we say that $S$ is more general than $T$ iff $K(S)<K(T)$ where $K(-)$ is the Kolmogorov complexity of its input set.
\end{defn}

\chapter{Model approximation}

\subsection{Approximation and error}
\begin{defn}
A theory $\Sigma$ \emph{approximates a theory $\Gamma$ by model} within the number $\epsilon$ called an error measure iff $\mu(M(\Sigma) \triangle M(\Gamma)) < \epsilon$ where $\mu:\{\mathcal{M}:\}\to [0,1]$ is a (probability) measure over the models and $[0,1] \subseteq \mathbb{R}$.
\end{defn}

\chapter{Score function}

\section{Axiomatization by score}
Find any axiomatization $A$ of the theory $O$ given $B$ with the greatest score $s(A,B,O)$ where $s$ is the \emph{score function}
$s:\mathcal{H} \times L \times L \to \mathbb{R}$.

\begin{exmp}
Define a score function $s$ by:
i. if $A$ infinite, then $s(A,B,O)=-1$,
ii. if $A \union B$ inconsistent, then $s(A,B,0)=-1$,
iii. otherwise $s(A,B,O)=\mu\{o \in O : A \union B \models o\}$.
\end{exmp}

\begin{exmp}
Define a score function $s:\mathcal{H} \times L \times L \to \mathbb{R}$ by $s(A,B,O)=1$ iff a theory $A \union B$ approximates a theory $O$ by model within a given error measure $\epsilon$, $s(A,B,O)=0$ otherwise.
\end{exmp}

\begin{note}
A score function is any total function taking into the consideration various criteria, e.g. minimum description length of the theory, finiteness of the theory, computational resources required to compute the theory. What a good score function is will be of our interest later.
\end{note}

\begin{exmp}
A Kolmogorov complexity with respect to the description language $L$ is a score function $K:L \to \mathbb{N}_0 \subseteq \mathbb{R}$.
\end{exmp}

\begin{note}
A Kolmogorov complexity $K$ can be thought of as a minimul description length with respect to the description language.
\end{note}
\begin{exmp}
Fix a description language to be a language of regular expressions. Let $L_1$ be a regular language given by its shortest description $0000000000$, $L_2$ given by its shortest description $10*$. Then $K(L_1)=10<K(L_2)=3$.
\end{exmp}

\chapter{Statistical learning}

\section{Incoherence of the generalization - Sawin-Demski paradox}
Consider the observations $P(0), \neg P(1), P(2)$ which are generalized by the rule $R1=\forall x. P(x)$ iff $x$ is even.
By inducing the $\Pi_1$ statement into the theory, it may be impossible for some true $\Pi_2$ statements to be part of the theory\cite{sawin2013computable}.
Therefore we limit ourselves to studying only the models where $\Pi_1$ statement true, $\Pi_2$ statement false.
