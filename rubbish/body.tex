
\section{The role of a language in learning the models}
Consider an empty language. It has one possible interpretation and one model up to an isomorphism.
By extending the language we can distinguish the models further. The problem of predicate invention is to invent a predicate so that the right distinction between models is made based on the language.
Questions:
1. How can we measure an expessivity and a complexity of a language?
2. How much information can be captured by a $\mathcal{L}$-theory given a language $\mathcal{L}$?
3. What is the maximal complexity of the language $\mathcal{L}$ for which the $\mathcal{L}$-theory is decidable?
4. Complexity of learning the equivalence class of the reality given a language.
5. Is there a language for which the task of learning the equivalence class of the reality is undecidable?
6. What percentage of the reality can we recover from observing and reasoning only with the statements in the restricted language?
7. Can a language be partitioned (or expressed as posets of sublanguages) into sublanguages and the theories of the sublanguages be learnt, then joint into a theory of the original language?

\chapter{Rationale ILP system}
After having reviewed ILP systems, we aim to design a custom ILP system called Rationale that would address some of the problems we have encountered in the ILP systems.

\section{Suggested solutions}
Our suggested solutions are an extension of the work\cite{yamamoto2012inverse} by Yamamoto, et al. in which they provide a complete theoretical framework for solving the problem of an explanatory induction.
   
\subsection{Inverse Subsumption for Complete Explanatory Induction}
\begin{lemma}\label{yamamoto2012inverseLemma2}\cite{yamamoto2012inverse}
Let $B$, $E$ and $I_H$ be a background theory, examples and an induction field,
respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$, $H$ satisfies the following condition:

$H \subsumes \tau(M(F \cup Taut(I_H))$.
\end{lemma}

\subsection{Extension to first-order theories}
A generalization of the antisubsumption results to the first-order clausal theories enables a more efficient computation of the minimal complement, especially if the number of potential tautologies is large and a computation of anti-subsumption since there is less need to search for a subsumed hypothesis as the hypothesis is already not ground.

\begin{proposition}(Soundness of the first-order extension)
Let $B$, $E$ and $I_H$ be a first-order background theory, examples and an induction field, respectively. Let $F$ be a bridge theory wrt $B$ and $E$. Suppose that $H \subsumes \tau(M(F \cup Taut(I_H)))$, then $B \cup H \models E$.
\end{proposition}
\begin{proof}
By subsumption $H \models \tau(M(F \cup Taut(I_H))) \equiv \neg F$. By the principle of the inverse entailment $B \cup \neg F \models E$, hence
$B \cup H \models E$.
\end{proof}

However, it is not clear whether the completeness is preserved in the first order case.

$\tau'(S)$ denotes the clausal theory obtained by removing every clause from $S$ that has a ground instance that factors to a tautology. $I_H$ is an induction field of not necessarily ground literals.

\begin{conjecture}\label{subsumptionConjectureFirstOrder}
Let $B$, $E$ and $I_H$ be a background theory, examples and an induction field, respectively. Let $F$ be a bridge theory wrt $B$ and $E$. For every hypothesis $H$ wrt $I_H$ and $F$, $H$ satisfies the following condition:
$H \subsumes \tau'(M(F \cup Taut(I_H)))$.
\end{conjecture}

\begin{exmp}
Let $E=\{r(y)\}$,
$B=\{\neg p(x) \vee r(x)\}$,
$H=\{s(x), \neg s(x) \vee p(x)\}$,
$I_H=\{s(x), \neg s(x), p(x)\}$.
Then $F=B \cup \neg E=\{\neg p(x) \vee r(x), \neg r(y) \}$.
$Taut(I_H)=\{s(x) \vee \neg s(x)\}$.
$M(F \cup Taut(I_H))=$
$\{p(x) \vee r(y) \vee \neg s(z), p(x) \vee r(y) \vee s(z),$
$\neg r(x) \vee r(y) \vee \neg s(z),\neg r(x) \vee r(y) \vee s(z) \}$.

$\tau'(M(F \cup Taut(I_H))=\{p(x) \vee r(y) \vee \neg s(z), p(x) \vee r(y) \vee s(z)\}$ which is subsumed by the hypothesis $H$.
\end{exmp}

\begin{exmp}
All theories need not be grounded. Adapting the previous example, if the examples were
$E_2=\{r(a), r(b)\}$ with the same hypothesis
$H=\{s(x), \neg s(x) \vee p(x)\}$, then
$\tau'(M(F_2 \cup Taut(I_H))=\{p(x) \vee r(a) \vee \neg s(z), p(x) \vee r(a) \vee s(z), p(x) \vee r(b) \vee \neg s(z), p(x) \vee r(b) \vee s(z) \}$ which is subsumed by $H$. A careful reader notices that the adapted $\tau'(M(F_2 \cup Taut(I_H))$ is just a partial instantiation of the previous
$\tau'(M(F \cup Taut(I_H))$ with the same substitution
$theta=\{a / y, b / y\}$ as used for the examples:
$\tau'(M(F_2 \cup Taut(I_H)) \theta=\tau'(M(F_2 \cup Taut(I_H))$,
$E_2 \theta = E$.
\end{exmp}

\subsection{Extension to cover negative examples}
We extend the completeness results and algorithm in a more general problem of explanatory induction that includes the negative examples.
\begin{defn}General ILP Problem setting: given the clausal theories $B$, $H$, $E^+$, $E^-$ find a hypothesis $H$ such that $B \wedge H \models E+$, $B \wedge H$ consistent, $B \wedge H \not\models E-$.
\end{defn}

\begin{lemma}\label{yamamoto2012inverseLemma2Converse}
Let $B \cup \neg E \models F$. $H \subsumes \tau(M(F \cup Taut(I_H)))$ implies
$B \cup H \models E$.
\end{lemma}
\begin{proof}
By the principle of the inverse entailment and by $\tau(M(F \cup Taut(I_H))) \equiv \neg F$.
\end{proof}

\begin{lemma}
Let $E-=\{e_1,...,e_n\}$, $F_i-=B \cup \{\neg e_i\}$ and $H$ be
a hypothesis wrt $I_H, B, E+, E-$.
Then $B \cup H \not\models E-$ iff
$\forall i \in \{1,...,n\}.$ $H \not\subsumes \tau(M(F_i- \cup Taut(I_H)))$.
\end{lemma}

\begin{proof}
Use \ref{yamamoto2012inverseLemma2}, \ref{yamamoto2012inverseLemma2Converse} and the fact that $E-$ is in a disjunctive normal form.
\end{proof}

\begin{conjecture}
$\tau(M(F_i- \cup Taut(I_H))) \subsumes \tau(M(F- \cup Taut(I_H)))$
where $F-=F_1- \wedge ... \wedge F_n-$.
\end{conjecture}

By the principle of inverse entailment
$B \wedge H \not\models E-$ iff
$B \wedge \neg E- \not\models \neg H$.

\begin{defn}
(Negative bridge theory) Let $B$ and $E-$ be a background theory and negative examples, respectively. Let $F-$ be a ground clausal theory. Then $F-$ is a negative bridge theory wrt $B$ and $E$ iff $B \wedge \neg E- \models F-$ holds.
\end{defn}

\begin{conjecture}\label{subsumptionConjectureNegativeExamples}
(Note: probably false).
Let $B$, $E-$ and $I_H$ be a background theory, negative examples and an induction field, respectively. Let $F-$ be a negative bridge theory wrt $B$ and $E-$. For every hypothesis $H$ wrt $I_H$ and $F-$,

$B \wedge H \not\models E-$ iff $H$ does not subsume
$\tau(M(F- \cup Taut(I_H)))$.
\end{conjecture}

\begin{defn}
Let $B$, $E+$, $E-$ and $I_H = \langle L \rangle$ be a background theory, positive examples, negative examples and an induction field, respectively.
Let $F$ be a bridge theory wrt $B$ and $E+$. Let $F-$ be a maximal negative bridge theory wrt $B$ and $E-$. A clausal theory $H$ is derived
by extended inverse subsumption with minimal complements from $F$ and $F-$ wrt $I_H$ if $H$ is constructed as
follows:
\begin{itemize}
\item Step 1. Compute $Taut(I_H)$;
\item Step 2. Compute $\tau(M(F \cup Taut(I_H)))$;
\item Step 3. Compute $\tau(M(F- \cup Taut(I_H)))$;
\item Step 4. Construct a clausal theory $H$ satisfying the conditions:
$H \subsumes \tau(M(F \cup Taut(I_H)))$,
$H \not\subsumes \tau(M(F- \cup Taut(I_H)))$.
\end{itemize}
\end{defn}

\subsection{Imparo extension}
\begin{defn}
Let $P=\langle B, U, I \rangle$ be an open definite program, $H$ be a correct hypothesis wrt $P$ and a ground example $E$, then $H$ is derivable by
\emph{connected theory inverse subsumption}
iff there exists a connected theory $T$ for $P$ and $E$ such that $H \subsumes T$.
We denote the statement by $P, E \vdash_{CTIS} H$.
\end{defn}

\begin{thm}\label{yamamoto2012inverseTheorem4}\cite{yamamoto2012inverse}
Let $S$ be a ground clausal theory. Then, $M^2(S) = \mu(S)$ holds.
\end{thm}

\begin{proposition}
Let $H \in I_H$ be an inductive solution for $P$ and $E$ such that $Taut(I_H)=\emptyset$, then $H$ is derivable by connected theory inverse subsumption from $P, E$.
\end{proposition}
\begin{proof}
By completeness of connected theory generalization\ref{completeness_ctg} there exists a connected theory $T$ such that $H \models T$. $M(T)$ is a bridge formula.
Hence by \ref{yamamoto2012inverseLemma2}
$H \subsumes \tau(M(M(T) \cup Taut(I_H))$. From the definition of the connected theory $\tau(T)=T$ and $\mu (T)=T$.
Therefore $\tau(M(M(T) \cup Taut(I_H))=\tau(M(M(T))=\tau(T)=T$ using
\ref{yamamoto2012inverseTheorem4}. Therefore $H \subsumes T$ as required.
\end{proof}
\begin{remark}
The previous proposition is stated and proved only to provide an alternative proof of a statement superceded by the stronger result \ref{completeness_ctis}.
\end{remark}

\begin{thm}\label{implicationByGroundClauses}
(Implication by Ground Clauses \cite{nienhuys1997foundations}). Let $\Sigma$ be a non-empty set of clauses,
and $C$ be a ground clause. Then $\Sigma \models C$ if and only if there is a finite set $\Sigma_g$ of ground
instances of clauses from $\Sigma$, such that $\Sigma_g \models C$.
\end{thm}

\begin{thm}\label{completeness_ctg}
\emph{Completeness of connected theory generalization}(Theorem4.6 in \cite{kimber2012learning})
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory generalisation.
\end{thm}
\begin{proof}\cite{kimber2012learning}
For the full proof, an interested reader is encouraged to read \cite{kimber2012learning}.
Since $H$ is a correct hypothesis for $P$ and $E$,
then $B \cup H \models E$ by definition.
Therefore, by \ref{implicationByGroundClauses}, there is a finite set $S$ of ground instances of clauses in $B \cup H$,
such that $S \models E$. Let $T = S \cap ground(H)$.
Since $T \subseteq S$, then $T$ is ground and finite, and
since $T \subseteq ground(H)$ then $H \models T$. 
Then Kimber proves that $T$ is a connected theory for $P$ and $E$.
\end{proof}

\begin{thm}
\emph{Completeness of connected theory inverse subsumption}.
\label{completeness_ctis}
Let $\langle B, U, I \rangle$ be a definite open program,
let $H$ be a definite program, and let $e$ be an atom.
If $H$ is an inductive solution for $P$ and
$E = \{e\}$, then $H$ is derivable from $P$ and $E$ by connected theory inverse subsumption.
\end{thm}
\begin{proof}
Construct a connected theory $T=S \cap ground(H)$ for $P$ and $E$ as in the proof of \ref{completeness_ctg}.
Then $H \subsumes ground(H) \subsumes S \cap ground(H) = T$,
hence $H \subsumes T$ by transitivity as required.
\end{proof}

\subsection{Faster consistency check\cite{yamamoto2012comparison}}
Checking of the consistency of $B \cup E$ in upward generalization can be done by finding a refutation for example, but in general this verification is expensive. However, using \ref{yamamoto2012inverseLemma2} one can reduce the consistency check problem into subsumption problem by verifying $\tau(M(B \cup \neg E \cup Taut(I_H))$ is subsumed by the hypothesis $H$. Computing $\tau(M(B \cup \neg E \cup Taut(I_H))$ is deterministic and with the application of \ref{subsumptionConjectureFirstOrder} tractable and potentially efficient. Checking the subsumption condition is trivial and efficient.

\subsection{Hypothesis selection}
Given clausal theories $E$, $B$, there may be several non-equivalent clausal theories $H$ such that $B \cup H \models E$ and $B \cup H \not\models false$. Denote the set of such (correct wrt $E$, $B$) hypotheses $\mathcal{H}$. The problem of hypothesis selection is to induce only one $H \in \mathcal{H}$.

We seek to resolve the hypothesis selection problem by providing a justification for the selected hypotheses. The problem may be reduced to the use of the argumentation frameworks such as Abstract Argumentation (AA) or Assumption Based Argumentation (ABA).
\subsection{Hypotheses enumeration}
Enumerating all the correct hypotheses can be done efficiently using the antisubsumption and the result \ref{yamamoto2012inverseLemma2}. First $\tau(M(B \cup \neg E \cup Taut(I_H))$ is computed, then antisubsumed theories $H$ can be enumerated trivially and efficiently.

\section{Unresolved problems}
\begin{itemize}
\item The background theory has to be Horn.
\end{itemize}

\section{Properties of an ILP system}
We further seek properties of ILP systems upon which we could base our comparison of different ILP systems. Recall $L$ is a set of all logical sentences for a given signature $\mathcal{L}$ and $B, E+, E-, H \subseteq L$, $\mathcal{H}, \mathcal{H}_2 \subseteq \powerset{L}$.

We ask:
\begin{enumerate}
\item On what inputs is $s$ defined?
\item What is the algorithm computing $s$?
\item What is the computational complexity of $s$?
\item On what inputs do different systems $s_1, s_2$ produce a different output?
\item If on a given input $i$ and $s_1(i) \not= s_2(i)$, is there a way to compare the systems based on their output solution set?
\item Is there a way to equip the input and the output with the semantics to provide an interpretation for the output produced from the input?
\end{enumerate}



\section{Algorithmic definition of an ILP system}
We provide a simplified algorithm for each of the systems in order to be able to extract properties of each system upon which a comparison could based.

\subsection{IE algorithm\cite{yamamoto2012inverse}}
Progol, Xhail, Imparo are based on the principle of the inverse entailment. By the principle of the inverse entailment $B \cup H \models E$ iff
$B \cup \neg E \models \neg H$. A hypothesis $H \in \mathcal{H}_2$ is a solution to the problem of the explanatory induction $(B,E+,E-,\mathcal{H})$ iff
$B \cup H \subseteq Cn(E+ \union \overline{E-})$,
 $false \not\in Cn(E+ \cup E-)$, $\mathcal{H}_2 \subseteq \mathcal{H}$
iff
$B \cup H \subseteq Cn(E+ \union \overline{E-})$,
 $false \not\in Cn(E+ \cup E-)$, $\mathcal{H}_2 \subseteq \mathcal{H}$
 
 They compute the hypothesis $H$ in two steps:
1. constructing an intermediate theory, 2. generalizing its negation into the hypothesis with the inverse of the entailment relation.


\subsection{Aleph}
\subsection{Progol}
\subsection{Toplog}
\subsubsection{Top theory construction}
\subsection{Imparo}
\subsection{Tal}
\subsection{Xhail}
    
\chapter{Definition of an ILP problem - notes}
For a given set of examples often multiple sets of hypotheses can explain such a set of examples. However, the common definition of an ILP problem does not convey which hypotheses should be chosen in such a case. This chapter presents an investigation of an definition of an ILP problem and critique of the incompleteness of the definition of an ILP problem.

Related to the definition of an ILP problem Cheng and Wolf say in \cite{nienhuys1997foundations}
"If E + is finite, then E = E -e will be a correct theory, but a rather unin-
teresting one. In this case, we would not have learned anything beyond the
given examples: the induced theory has no predictive power. To avoid this,
we can put some constraints on the theory. For instance, we might demand
that E contains less clauses than the number of given positive examples. In
that case, E = E + is ruled out. Since constraints like these mainly depend
on the particular application at hand, we will not devote much attention to
them."
They acknowledge that their definition of an ILP problem does not specify what explanation $H$ should explain the examples. However, to the author of this thesis it is unclear how one would complete the definition with respect to the particular application.

Muggleton in his Inverse Entailment and Progol \cite{muggleton1995inverse} says:
"the association of probability values with hypotheses requires the assumption of a prior probability distribution over the hypothesis language. Occam's razor can be taken as an instance of a distribution which assigns higher prior probability to simpler hypotheses. It has been shown [4] that without such distributional assumptions the class of all logic programs is not even PAC-predictable. On the other hand, it has recently been demonstrated [42] that the class of all time-bounded logic programs is polynomial-time learnable (U-learnable) under fairly broad families of prior probability distributions. Appendix B gives more details of the relationship between data, posterior probabilities and U-learnability."

\section{ILP theory and methods}
\subsection{Completeness}
From ILP theory and methods \cite{muggleton1994inductive}:
An ILP system with a bias $\mathcal{H}$ is complete iff for all $E+, E-,B$ there exists $H \in \mathcal{H}$ such that $B \cup H \models E+$ and $B \cup H \not \models E-$.
\subsection{Non-monotonic setting for ILP}
Validity: all $h \in H$ are true in $M+(B)$,
Completeness: if general clause $g$ is true in $M+(B)$, then $H \models g$,	
Minimality: there is no proper subset $G$ of $H$ which is valid and complete.
Question: is $H$ unique from given the minimality condition?

\chapter{Learning hypotheses of the least Kolmogorov complexity}

Given a set of observations $O$, the background knowledge $B$, if the theory $O \union B$ is not complete, i.e. there is more than one model of $O \union B$ up to isomorphism, then there are several hypotheses $H \in \mathcal{H}$ that can explain the observations from the background knowledge, i.e. $B \union H \models O$, and are consistent with the background knowledge, i.e. $B \union H \not\models false$.

What quantifiable properties hypothesis should have can be captured by a score function. Arguing which score function is more desirable is a problem in its own right. We explore the possibility of a score function to be Kolmogorov complexity measure. We would like to find out what ILP systems induce their hypotheses of the least Kolmogorov complexity. The problem proves to be difficult as due to complex grammatical structure of the first order sentences of a logic program. We therefore provide an abstraction where a sentence is represented by an element of a set in hope that understanding the problem in its relaxed version will shed more light into its original version.

\subsection{Problem of induction - computability setting}

\begin{defn}
The Kolmogorov complexity $K(S)$ of a set $S$ is the Kolmogorov complexity
of a formula $\psi$ with the least Kolmogorov complexity that defines $S$.
\end{defn}

Given a set $P \subseteq \mathbb{N}$ of positive examples,
a set $N \subseteq \mathbb{N}$ of negative examples,
find a computable set $S = \{n \in \mathbb{N} : \mathbb{N} \models \phi(n) \} \subseteq \mathbb{N}$ that has the least Kolmogorov complexity.

In a specialized version of the problem, the criterion of Kolmogorov complexity can be replaced by other property definable by a real-valued score function on a triple $\langle P, N, \phi \rangle$. Similarly, we may require weaker conditions on a set $S$, e.g. computable with respect to some oracle set (background knowledge) $B$ or being of other Turing (or enumeration) degree.

\begin{defn}
TODO Herbrand Interpretation, Hebrand Model.
\end{defn}

We can think of the model of an environment as a Hebrand model. Notice that the powerset of natural numbers is equinumerous with the set of Herbrand models. Therefore we can think of a natural number as a ground atom.

\begin{defn}
An \emph{inductive inference system} IIS is a computable function
$f: \powerset{\mathbb{N}} \times \powerset{\mathbb{N}} \times \powerset{\powerset{\mathbb{N}}}
\to \powerset{\mathbb{N}},
\langle P, N, \mathcal{H} \rangle \mapsto S$
satisfying the conditions:

\begin{enumerate}
\item $S$ is a computable set,
\item $S \in \mathcal{H}$,
\item $P \subseteq S$,
\item $N \cap S = \emptyset$.
\end{enumerate}
\end{defn}

We call $P$ a set of positive examples, $N$ a set of negative examples, $\mathcal{H}$ a bias, $S$ a solution (a hypothesis) to a problem of induction.

\section{Bottom and top theories}
The following definitions are motivated by a top theory of TDHD framework and a bottom clause of Prolog systems.
\begin{defn}
A top theory $\top$ with respect to $\langle P, N \rangle$ is
$\{x \in \mathbb{N} : \psi_\top(x)\}$,
a set definable by $\psi_\top$, or implicitly $\psi$ satisfying
$P \subseteq \top$.
A set $\mathcal{H}=\powerset{\top}$ is called a top theory bias.
\end{defn}

\begin{defn}
A bottom theory $\bot$ with respect to $\langle P, N \rangle$ is
$\{x \in \mathbb{N} : \psi_\bot(x)\}$,
a set definable by $\psi_\bot$, or implicitly $\psi$ satisfying
$N \cap \bot = \emptyset$.
A set $\mathcal{H}=\powerset{\bot}$ is called a bottom theory bias.
\end{defn}

\section{Generalization}
In order to define a notion of generalization in an abstracted setting we should understand what a generalization is in various ILP contexts.

Inoue in \cite{inoue2012dnf} defines $\phi$ to be more general than $\psi$ iff
$M(\phi) \subseteq M(\psi)$ iff $\phi \models \psi$. An important observation is that as we generalize $\psi$ strictly the number of the possible models of $\phi$ shrinks, generalization increases certainty.

However, in systems like Toplog and Imparo whose logics use a Negation as Failure NAF, given a set of sentences $\Sigma$ it is complete since
$\forall \psi. \Sigma \models \psi \vee \Sigma \models \neg\psi$. Therefore every such consistent $\Sigma$ has only one Herbrand model. The notion of generalization cannot be defined in terms of models in logics with NAF. The intuition of a generalization comes from the $\theta$-subsumption.

\begin{defn}
Let a $C$, $D$ be sets of clauses, then $C$ $\theta$-subsumes $D$  written
$C \ge_\theta D$ iff $C \theta \subseteq D$.
\end{defn}

Notice that if $C \ge_\theta D$ then $C \models D$. Consider the case of $C=\{\neg female(x) \vee woman(x)\}$ and
$D=\{
\neg female(susan) \vee woman(susan),
\neg female(mary) \vee woman(mary),
\neg female(ann) \vee woman(ann) \}$.
Then $C$ $\theta$-subsumes $D$ with the substitution
$\theta=\{susan \backslash x, mary \backslash x, ann \backslash x\}$.
From the example we can see that generalization corresponds to deriving a more general rule from the specific instances of that rule. The theory $C$ is shorter than $D$ and less complex in a sense that it is easier to remember that every female is a woman than to remember that Susan is a woman, Mary is a woman, Ann is a woman given that we know that Susan, Mary, Ann are females.
Intuitively a set $S$ is simpler than a set $T$ if it can be defined in a simpler way:

\begin{defn}
Let $S$ and $T$ be subsets of $\mathbb{N}$, then we say that $S$ is more general than $T$ iff $K(S)<K(T)$ where $K(-)$ is the Kolmogorov complexity of its input set.
\end{defn}

\chapter{Model approximation}

\subsection{Approximation and error}
\begin{defn}
A theory $\Sigma$ \emph{approximates a theory $\Gamma$ by model} within the number $\epsilon$ called an error measure iff $\mu(M(\Sigma) \triangle M(\Gamma)) < \epsilon$ where $\mu:\{\mathcal{M}:\}\to [0,1]$ is a (probability) measure over the models and $[0,1] \subseteq \mathbb{R}$.
\end{defn}

\chapter{Score function}

\section{Axiomatization by score}
Find any axiomatization $A$ of the theory $O$ given $B$ with the greatest score $s(A,B,O)$ where $s$ is the \emph{score function}
$s:\mathcal{H} \times L \times L \to \mathbb{R}$.

\begin{exmp}
Define a score function $s$ by:
i. if $A$ infinite, then $s(A,B,O)=-1$,
ii. if $A \union B$ inconsistent, then $s(A,B,0)=-1$,
iii. otherwise $s(A,B,O)=\mu\{o \in O : A \union B \models o\}$.
\end{exmp}

\begin{exmp}
Define a score function $s:\mathcal{H} \times L \times L \to \mathbb{R}$ by $s(A,B,O)=1$ iff a theory $A \union B$ approximates a theory $O$ by model within a given error measure $\epsilon$, $s(A,B,O)=0$ otherwise.
\end{exmp}

\begin{note}
A score function is any total function taking into the consideration various criteria, e.g. minimum description length of the theory, finiteness of the theory, computational resources required to compute the theory. What a good score function is will be of our interest later.
\end{note}

\begin{exmp}
A Kolmogorov complexity with respect to the description language $L$ is a score function $K:L \to \mathbb{N}_0 \subseteq \mathbb{R}$.
\end{exmp}

\begin{note}
A Kolmogorov complexity $K$ can be thought of as a minimul description length with respect to the description language.
\end{note}
\begin{exmp}
Fix a description language to be a language of regular expressions. Let $L_1$ be a regular language given by its shortest description $0000000000$, $L_2$ given by its shortest description $10*$. Then $K(L_1)=10<K(L_2)=3$.
\end{exmp}

\chapter{Statistical learning}

\section{Incoherence of the generalization - Sawin-Demski paradox}
Consider the observations $P(0), \neg P(1), P(2)$ which are generalized by the rule $R1=\forall x. P(x)$ iff $x$ is even.
By inducing the $\Pi_1$ statement into the theory, it may be impossible for some true $\Pi_2$ statements to be part of the theory\cite{sawin2013computable}.
Therefore we limit ourselves to studying only the models where $\Pi_1$ statement true, $\Pi_2$ statement false.
